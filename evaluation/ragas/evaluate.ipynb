{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Retrieval-Augmented Generation (RAG) Applications: Importance and Impact  \n",
    "\n",
    "Evaluating RAG-based applications is crucial to ensure the reliability, relevance, and efficiency of their responses. Unlike using LLMs by themselves, RAG-based applications combine the power of Large-Language-Models with vector stores and retrieval components, which introduces complexity. Hence, evaluation of different components in RAG pipelines is crucial. \n",
    "\n",
    "#### Key Pitfalls in RAG Applications\n",
    "- **Hallucination**: Even with retrieved context, LLMs can still generate inaccurate or misleading responses.  \n",
    "- **Irrelevant Retrieval**: Poor retrieval results in the model relying on incomplete or incorrect context.  \n",
    "- **Latency & Efficiency**: Combining retrieval and generation can introduce delays, requiring performance optimization. \n",
    "\n",
    "#### Advantages of Proper Evaluation\n",
    "- **Improved Accuracy**: Ensures the factual correctness of responses by using a knowledge base.  \n",
    "- **Enhanced Relevance**: Helps refine retrieval mechanisms, ensuring responses are more context-aware.  \n",
    "- **Robustness & Adaptability**: Identifies failure points, enabling continuous improvements to model behavior.  \n",
    "\n",
    "#### Impact on Future Development  \n",
    "A structured evaluation process enables better benchmarking, allowing for iterative improvements in retrieval models, embeddings, and LLM fine-tuning. It also promotes transparency in AI decision-making, paving the way for more explainable and ethical AI systems and thus gaining a wider adoption. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "* Execute the script in the root of the `evaluation` folder.\n",
    "* If executing the script fails run: `chmod u+x setup.sh`.\n",
    "* It will install all required dependencies.\n",
    "* Finally, make sure you select it in the notebook by specifying `eval` as kernel.\n",
    "\n",
    "---\n",
    "\n",
    "*(OPTIONAL STEP)*\n",
    "\n",
    "**RAGAs** provides a cloud platform where a dataset and evaluation results can be stored and viewed.\n",
    "To use it follow this link: [RAGAs.io](https://app.ragas.io/).\n",
    "* Sign-up\n",
    "* Retrieve the **token**\n",
    "* Create a `.env` file with the following content:\n",
    "```bash\n",
    "RAGAS_APP_TOKEN=apt.......-9f6ed\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Note**\n",
    "* In my project I've used **DeepEval** to generate a synthetic dataset and it was uploaded on **ConfidentAI**. For that reason I will pull it from my own account.\n",
    "* If you have used my `generate` notebook with **RAGAs** to generate testdata you can load it aswell.\n",
    "* If you want to follow along and pull the data from **DeepEval** make sure you have a `.env` file with **DeepEval API key**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ðŸŽ‰ðŸ¥³ Congratulations! You've successfully logged in! ðŸ™Œ \n",
       "</pre>\n"
      ],
      "text/plain": [
       "ðŸŽ‰ðŸ¥³ Congratulations! You've successfully logged in! ðŸ™Œ \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from deepeval import login_with_confident_api_key\n",
    "\n",
    "# If you have followed the optional step, execute this cell\n",
    "# My .env file is stored at the root of the evaluation folder so I can\n",
    "# combine all environment variables for all frameworks\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "deepeval_api_key: str = os.getenv(\"DEEPEVAL_API_KEY\")\n",
    "\n",
    "# You should get a message letting you know you are logged-in.\n",
    "login_with_confident_api_key(deepeval_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the evaluation dataset\n",
    "\n",
    "According to **RAGAs** an **evaluation dataset** is a homogeneous collection of **data samples** with the sole purpose of measuring the capabilities and performance of an AI application.\n",
    "\n",
    "- **Structure**:\n",
    "\n",
    "    - Contains either **SingleTurnSample** or **MultiTurnSample** object instances, each of them representing a unique interaction between a **Persona** and the AI-system.\n",
    "\n",
    "    - **NOTE**: The dataset can contain **ONLY** a single type of samples. They cannot be mixed together into a single dataset.\n",
    "\n",
    "**Samples** represent a single unit of interaction with the underlying system. As mentioned they can be either **SingleTurnSample** or **MultiTurnSample**.\n",
    "\n",
    "For this project I focus solely on **SingleTurnSample** objects, since I'm evaluating independent queries, not multi-turn interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/eval/lib/python3.12/\n",
       "site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/eval/lib/python3.12/\n",
       "site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import deepeval.dataset\n",
    "\n",
    "# To avoid conflicts between RAGAs and DeepEval since both use the same name\n",
    "deepeval_eval_dataset = deepeval.dataset.EvaluationDataset()\n",
    "deepeval_eval_dataset.pull(\n",
    "    alias=os.getenv(\"DATASET_ALIAS\"),\n",
    "    auto_convert_goldens_to_test_cases=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/eval/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from ragas import EvaluationDataset, SingleTurnSample\n",
    "\n",
    "# After pulling my dataset from ConfidentAI I need to convert it into proper RAGAs evaluation dataset\n",
    "\n",
    "samples: List[SingleTurnSample] = []\n",
    "for test_case in deepeval_eval_dataset.test_cases:\n",
    "    single_turn_sample = SingleTurnSample(\n",
    "        user_input=test_case.input,\n",
    "        retrieved_contexts=test_case.retrieval_context,\n",
    "        reference_contexts=test_case.context,\n",
    "        response=test_case.actual_output,\n",
    "        reference=test_case.expected_output\n",
    "    )\n",
    "    samples.append(single_turn_sample)\n",
    "    \n",
    "evaluation_dataset = EvaluationDataset(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "from ragas import (\n",
    "    evaluate,\n",
    "    RunConfig,\n",
    "    DiskCacheBackend\n",
    ")\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "load_dotenv(\"../../env/rag.env\") \n",
    "\n",
    "ollama_llm = OllamaLLM(\n",
    "    model=os.getenv(\"CHAT_MODEL\"),\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=float(os.getenv(\"TEMPERATURE\")),\n",
    "    num_ctx=24000,\n",
    "    format=\"json\"\n",
    ")\n",
    "\n",
    "ollama_embeddings = OllamaEmbeddings(\n",
    "    model=os.getenv(\"EMBEDDING_MODEL\"),\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    timeout = 43200, # twelve hours, depending on GPU, model, testsize, etc -> can experinment\n",
    "    max_wait = 30,\n",
    "    log_tenacity = True\n",
    ")\n",
    "\n",
    "cacher = DiskCacheBackend(\".cache\")\n",
    "\n",
    "llm = LangchainLLMWrapper(\n",
    "    langchain_llm=ollama_llm,\n",
    "    run_config=run_config,\n",
    "    cache=cacher\n",
    ")\n",
    "\n",
    "embeddings = LangchainEmbeddingsWrapper(\n",
    "    embeddings=ollama_embeddings,\n",
    "    run_config=run_config,\n",
    "    cache=cacher\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "Metrics in **RAGAs** can be classified depending on the mechanism they evaluate the application as:\n",
    "\n",
    "- **LLM-based** - where a LLM is used to perform the evaluation. There might be more than one queries submitted to the LLM to evaluate the application on a single metric. Furthermore, this type of metrics mimic a user, since they tend to be quite non-deterministic and unpredictable. \n",
    "\n",
    "- **Non-LLM based** - where no LLM is required or used to perform the evaluation. These metrics tend to be much faster and predictable than the previous type. However, in my opinion are maybe not the best for evaluating a RAG system where a single query can be submitted multiple times and each time a new response might be generated.\n",
    "\n",
    "The previously mentioned metric types can be further classified into **SingleTurnMetric** and **MultiTurnMetric** respectively. I would like to note again that in this project the **SingleTurnMetric** would be relevant and used.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Context Precision**\n",
    "\n",
    "> **TL;DR:** What fraction of the retrieved chunks are **actually relevant**?\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition**:\n",
    "\n",
    "**Context Precision** is a crucial metric for evaluating the **retrieval capabilities** of a RAG (Retrieval-Augmented Generation) system. Since the **LLM's response** depends heavily on the **context retrieved from the knowledge base**, it is essential to have a reliable retrieval mechanism that fetches **only relevant** information.  \n",
    "\n",
    "By achieving **high context precision**, the system ensures that the retrieved information is **more relevant**, leading to **more accurate responses** and **reduced hallucinations**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Formula:**\n",
    "For each retrieved chunk at **rank (k)**:\n",
    "\n",
    "$\\text{Precision@k} = \\frac{\\text{Number of relevant chunks at rank k}}{\\text{Total number of retrieved chunks at rank k}}$\n",
    "\n",
    "We then compute **Context Precision @ K** as the **mean of all Precision@k values**, weighted by relevance:\n",
    "\n",
    "$\\text{Context Precision @ K} = \\frac{\\sum_{k=1}^{K} \\text{Precision@k} \\times \\text{Relevance}(k)}{\\text{Total number of relevant chunks in top K results}}$\n",
    "\n",
    "Where:\n",
    "- **Precision@k** is the proportion of relevant chunks at rank \\( k \\).\n",
    "- **Relevance(k)** is a binary indicator (1 if the chunk at rank \\( k \\) is relevant, 0 otherwise).\n",
    "- **K** is the total number of retrieved chunks.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Calculation**\n",
    "Suppose our **retriever fetches 4 chunks**, and **2 of them** are relevant. We calculate **Precision@k** at each rank:\n",
    "\n",
    "| Rank \\( k \\) | Retrieved Chunk | Relevant? | Precision@k |\n",
    "|-------------|----------------|------------|-------------|\n",
    "| 1           | âœ… Relevant      | âœ… (1)      | \\( $\\frac{1}{1}$ = 1.00 \\) |\n",
    "| 2           | âŒ Not Relevant  | âŒ (0)      | \\( $\\frac{1}{2}$ = 0.50 \\) |\n",
    "| 3           | âœ… Relevant      | âœ… (1)      | \\( $\\frac{2}{3}$ = 0.67 \\) |\n",
    "| 4           | âŒ Not Relevant  | âŒ (0)      | \\( $\\frac{2}{4}$ = 0.50 \\) |\n",
    "\n",
    "Now, using the **Context Precision formula**:\n",
    "\n",
    "\n",
    "$\\text{Context Precision @ 4} = \\frac{(1.00 \\times 1) + (0.50 \\times 0) + (0.67 \\times 1) + (0.50 \\times 0)}{2} = $\n",
    "\n",
    "= $\\frac{1.00 + 0 + 0.67 + 0}{2} = \\frac{1.67}{2}$ = 0.835\n",
    "\n",
    "Thus, **Context Precision @ 4 = 0.835**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why is This Useful?**\n",
    "\n",
    "- **Evaluates Retriever Quality** â†’ Ensures retrieved chunks contain **relevant** information.\n",
    "\n",
    "- **Improves RAG Performance** â†’ Helps **reduce hallucinations** and improves **LLM accuracy**.\n",
    "\n",
    "- **Higher values** for this metric would signify a good retriever, ranking **relevant chunks** high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    LLMContextPrecisionWithReference,\n",
    "    LLMContextPrecisionWithoutReference,\n",
    "    NonLLMContextPrecisionWithReference\n",
    ")\n",
    "\n",
    "# Uses the user_input, retrieved_contexts, and reference columns\n",
    "llm_context_precision_with_re = LLMContextPrecisionWithReference()\n",
    "\n",
    "# Uses the user_input, retrieved_contexts, and response columns\n",
    "llm_context_precision_without_re = LLMContextPrecisionWithoutReference()\n",
    "\n",
    "\"\"\"\n",
    "This particular version uses a distance measure to determine the semantic similarity\n",
    "between chunks from the retrieved context and reference context.\n",
    "\"\"\"\n",
    "context_precision_with_reference = NonLLMContextPrecisionWithReference(threshold=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Context Recall**\n",
    "\n",
    "### TL;DR:\n",
    "> **How many of the relevant chunks were actually retrieved? Did we miss relevant chunks?**\n",
    "\n",
    "---\n",
    "\n",
    "### **Defition**:\n",
    "\n",
    "**Context Recall** measures how many of the **relevant documents (or pieces of information)** were successfully retrieved. This metric ensures that the retrieval process does not **miss important information** that could be used to generate a response. A **higher recall** means that fewer relevant documents were left out, making it crucial for applications that prioritize completeness over precision. Since recall focuses on **not missing relevant data**, it always requires a reference set for comparison.\n",
    "\n",
    "---\n",
    "\n",
    "### LLM-Based Context Recall\n",
    "\n",
    "This metric evaluates recall by analyzing the **claims** in the reference (expected) response and checking whether they can be attributed to the retrieved context. In an ideal scenario, **all claims** in the reference answer should be **supported** by the retrieved context.\n",
    "\n",
    "### **Formula**:\n",
    "\n",
    "**Context Recall** = $\\frac{\\text{Number of claims in the reference supported by the retrieved context}}{\\text{Total number of claims in the reference}}$\n",
    "\n",
    "A recall score **closer to 1** indicates that most relevant data has been successfully retrieved, while a **lower recall** means that key information was missed.\n",
    "\n",
    "### **Non-LLM-Based Context Recall**\n",
    "\n",
    "### **Definition**:\n",
    "\n",
    "**Non-LLM-Based Context Recall** measures how many of the **relevant documents (or pieces of information)** were successfully retrieved **without relying on a language model** for evaluation. This metric ensures that the retrieval process does not **miss important information** that could be used to generate a response. A **higher recall** means that fewer relevant documents were left out, making it crucial for applications that prioritize completeness over precision. Since recall focuses on **not missing relevant data**, it always requires a reference set for comparison.\n",
    "\n",
    "---\n",
    "\n",
    "### **How It Works**\n",
    "This metric calculates recall based on **string similarity** rather than using an LLM to assess the retrieved content. It follows these steps:\n",
    "\n",
    "1. **Extract Retrieved and Reference Contexts**  \n",
    "   - Uses `retrieved_contexts` (what was fetched) and `reference_contexts` (ground truth).  \n",
    "2. **Compute Similarity Scores**  \n",
    "   - Compares each **retrieved context** against each **reference context** using a similarity function.\n",
    "3. **Find the Best Match for Each Reference Context**  \n",
    "   - For each **reference context**, it selects the **retrieved context** with the **highest similarity score**.\n",
    "4. **Apply a Threshold (`0.5` by default)**  \n",
    "   - If the similarity score is **above the threshold**, the reference context is considered **retrieved successfully**.\n",
    "5. **Compute Recall Score**  \n",
    "   - The recall score is the proportion of reference contexts that have at least one matching retrieved context **above the threshold**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Formula**\n",
    "\n",
    "**Non-LLM-Based Context Recall** =  $\\frac{\\text{Number of reference contexts with a high-scoring retrieved match}}{\\text{Total number of reference contexts}} $\n",
    "\n",
    "Where:\n",
    "- A **retrieved match** is valid if its **similarity score > threshold**.\n",
    "- The **threshold** is configurable (default = `0.5`).\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Calculation**\n",
    "#### **Given:**\n",
    "- `retrieved_contexts = [\"text A\", \"text B\", \"text C\"]`\n",
    "- `reference_contexts = [\"text X\", \"text Y\"]`\n",
    "- **Similarity scores:**  \n",
    "  - `text A` â†” `text X` = **0.7** âœ…\n",
    "  - `text B` â†” `text X` = **0.4** âŒ\n",
    "  - `text C` â†” `text X` = **0.6** âœ… (max = 0.7)\n",
    "  - `text A` â†” `text Y` = **0.2** âŒ\n",
    "  - `text B` â†” `text Y` = **0.5** âœ…\n",
    "  - `text C` â†” `text Y` = **0.3** âŒ (max = 0.5)\n",
    "\n",
    "#### **Final Score Computation**\n",
    "- `text X` has a match (`0.7 > 0.5`) â†’ âœ… **1**\n",
    "- `text Y` has a match (`0.5 == 0.5`) â†’ âœ… **1**\n",
    "- **Score = (1+1) / 2 = 1.0 (100%)** âœ…\n",
    "\n",
    "---\n",
    "\n",
    "### **Why is This Useful?**\n",
    "- **Evaluates Retriever Performance Without an LLM** â†’ Ensures relevance using **string similarity**, avoiding model biases.\n",
    "- **Helps Optimize RAG Pipelines** â†’ Higher recall ensures more **comprehensive** context retrieval.\n",
    "- **Works Well with Threshold-Based Comparisons** â†’ Adjustable strictness for different applications.\n",
    "\n",
    "A recall score **closer to 1** means that most relevant contexts were retrieved, while a **lower recall** suggests that key information was missed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    LLMContextRecall,\n",
    "    NonLLMContextRecall   \n",
    ")\n",
    "\n",
    "# Uses the user_input, reference and the retrieved_contexts columns\n",
    "# Reference is used instead of reference_contexts since annotating the reference_contexts is time-consuming\n",
    "llm_context_recall = LLMContextRecall()\n",
    "\n",
    "# Uses the response- and the retrieved_contexts columns, by performing a semantic similarity check\n",
    "context_recall = NonLLMContextRecall(threshold=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Context Entities Recall**\n",
    "\n",
    "### TL;DR:\n",
    "> **It is a measure of what fraction of entities are recalled from reference**\n",
    "\n",
    "---\n",
    "\n",
    "### Definition:\n",
    "`ContextEntityRecall` metric measures the recall of the retrieved context, based on the number of entities present in both `reference` and `retrieved_contexts`, relative to the total number of entities in the `reference`. In simple terms, it evaluates how well the retrieved contexts capture the entities from the original reference.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula:\n",
    "\n",
    "To compute this metric, we define two sets:\n",
    "\n",
    "- **RE**: The set of entities in the reference.\n",
    "- **RCE**: The set of entities in the retrieved contexts.\n",
    "\n",
    "We determine the number of entities common to both sets (**RCE** $\\cap$  **RE**) and divide it by the total number of entities in the reference (**RE**). The formula is:\n",
    "\n",
    "$ \\text{Context Entity Recall} = \\frac{\\text{Number of common entities between RCE and RE}}{\\text{Total number of entities in RE}} = \\frac{\\text{RCE } \\cap \\text{ RE}}{\\text{RE}} $\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Calculation**\n",
    "\n",
    "#### **Scenario:**\n",
    "\n",
    "We have a **reference** and two retrieved contexts:\n",
    "\n",
    "**Reference:**\n",
    "> \"The Taj Mahal is an ivory-white marble mausoleum on the right bank of the river Yamuna in the Indian city of Agra. It was commissioned in 1631 by the Mughal emperor Shah Jahan to house the tomb of his favorite wife, Mumtaz Mahal.\"\n",
    "\n",
    "**Retrieved Contexts:**\n",
    "\n",
    "- **High Entity Recall Context:**\n",
    "  > \"The Taj Mahal is a symbol of love and architectural marvel located in Agra, India. It was built by the Mughal emperor Shah Jahan in memory of his beloved wife, Mumtaz Mahal. The structure is renowned for its intricate marble work and beautiful gardens surrounding it.\"\n",
    "\n",
    "- **Low Entity Recall Context:**\n",
    "  > \"The Taj Mahal is an iconic monument in India. It is a UNESCO World Heritage Site and attracts millions of visitors annually. The intricate carvings and stunning architecture make it a must-visit destination.\"\n",
    "\n",
    "#### **Step 1: Extract Entities**\n",
    "\n",
    "Entities in the **Reference (RE)**:\n",
    "> \\[\"Taj Mahal\", \"Yamuna\", \"Agra\", \"1631\", \"Shah Jahan\", \"Mumtaz Mahal\"\\]\n",
    "\n",
    "Entities in **High Recall Context (RCE1)**:\n",
    "> \\[\"Taj Mahal\", \"Agra\", \"Shah Jahan\", \"Mumtaz Mahal\", \"India\"\\]\n",
    "\n",
    "Entities in **Low Recall Context (RCE2)**:\n",
    "> \\[\"Taj Mahal\", \"UNESCO\", \"India\"\\]\n",
    "\n",
    "#### **Step 2: Compute Context Entity Recall**\n",
    "\n",
    "For **High Recall Context (RCE1)**:\n",
    "$ \\text{Context Entity Recall} = \\frac{4}{6} = 0.67 $ \n",
    "\n",
    "For **Low Recall Context (RCE2)**:\n",
    "$ \\text{Context Entity Recall} = \\frac{1}{6} = 0.17 $\n",
    "\n",
    "Since the first context retains more entities from the reference, it has a **higher entity recall**, indicating it is **more comprehensive** in capturing the essential information.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Insights:**\n",
    "\n",
    "* **Higher Entity Recall Improves Answer Completeness:** If entities are important for context, a higher recall ensures critical information is included in the retrieved context.\n",
    "\n",
    "* **Useful for Fact-Heavy Applications:** Applications in **legal, medical, and historical domains** benefit significantly from high entity recall.\n",
    "\n",
    "* **Balances with Context Precision:** While high recall is beneficial, retrieving too many irrelevant entities can introduce noise. **Optimizing both recall and precision** is crucial for effective retrieval in RAG systems.\n",
    "\n",
    "* **Comparison of Retrieval Mechanisms:** If two retrieval mechanisms fetch different contexts, **Context Entity Recall** helps determine which one is better at preserving key entities from the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import ContextEntityRecall\n",
    "\n",
    "# Uses the reference and retrieved_context columns\n",
    "context_entity_recall = ContextEntityRecall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Noise Sensitivity**\n",
    "\n",
    "### TL;DR:\n",
    "> **How prone is the system to generating incorrect claims from retrieved contexts?**\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition**:\n",
    "**Noise Sensitivity** measures how often a system makes errors by providing incorrect responses when utilizing either relevant or irrelevant retrieved documents. This metric evaluates the robustness of a Retrieval-Augmented Generation (RAG) system against potentially misleading or noisy information.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conceptual Insight**:\n",
    "The metric fundamentally tests how easily an LLM can be \"tricked\" into generating factually incorrect responses. It distinguishes between two critical scenarios:\n",
    "\n",
    "1. **Relevant Context Noise**: \n",
    "   - More subtle and dangerous\n",
    "   - Noise is camouflaged within seemingly pertinent information\n",
    "   - High risk of inadvertently incorporating incorrect claims\n",
    "\n",
    "2. **Irrelevant Context Noise**:\n",
    "   - A robust LLM should completely resist this\n",
    "   - Contexts unrelated to the user's query\n",
    "   - Zero tolerance for incorporating unrelated information\n",
    "\n",
    "---\n",
    "\n",
    "### LLM-Based Noise Sensitivity\n",
    "\n",
    "The metric assesses noise sensitivity by decomposing the response and reference into individual claims, then analyzing:\n",
    "- Whether claims are correct according to the ground truth\n",
    "- Whether incorrect claims can be attributed to retrieved contexts (either relevant or irrelevant)\n",
    "\n",
    "---\n",
    "\n",
    "### **Calculation Approach**:\n",
    "\n",
    "1. **Decompose Statements**: Break down the reference and response into individual claims using an LLM evaluator\n",
    "2. **Context Verification**: Check if claims can be supported by retrieved contexts using NLI techniques\n",
    "3. **Identify Relevant Contexts**: Determine which contexts support the ground truth\n",
    "4. **Map Response Claims**: Map each claim in the response to contexts that support it\n",
    "5. **Identify Incorrect Claims**: Determine which response claims are unsupported by the ground truth\n",
    "6. **Calculate Sensitivity**: Compute the proportion of incorrect claims derived from contexts\n",
    "\n",
    "---\n",
    "\n",
    "### **Formula**:\n",
    "\n",
    "Let's define the following:\n",
    "- $S_r$ = Set of response statements\n",
    "- $S_i$ = Set of incorrect statements in the response (not supported by ground truth)\n",
    "- $C_r$ = Set of relevant contexts (contexts that support ground truth)\n",
    "- $C_i$ = Set of irrelevant contexts (contexts that don't support ground truth)\n",
    "- $f(s,c)$ = Function that returns 1 if statement $s$ can be inferred from context $c$, 0 otherwise\n",
    "\n",
    "For each statement $s$ in the response, we can define:\n",
    "- $\\text{relevant\\_faithful}(s) = \\max_{c \\in C_r} f(s,c)$ (1 if statement can be inferred from any relevant context)\n",
    "- $\\text{irrelevant\\_faithful}(s) = \\max_{c \\in C_i} f(s,c) \\cdot (1 - \\text{relevant\\_faithful}(s))$ (1 if statement can be inferred from irrelevant context and not from any relevant context)\n",
    "\n",
    "Then:\n",
    "\n",
    "**Relevant Mode Noise Sensitivity**:\n",
    "$$\\text{NS}_{\\text{relevant}} = \\frac{\\sum_{s \\in S_i} \\text{relevant\\_faithful}(s)}{|S_r|}$$\n",
    "\n",
    "**Irrelevant Mode Noise Sensitivity**:\n",
    "$$\\text{NS}_{\\text{irrelevant}} = \\frac{\\sum_{s \\in S_i} \\text{irrelevant\\_faithful}(s)}{|S_r|}$$\n",
    "\n",
    "Where $|S_r|$ is the total number of statements in the response.\n",
    "\n",
    "In plain English:\n",
    "- Relevant mode: Proportion of response statements that are both incorrect and supported by relevant contexts\n",
    "- Irrelevant mode: Proportion of response statements that are both incorrect and supported only by irrelevant contextst\n",
    "\n",
    "A score **closer to 0** indicates better performance, suggesting:\n",
    "- Fewer incorrect claims\n",
    "- Less influence from noisy or irrelevant contexts\n",
    "- More robust response generation\n",
    "\n",
    "---\n",
    "\n",
    "### **Modes**:\n",
    "- **Relevant Mode** (default): Focuses on noise in relevant contexts\n",
    "- **Irrelevant Mode**: Analyzes noise from irrelevant retrieved documents\n",
    "\n",
    "---\n",
    "\n",
    "### **Evaluation Expectations**:\n",
    "- **Relevant Context**: Some noise tolerance, but should be minimal\n",
    "- **Irrelevant Context**: Virtually zero noise should be incorporated\n",
    "  - A high-quality LLM should completely disregard irrelevant information\n",
    "  - Any noise from irrelevant contexts indicates a significant vulnerability\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Scenario**:\n",
    "Consider a query about the Life Insurance Corporation of India (LIC):\n",
    "- **Question**: \"What is the Life Insurance Corporation of India (LIC) known for?\"\n",
    "- **Reference**: \"The Life Insurance Corporation of India (LIC) is the largest insurance company in India, established in 1956 through the nationalization of the insurance industry. It is known for managing a large portfolio of investments.\"\n",
    "- **Response**: \"The Life Insurance Corporation of India (LIC) is the largest insurance company in India, known for its vast portfolio of investments. LIC contributes to the financial stability of the country.\"\n",
    "- **Retrieved Contexts**: Mix of relevant information about LIC and irrelevant information about the Indian economy\n",
    "- **Noise Sensitivity Score**: 0.33 (one incorrect claim out of three total claims)\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Insights**:\n",
    "- Helps identify system vulnerabilities to hallucination\n",
    "- Provides a quantitative measure of response reliability\n",
    "- Distinguishes between subtle (relevant) and gross (irrelevant) information distortions\n",
    "- Serves as a critical component in comprehensive RAG system evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import NoiseSensitivity\n",
    "\n",
    "# Uses the user_input, reference, response, and the retrieved_contexts columns\n",
    "noise_sensitivity_mode_relevant = NoiseSensitivity(name=\"noise_sensitivity_mode_relevant\")\n",
    "noise_sensitivity_mode_irrelevant = NoiseSensitivity(name=\"noise_sensitivity_mode_irrelevant\", mode=\"irrelevant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Response Relevancy**\n",
    "\n",
    "### TL;DR:\n",
    "> **How well does the answer address the original user query?**\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition**:\n",
    "**Response Relevancy** measures how closely an AI-generated response aligns with the original user input. It evaluates the answer's ability to directly and appropriately address the user's question, penalizing responses that are incomplete, off-topic, or include unnecessary details. It doesn't judge the **factual accuracy**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conceptual Insight**:\n",
    "The metric focuses on the semantic alignment between:\n",
    "- The original user query\n",
    "- The generated response\n",
    "\n",
    "Key evaluation criteria:\n",
    "1. **Direct Address**: Does the answer directly tackle the user's question?\n",
    "2. **Information Completeness**: Does the response provide sufficient information?\n",
    "3. **Topic Coherence**: Does the answer stay focused on the query's intent?\n",
    "\n",
    "---\n",
    "\n",
    "### **Calculation Approach**:\n",
    "\n",
    "1. **Question Generation**: \n",
    "   - Use the LLM to generate artificial questions based on the response\n",
    "   - Default is to create 3 variant questions\n",
    "   - These questions should capture the essence of the response\n",
    "\n",
    "2. **Semantic Similarity**:\n",
    "   - Compute cosine similarity between:\n",
    "     - Original user input embedding\n",
    "     - Embeddings of generated questions\n",
    "   - Measures how closely the questions match the original query\n",
    "\n",
    "3. **Scoring**:\n",
    "   - Average the cosine similarity scores\n",
    "   - Higher scores indicate better relevance\n",
    "   - Scores typically range between 0 and 1\n",
    "\n",
    "---\n",
    "\n",
    "### **Formula**:\n",
    "\n",
    "**Response Relevancy** = $\\frac{\\sum_{i=1}^{N} \\text{cosine\\_similarity}(E_{g_{i}}, E_{o})}{N}$ \n",
    "\n",
    "Where:\n",
    "- $E_{g_{i}}$: Embedding of the i-th generated question\n",
    "- $E_{o}$: Embedding of the original user input\n",
    "- N: Number of generated questions (default: 3)\n",
    "\n",
    "---\n",
    "\n",
    "### **Evaluation Expectations**:\n",
    "- **High Score (Close to 1)**: \n",
    "  - Answer directly addresses the query\n",
    "  - Comprehensive and focused response\n",
    "  - Minimal irrelevant information\n",
    "\n",
    "- **Low Score (Close to 0)**: \n",
    "  - Response is off-topic\n",
    "  - Incomplete or evasive answer\n",
    "  - Includes excessive unrelated details\n",
    "\n",
    "---\n",
    "\n",
    "### **Important Limitations**:\n",
    "- **No Factuality Check**: \n",
    "  - Measures relevance, not accuracy\n",
    "  - Does not verify the truthfulness of the response\n",
    "- **Embedding-Based**: \n",
    "  - Relies on semantic similarity\n",
    "  - May not catch nuanced relevance\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Scenario**:\n",
    "- **User Query**: \"When was the first Super Bowl?\"\n",
    "- **Good Response**: \"The first Super Bowl was held on Jan 15, 1967, between the Green Bay Packers and Kansas City Chiefs.\"\n",
    "- **Poor Response**: \"Football is a popular sport in the United States with many interesting historical moments.\"\n",
    "\n",
    "### **Key Insights**:\n",
    "- Helps evaluate response quality beyond simple keyword matching\n",
    "- Provides a quantitative measure of semantic alignment\n",
    "- Supports improving AI system's query understanding\n",
    "- Identifies potential issues with off-topic or unfocused responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import ResponseRelevancy\n",
    "\n",
    "# Uses the user_input and response colummns\n",
    "response_relevancy = ResponseRelevancy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Faithfulness**\n",
    "\n",
    "### TL;DR:\n",
    "> **How factually consistent is the response with the retrieved context?**\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition**:\n",
    "The **Faithfulness** metric measures how factually consistent a response is with the retrieved context. It ranges from 0 to 1, with higher scores indicating better consistency.\n",
    "\n",
    "A response is considered faithful if all its claims can be supported by the retrieved context.\n",
    "\n",
    "---\n",
    "\n",
    "### **Calculation Approach**:\n",
    "\n",
    "1. **Identify Claims**: \n",
    "   - Break down the response into individual statements\n",
    "   - Examine each claim systematically\n",
    "\n",
    "2. **Context Verification**:\n",
    "   - Check each claim to see if it can be inferred from the retrieved context\n",
    "   - Determine the support level of each statement\n",
    "\n",
    "3. **Scoring**:\n",
    "   - Compute the faithfulness score using the formula:\n",
    "     \n",
    "     $\\text{Faithfulness Score} = \\frac{\\text{Number of claims supported by the retrieved context}}{\\text{Total number of claims in the response}}$\n",
    "\n",
    "---\n",
    "\n",
    "### **Example**:\n",
    "\n",
    "**Question**: Where and when was Einstein born?\n",
    "\n",
    "**Context**: Albert Einstein (born 14 March 1879) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time.\n",
    "\n",
    "**High Faithfulness Answer**: Einstein was born in Germany on 14th March 1879.\n",
    "\n",
    "**Low Faithfulness Answer**: Einstein was born in Germany on 20th March 1879.\n",
    "\n",
    "#### Calculation Steps:\n",
    "- **Step 1**: Break the generated answer into individual statements\n",
    "- **Step 2**: Verify if each statement can be inferred from the given context\n",
    "- **Step 3**: Apply the faithfulness formula\n",
    "\n",
    "---\n",
    "\n",
    "### **Evaluation Expectations**:\n",
    "- **Perfect Score (1.0)**: \n",
    "  - All claims are directly supported by the context\n",
    "  - No extraneous or unsupported information\n",
    "\n",
    "- **Partial Score**: \n",
    "  - Some claims are supported\n",
    "  - Partial consistency with the retrieved context\n",
    "\n",
    "- **Low Score (Close to 0)**: \n",
    "  - Most claims cannot be verified\n",
    "  - Significant deviation from the original context\n",
    "\n",
    "---\n",
    "\n",
    "### **Advanced Verification**:\n",
    "**HHEM-2.1-Open** can be used as a classifier model to:\n",
    "- Detect hallucinations in LLM-generated text\n",
    "- Cross-check claims with the given context\n",
    "- Efficiently determine claim inferability\n",
    "- Avoid **biases** by not using `LLM-As-A-Judge`\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Insights**:\n",
    "- Measures the factual consistency of AI-generated responses\n",
    "- Helps identify potential hallucinations or fabrications\n",
    "- Provides a quantitative assessment of contextual alignment\n",
    "- Supports improving the reliability of AI-generated content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type HHEMv2Config to instantiate a model of type HHEMv2. This is not supported for all configurations of models and can yield errors.\n",
      "/home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import FaithfulnesswithHHEM\n",
    "\n",
    "# Uses the user_input,  response and retrieved_contexts columns\n",
    "faithfulness = FaithfulnesswithHHEM(device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Answer Accuracy**\n",
    "\n",
    "### TL;DR:\n",
    "> **How closely does a model's response match the reference ground truth?**\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition**:\n",
    "**Answer Accuracy** measures the agreement between a model's response and a reference ground truth for a given question. This is done via two distinct \"LLM-as-a-judge\" prompts that each return a rating (0, 2, or 4). The metric converts these ratings into a [0,1] scale and then takes the average of the two scores from the judges. Higher scores indicate that the model's answer closely matches the reference. This dual-perspective approach helps ensure a more balanced and robust evaluation by considering the alignment from both directions. It mitigates potential biases that might exist when evaluating in only one direction.\n",
    "\n",
    "---\n",
    "\n",
    "### **Rating Scale**:\n",
    "* **0** â†’ The **response** is inaccurate or does not address the same question as the **reference**.\n",
    "* **2** â†’ The **response** partially aligns with the **reference**.\n",
    "* **4** â†’ The **response** exactly aligns with the **reference**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Calculation Approach**:\n",
    "\n",
    "1. **Dual-Perspective Evaluation**: \n",
    "   - **Template 1**: The LLM compares the **response** with the **reference** and rates it on a scale of **0, 2, or 4**.\n",
    "   - **Template 2**: The LLM evaluates the same comparison again, but with the roles of **response** and **reference** swapped.\n",
    "\n",
    "2. **Score Processing**:\n",
    "   - Each rating is converted to a [0,1] scale by dividing by 4\n",
    "   - For invalid scores, NaN (Not a Number) is returned\n",
    "\n",
    "3. **Score Aggregation**:\n",
    "   - If both scores are valid: Calculate the average of both scores\n",
    "   - If one score is invalid: Use the valid score\n",
    "   - If both scores are invalid: Return NaN\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Calculation**:\n",
    "* **User Input**: \"When was Einstein born?\"\n",
    "* **Response**: \"Albert Einstein was born in 1879.\"\n",
    "* **Reference**: \"Albert Einstein was born in 1879.\"\n",
    "\n",
    "Assuming both templates return a rating of **4** (indicating exact alignment):\n",
    "* A rating of **4** corresponds to **1** on the [0,1] scale.\n",
    "* Averaging the two scores: (1 + 1) / 2 = **1**.\n",
    "\n",
    "Thus, the final **Answer Accuracy** score is **1**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Handling Additional Information**:\n",
    "The **dual-perspective approach** has important implications for **responses containing additional information beyond the reference**:\n",
    "\n",
    "Scenario: Response is fully accurate but contains extra details not in the reference\n",
    "\n",
    "* First Evaluation (Response â†’ Reference): High score (e.g., 1.0) - The response contains all reference information\n",
    "* Second Evaluation (Reference â†’ Response): Lower score (e.g., 0.5) - The reference doesn't contain all response details\n",
    "* Final Score: Average of both (e.g., 0.75) - Lower than if only using the first perspective\n",
    "\n",
    "This reveals a key characteristic: the metric **implicitly penalizes responses that go beyond the scope of the reference**, even when the core answer is correct.\n",
    "\n",
    "#### Design Philosophy:\n",
    "\n",
    "* Precision Emphasis: Values concise, targeted answers\n",
    "* Scope Alignment: Rewards responses that address exactly what was asked without additional information\n",
    "* Bi-directional Matching: Ensures both completeness and relevance\n",
    "\n",
    "#### Considerations:\n",
    "\n",
    "* Beneficial: In factual QA systems where conciseness and precision are valued\n",
    "* Limiting: In educational contexts where additional context enhances understanding\n",
    "* Alternative: If only evaluating whether the response contains the correct answer (regardless of extra information), consider using only the first perspective\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Implementation Details**:\n",
    "- Retry mechanism for handling invalid scores\n",
    "- Temperature set to 0.10 for consistent evaluations\n",
    "- Error handling to ensure robustness\n",
    "- Score conversion from [0,4] scale to [0,1] scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LangchainLLMWrapper' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnest_asyncio\u001b[39;00m\n\u001b[32m      8\u001b[39m nest_asyncio.apply()\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m answer_accuracy.single_turn_ascore(\n\u001b[32m     11\u001b[39m     sample=evaluation_dataset.samples[\u001b[32m0\u001b[39m],   \n\u001b[32m     12\u001b[39m     timeout=\u001b[32m300\u001b[39m\n\u001b[32m     13\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/eval/lib/python3.12/site-packages/ragas/metrics/base.py:526\u001b[39m, in \u001b[36mSingleTurnMetric.single_turn_ascore\u001b[39m\u001b[34m(self, sample, callbacks, timeout)\u001b[39m\n\u001b[32m    524\u001b[39m callbacks = callbacks \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[32m    525\u001b[39m \u001b[38;5;66;03m# only get the required columns\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m sample = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_only_required_columns_single_turn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m rm, group_cm = new_group(\n\u001b[32m    528\u001b[39m     \u001b[38;5;28mself\u001b[39m.name,\n\u001b[32m    529\u001b[39m     inputs=sample.to_dict(),\n\u001b[32m    530\u001b[39m     callbacks=callbacks,\n\u001b[32m    531\u001b[39m     metadata={\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: ChainType.METRIC},\n\u001b[32m    532\u001b[39m )\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/eval/lib/python3.12/site-packages/ragas/metrics/base.py:454\u001b[39m, in \u001b[36mSingleTurnMetric._only_required_columns_single_turn\u001b[39m\u001b[34m(self, sample)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_only_required_columns_single_turn\u001b[39m(\n\u001b[32m    449\u001b[39m     \u001b[38;5;28mself\u001b[39m, sample: SingleTurnSample\n\u001b[32m    450\u001b[39m ) -> SingleTurnSample:\n\u001b[32m    451\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    452\u001b[39m \u001b[33;03m    Simplify the sample to only include the required columns.\u001b[39;00m\n\u001b[32m    453\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m     required_columns = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_required_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwith_optional\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m.get(\n\u001b[32m    455\u001b[39m         MetricType.SINGLE_TURN.name, \u001b[38;5;28mset\u001b[39m()\n\u001b[32m    456\u001b[39m     )\n\u001b[32m    457\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m required_columns:\n\u001b[32m    458\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m sample\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/eval/lib/python3.12/site-packages/ragas/metrics/base.py:115\u001b[39m, in \u001b[36mMetric.get_required_columns\u001b[39m\u001b[34m(self, with_optional)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m with_optional:\n\u001b[32m    113\u001b[39m     \u001b[38;5;66;03m# get all the required columns with optional columns, remove the optional suffix\u001b[39;00m\n\u001b[32m    114\u001b[39m     required_columns = {}\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_required_columns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m():\n\u001b[32m    116\u001b[39m         \u001b[38;5;66;03m# if any column ends with \":optional\", add it to the required columns after removing the suffix\u001b[39;00m\n\u001b[32m    117\u001b[39m         required_columns[k.name] = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m    118\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m v:\n",
      "\u001b[31mAttributeError\u001b[39m: 'LangchainLLMWrapper' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import AnswerAccuracy\n",
    "\n",
    "# Uses the user_input, response and reference columns\n",
    "answer_accuracy = AnswerAccuracy(llm)\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "response = await answer_accuracy.single_turn_ascore(\n",
    "    sample=evaluation_dataset.samples[0],   \n",
    "    timeout=300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Context Relevance**\n",
    "\n",
    "### TL;DR:\n",
    "> **How well do the retrieved contexts align with the user's query?**\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition**:\n",
    "**Context Relevance** evaluates whether the **retrieved_contexts** (chunks or passages) are pertinent to the **user_input**. This is done via two independent \"LLM-as-a-judge\" prompt calls that each rate the relevance on a scale of **0, 1, or 2**. The ratings are then converted to a [0,1] scale and averaged to produce the final score. Higher scores indicate that the contexts are more closely aligned with the user's query.\n",
    "\n",
    "---\n",
    "\n",
    "### **Rating Scale**:\n",
    "* **0** â†’ The retrieved contexts are not relevant to the user's query at all.\n",
    "* **1** â†’ The contexts are partially relevant.\n",
    "* **2** â†’ The contexts are completely relevant.\n",
    "\n",
    "---\n",
    "\n",
    "### **Calculation Approach**:\n",
    "\n",
    "1. **Dual-Perspective Evaluation**: \n",
    "   - The LLM is prompted with two distinct templates (template_relevance1 and template_relevance2) to evaluate the relevance of the retrieved contexts\n",
    "   - Each template returns a relevance rating of **0**, **1**, or **2**\n",
    "   - Using two templates increases robustness and reduces potential bias\n",
    "\n",
    "2. **Score Normalization**:\n",
    "   - Each rating is normalized to a [0,1] scale by dividing by 2\n",
    "   - This creates a standardized scale for comparison\n",
    "\n",
    "3. **Score Aggregation**:\n",
    "   - If both ratings are valid: Calculate the average of the normalized values\n",
    "   - If only one rating is valid: Use that score as the final result\n",
    "   - If no valid ratings: Return NaN (Not a Number)\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Calculation**:\n",
    "* **User Input**: \"When and Where Albert Einstein was born?\"\n",
    "* **Retrieved Contexts**:\n",
    "  * \"Albert Einstein was born March 14, 1879.\"\n",
    "  * \"Albert Einstein was born at Ulm, in WÃ¼rttemberg, Germany.\"\n",
    "\n",
    "In this example, the two retrieved contexts together fully address the user's query by providing both the birth date and location of Albert Einstein.\n",
    "\n",
    "**Evaluation Process**:\n",
    "1. First template rates the contexts as **2** (fully relevant)\n",
    "2. Second template also rates the contexts as **2** (fully relevant)\n",
    "3. Normalizing: 2/2 = **1.0** for each rating\n",
    "4. Averaging: (1.0 + 1.0)/2 = **1.0**\n",
    "\n",
    "Thus, the final **Context Relevance** score is **1.0**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Handling Partial Relevance**:\n",
    "\n",
    "The Context Relevance metric can effectively differentiate between different levels of relevance:\n",
    "\n",
    "* **Highly Relevant Context** (Score near 1.0):\n",
    "  * Directly addresses all aspects of the user's query\n",
    "  * Contains specific information needed to formulate a complete answer\n",
    "\n",
    "* **Partially Relevant Context** (Score around 0.5):\n",
    "  * Addresses some aspects of the query but not others\n",
    "  * Contains tangentially related information\n",
    "  * May require additional context to fully answer the query\n",
    "\n",
    "* **Irrelevant Context** (Score near 0.0):\n",
    "  * Contains information unrelated to the query\n",
    "  * Focuses on different aspects or topics entirely\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Implementation Considerations**:\n",
    "\n",
    "- The metric evaluates the collective relevance of all provided contexts\n",
    "- Multiple contexts can complement each other to achieve a higher score\n",
    "- The scoring is query-focused rather than response-focused\n",
    "- Using two independent judgments increases reliability and reduces bias\n",
    "- The metric is lightweight and token-efficient\n",
    "- Lower temperatures in LLM calls (e.g., 0.10) help ensure consistent evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import ContextRelevance\n",
    "\n",
    "# Uses the user_input and retrieved_contexts columns\n",
    "context_relevance = ContextRelevance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response Groundedness\n",
    "\n",
    "### TL;DR:\n",
    "> **How well is the response supported by the retrieved contexts?**\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition**:\n",
    "**Response Groundedness** measures how well a response is supported or \"grounded\" by the retrieved contexts. It assesses whether each claim in the response can be found, either wholly or partially, in the provided contexts. This is evaluated via two independent \"LLM-as-a-judge\" prompt calls that each rate the groundedness on a scale of **0, 1, or 2**. The ratings are then converted to a [0,1] scale and averaged to produce the final score. Higher scores indicate that the response is more thoroughly supported by the retrieved contexts.\n",
    "\n",
    "---\n",
    "\n",
    "### **Rating Scale**:\n",
    "* **0** â†’ The response is not grounded in the context at all.\n",
    "* **1** â†’ The response is partially grounded.\n",
    "* **2** â†’ The response is fully grounded (every statement can be found or inferred from the retrieved context).\n",
    "\n",
    "---\n",
    "\n",
    "### **Calculation Approach**:\n",
    "\n",
    "1. **Dual-Perspective Evaluation**: \n",
    "   - The LLM is prompted with two distinct templates to evaluate the grounding of the response\n",
    "   - Each template returns a groundedness rating of **0**, **1**, or **2**\n",
    "   - Using two templates increases robustness and reduces potential bias\n",
    "\n",
    "2. **Score Normalization**:\n",
    "   - Each rating is normalized to a [0,1] scale by dividing by 2\n",
    "   - This creates a standardized scale for comparison\n",
    "\n",
    "3. **Score Aggregation**:\n",
    "   - If both ratings are valid: Calculate the average of the normalized values\n",
    "   - If only one rating is valid: Use that score as the final result\n",
    "   - If no valid ratings: Return NaN (Not a Number)\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Calculation**:\n",
    "* **Response**: \"Albert Einstein was born in 1879.\"\n",
    "* **Retrieved Contexts**:\n",
    "  * \"Albert Einstein was born March 14, 1879.\"\n",
    "  * \"Albert Einstein was born at Ulm, in WÃ¼rttemberg, Germany.\"\n",
    "\n",
    "In this example, the response's claim is supported by the first context.\n",
    "\n",
    "**Evaluation Process**:\n",
    "1. First template rates the response as **2** (fully grounded)\n",
    "2. Second template also rates the response as **2** (fully grounded)\n",
    "3. Normalizing: 2/2 = **1.0** for each rating\n",
    "4. Averaging: (1.0 + 1.0)/2 = **1.0**\n",
    "\n",
    "Thus, the final **Response Groundedness** score is **1.0**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Handling Additional/Incompatible Contexts**:\n",
    "\n",
    "An important feature of Response Groundedness is that it only evaluates whether the response is supported by the contexts, not whether all contexts are utilized:\n",
    "\n",
    "* **Additional Contexts**: If the system retrieves contexts that are not used in the response, the groundedness score is **not penalized**. The metric only checks if the claims in the response are supported by at least some of the retrieved contexts.\n",
    "\n",
    "* **Incompatible Contexts**: Similarly, if some retrieved contexts contain information that contradicts other contexts, the response is still considered grounded as long as each of its claims is supported by at least one of the provided contexts.\n",
    "\n",
    "* **Partial Context Usage**: A response can achieve a high groundedness score even if it only utilizes a small subset of the retrieved contexts, as long as that subset fully supports all claims made.\n",
    "\n",
    "---\n",
    "\n",
    "### **Handling Partial Groundedness**:\n",
    "\n",
    "The Response Groundedness metric can effectively differentiate between different levels of support:\n",
    "\n",
    "* **Fully Grounded Response** (Score near 1.0):\n",
    "  * Every claim in the response is directly supported by the contexts\n",
    "  * No unsupported assertions or extrapolations\n",
    "\n",
    "* **Partially Grounded Response** (Score around 0.5):\n",
    "  * Some claims are supported while others lack contextual evidence\n",
    "  * Includes reasonable inferences that go slightly beyond the contexts\n",
    "  * Contains minor embellishments not directly stated in contexts\n",
    "\n",
    "* **Ungrounded Response** (Score near 0.0):\n",
    "  * Contains claims with no support in the retrieved contexts\n",
    "  * Presents information contradicting the provided contexts\n",
    "  * Fabricates details entirely absent from the contexts\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Implementation Considerations**:\n",
    "\n",
    "- The metric evaluates whether the response is supported by the contexts, not whether all contexts are utilized\n",
    "- Contexts that are not used in the response do not affect the score\n",
    "- Using two independent judgments increases reliability and reduces bias\n",
    "- The metric is lightweight and token-efficient\n",
    "- Lower temperatures in LLM calls help ensure consistent evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import ResponseGroundedness\n",
    "\n",
    "# Uses the retrieved_context and response columns\n",
    "response_groundedness = ResponseGroundedness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Factual Correctness**\n",
    "\n",
    "### TL;DR:\n",
    "> **How factually consistent is a model's response with the reference ground truth?**\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition**:\n",
    "**Factual Correctness** measures the degree to which a model's response aligns with the reference by assessing the factual consistency between the two. It does this by breaking down both the response and reference into distinct claims and then determining whether these claims match using natural language inference (NLI). This metric helps evaluate how accurately the generated response retains factual integrity.\n",
    "\n",
    "The factual correctness score ranges from **0 to 1**, where higher values indicate better factual alignment.\n",
    "\n",
    "---\n",
    "\n",
    "### **Claim Breakdown and Classification**:\n",
    "This metric identifies three types of claims:\n",
    "- **True Positives (TP):** Claims that are present in both the response and the reference.\n",
    "- **False Positives (FP):** Claims that are in the response but not supported by the reference.\n",
    "- **False Negatives (FN):** Claims that are in the reference but missing from the response.\n",
    "\n",
    "---\n",
    "\n",
    "### **Calculation Modes**:\n",
    "The metric can be computed in three different modes:\n",
    "\n",
    "#### **Precision Mode:**\n",
    "> Measures how much of the information in the response is factually correct.\n",
    "\n",
    "$ \\text{Precision} = \\frac{TP}{TP + FP} $\n",
    "\n",
    "- High precision means the response avoids including unsupported claims.\n",
    "- Penalizes responses that introduce hallucinated information.\n",
    "\n",
    "#### **Recall Mode:**\n",
    "> Measures how much of the reference information is retained in the response.\n",
    "\n",
    "$ \\text{Recall} = \\frac{TP}{TP + FN} $\n",
    "\n",
    "- High recall means the response includes all important reference claims.\n",
    "- Penalizes responses that omit key factual details.\n",
    "\n",
    "#### **F1 Mode (Default):**\n",
    "> Balances precision and recall for a comprehensive factual correctness score.\n",
    "\n",
    "$ \\text{F1} = \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} $\n",
    "\n",
    "---\n",
    "\n",
    "### **Controlling the Number of Claims**\n",
    "Each sentence in the response and reference can be decomposed into multiple claims. The granularity of this decomposition is determined by **atomicity** and **coverage**.\n",
    "\n",
    "#### **Atomicity:**\n",
    "Controls how much a sentence is broken down:\n",
    "- **High Atomicity:** Breaks a sentence into fine-grained claims.\n",
    "- **Low Atomicity:** Keeps a sentence more intact with minimal decomposition.\n",
    "\n",
    "#### **Coverage:**\n",
    "Determines how much information is extracted:\n",
    "- **High Coverage:** Extracts all details from the original sentence.\n",
    "- **Low Coverage:** Focuses on key information, omitting minor details.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example of Atomicity and Coverage Adjustments**\n",
    "#### **High Atomicity & High Coverage:**\n",
    "```python\n",
    "scorer = FactualCorrectness(mode=\"precision\", atomicity=\"high\", coverage=\"high\")\n",
    "```\n",
    "**Original Sentence:**  \n",
    "> \"Marie Curie was a Polish and naturalized-French physicist and chemist who conducted pioneering research on radioactivity.\"\n",
    "\n",
    "**Decomposed Claims:**\n",
    "- \"Marie Curie was a Polish physicist.\"\n",
    "- \"Marie Curie was a naturalized-French physicist.\"\n",
    "- \"Marie Curie was a chemist.\"\n",
    "- \"Marie Curie conducted pioneering research on radioactivity.\"\n",
    "\n",
    "---\n",
    "\n",
    "### **Practical Application**\n",
    "- **High Atomicity & High Coverage** â†’ Best for detailed fact-checking and claim extraction.\n",
    "- **Low Atomicity & Low Coverage** â†’ Suitable for summarization or when only key facts are needed.\n",
    "\n",
    "This flexibility ensures the metric can be tailored to different levels of granularity based on the application.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways:**\n",
    "* Factual correctness evaluates the factual overlap between a response and a reference.  \n",
    "* It provides **precision**, **recall**, and **F1-score** options for measurement.  \n",
    "* Atomicity and coverage control the **granularity** of claim decomposition.  \n",
    "* Helps identify hallucinations (FP) and missing information (FN) in responses.  \n",
    "\n",
    "This metric is crucial for **evaluating Retrieval-Augmented Generation (RAG) systems** where factual consistency is a priority!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import FactualCorrectness\n",
    "\n",
    "# Uses the response and reference columns\n",
    "factual_correctness = FactualCorrectness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Answer Correctness**\n",
    "\n",
    "### TL;DR:  \n",
    "> **How well does a modelâ€™s response align with the ground truth in terms of factual correctness and completeness?**\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition**:  \n",
    "**Answer Correctness** evaluates how accurately a model-generated response corresponds to a given ground truth. It does this by breaking down both texts into discrete factual statements and determining whether these statements match. The metric assesses correctness through a combination of **factual alignment** and **semantic similarity**.\n",
    "\n",
    "The correctness score ranges from **0 to 1**, where higher values indicate better alignment with the ground truth.\n",
    "\n",
    "---\n",
    "\n",
    "### **Statement Breakdown and Classification**:  \n",
    "To assess correctness, the response is decomposed into factual statements, which are then categorized as follows:\n",
    "\n",
    "- **True Positives (TP):** Statements in the response that are also in the ground truth.\n",
    "- **False Positives (FP):** Statements in the response that are not supported by the ground truth.\n",
    "- **False Negatives (FN):** Statements in the ground truth that are missing from the response.\n",
    "\n",
    "---\n",
    "\n",
    "### **Calculation Formula**:  \n",
    "The **F1 Score** used for Answer Correctness is defined as:\n",
    "\n",
    "$ F1 = \\frac{|TP|}{|TP| + 0.5 \\times (|FP| + |FN|)} $ \n",
    "\n",
    "- This formula ensures a balance between **precision (avoiding hallucinated facts)** and **recall (retaining key information).**\n",
    "- The **0.5 weight on FP and FN** prevents excessive penalization while ensuring factual accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### **Computation Steps**:  \n",
    "1. **Extract factual statements** from both the response and the ground truth.  \n",
    "2. **Classify statements** into TP, FP, and FN using a **Correctness Classifier** based on natural language inference (NLI).  \n",
    "3. **Compute factual correctness** using the F1-like score.  \n",
    "4. **Compute semantic similarity** if slight wording variations are acceptable.  \n",
    "5. **Combine both scores** using predefined weight factors (default: factual correctness = 0.75, semantic similarity = 0.25).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Computation Modes**:  \n",
    "\n",
    "#### **Factual Alignment Mode (Default)**  \n",
    "> Focuses on exact factual correctness using the TP, FP, and FN classification.  \n",
    "\n",
    "F1 = $\\frac{|TP|}{|TP| + 0.5 \\times (|FP| + |FN|)} $\n",
    "\n",
    "- Ensures that incorrect statements (FP) and missing information (FN) reduce the score.\n",
    "\n",
    "#### **Semantic Similarity Mode**  \n",
    "> Measures overall alignment by incorporating semantic closeness between response and ground truth.  \n",
    "\n",
    "- Uses **embeddings-based similarity scoring** (e.g., cosine similarity).\n",
    "- Allows for flexible wording but still ensures factual correctness.\n",
    "\n",
    "#### **Weighted Combination Mode**  \n",
    "> A hybrid approach balancing factual alignment and semantic similarity.  \n",
    "\n",
    "\n",
    "Score = $ w_1 \\times \\text{Factual Correctness} + w_2 \\times \\text{Semantic Similarity} $\n",
    "\n",
    "(Default: **\\( w_1 = 0.75, w_2 = 0.25 \\)**)\n",
    "\n",
    "---\n",
    "\n",
    "### **Controlling Granularity of Statements**  \n",
    "\n",
    "The metric supports **customizing the granularity** of extracted statements using:  \n",
    "\n",
    "#### **Atomicity:**  \n",
    "- **High Atomicity** â†’ Extracts fine-grained claims from a sentence.  \n",
    "- **Low Atomicity** â†’ Retains larger sentence-level claims.  \n",
    "\n",
    "#### **Coverage:**  \n",
    "- **High Coverage** â†’ Extracts all possible claims from a response.  \n",
    "- **Low Coverage** â†’ Focuses only on the key facts.  \n",
    "\n",
    "This flexibility ensures adaptability for different evaluation needs.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example of Atomicity and Coverage Adjustments**  \n",
    "#### **High Atomicity & High Coverage Example:**\n",
    "```python\n",
    "scorer = AnswerCorrectness(atomicity=\"high\", coverage=\"high\")\n",
    "```\n",
    "**Original Sentence:**  \n",
    "> \"The Eiffel Tower was designed by Gustave Eiffel and completed in 1889.\"  \n",
    "\n",
    "**Decomposed Claims:**  \n",
    "- \"The Eiffel Tower was designed by Gustave Eiffel.\"  \n",
    "- \"The Eiffel Tower was completed in 1889.\"  \n",
    "\n",
    "---\n",
    "\n",
    "### **Practical Applications**  \n",
    "- **High Atomicity & High Coverage** â†’ Best for **fact-checking and evaluation** in RAG applications.  \n",
    "- **Low Atomicity & Low Coverage** â†’ Suitable for **summary-level correctness**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways:**  \n",
    "* Measures factual correctness and completeness in generated answers.  \n",
    "* Uses **True Positives (TP), False Positives (FP), and False Negatives (FN)** for scoring.  \n",
    "* Provides an **F1-like score** for correctness evaluation.  \n",
    "* Supports **semantic similarity** as an additional scoring mode.  \n",
    "* Can be adjusted for **granularity (atomicity) and coverage** to control claim decomposition.  \n",
    "\n",
    "This metric is essential for **evaluating Retrieval-Augmented Generation (RAG) systems**, ensuring factually grounded responses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import AnswerCorrectness\n",
    "\n",
    "# Uses the user_input, response and reference columns\n",
    "answer_correctness = AnswerCorrectness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 260/260 [3:15:39<00:00, 45.15s/it]   \n"
     ]
    }
   ],
   "source": [
    "result = evaluate(\n",
    "    dataset=evaluation_dataset,\n",
    "    metrics=[\n",
    "        llm_context_precision_with_re,\n",
    "        llm_context_precision_without_re,\n",
    "        context_precision_with_reference,\n",
    "        llm_context_recall,\n",
    "        context_recall,\n",
    "        context_entity_recall,\n",
    "        noise_sensitivity_mode_relevant,\n",
    "        noise_sensitivity_mode_irrelevant,\n",
    "        response_relevancy,\n",
    "        faithfulness,\n",
    "        answer_accuracy,\n",
    "        context_relevance,\n",
    "        response_groundedness,\n",
    "        factual_correctness,\n",
    "        answer_correctness\n",
    "    ],\n",
    "    llm=llm,\n",
    "    embeddings=embeddings,\n",
    "    run_config=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm_context_precision_with_reference': 0.7500, 'context_recall': 0.5730, 'answer_relevancy': 0.9423, 'faithfulness_with_hhem': 0.4200, 'factual_correctness(mode=f1)': 0.4952}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = result.to_pandas()\n",
    "result_df.to_csv('metrics_evaluation.csv', index=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results uploaded! View at https://app.ragas.io/dashboard/alignment/evaluation/4cd1785f-206c-474d-a92e-f3a90538aa51\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://app.ragas.io/dashboard/alignment/evaluation/4cd1785f-206c-474d-a92e-f3a90538aa51'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.upload()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
