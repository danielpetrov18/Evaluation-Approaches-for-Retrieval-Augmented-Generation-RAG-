{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Retrieval-Augmented Generation (RAG) Applications: Importance and Impact  \n",
    "\n",
    "Evaluating RAG-based applications is crucial to ensure the reliability, relevance, and efficiency of their responses. Unlike using LLMs by themselves, RAG-based applications combine the power of Large-Language-Models with vector stores and retrieval components, which introduces complexity. Hence, evaluation of different components in RAG pipelines and the pipeline as a whole is crucial. \n",
    "\n",
    "#### Key Pitfalls in RAG Applications\n",
    "- **Hallucination**: Even with retrieved context, LLMs can still generate inaccurate or misleading responses, due to inproper incorporation of the context or just relying on their own knowledge, which might contradict the context.\n",
    "- **Irrelevant Retrieval**: Poor retrieval results, due to a bad embedding model, high/low chunk size, high/low limit (top_k) or lack of re-ranking.\n",
    "- **Latency & Efficiency**: Combining retrieval and generation can introduce delays, requiring performance optimization and can be quite tricky to debug.\n",
    "\n",
    "#### Advantages of Proper Evaluation\n",
    "- **Improved Accuracy**: Ensures the factual correctness of responses by using a knowledge base.  \n",
    "- **Enhanced Relevance**: Helps refine retrieval mechanisms, ensuring responses are more context-aware.  \n",
    "- **Robustness & Adaptability**: Identifies failure points, enabling continuous improvements to model behavior.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "* Execute the script in the root of the `evaluation` folder.\n",
    "* If executing the script fails run: `chmod u+x setup.sh`.\n",
    "* It will install all required dependencies.\n",
    "* Finally, make sure you select it in the notebook by specifying `eval` as kernel.\n",
    "\n",
    "---\n",
    "\n",
    "*(OPTIONAL STEP)*\n",
    "\n",
    "**RAGAs** provides a cloud platform where a dataset and evaluation results can be stored and viewed.\n",
    "To use it follow this link: [RAGAs.io](https://app.ragas.io/).\n",
    "* Sign-up\n",
    "* Retrieve the **token**\n",
    "* Create a `.env` file with the following content:\n",
    "```bash\n",
    "RAGAS_APP_TOKEN=apt.......-9f6ed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the evaluation dataset\n",
    "\n",
    "According to **RAGAs** an **evaluation dataset** is a homogeneous collection of **data samples** with the sole purpose of measuring the capabilities and performance of an AI application.\n",
    "\n",
    "- **Structure**:\n",
    "\n",
    "    - Contains either **SingleTurnSample** or **MultiTurnSample** object instances, representing a unique interaction or a set of interactions respectively between a **Persona** and the AI-system.\n",
    "\n",
    "    - **NOTE**: The dataset can contain **ONLY** a single type of samples. They cannot be mixed together into a single dataset.\n",
    "\n",
    "**Samples** represent a single unit of interaction with the underlying system. As mentioned they can be either **SingleTurnSample** or **MultiTurnSample**.\n",
    "\n",
    "For this project I focus solely on **SingleTurnSample** objects, since I'm evaluating independent queries, not multi-turn interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict\n",
    "from ragas import EvaluationDataset, SingleTurnSample\n",
    "\n",
    "# The are located under `./datasets`\n",
    "filepath: str = input(\"Please specify which dataset to evaluate (only the file name): \")\n",
    "\n",
    "goldens: List[Dict] = []\n",
    "try:\n",
    "    with open(file=f\"./datasets/{filepath}.jsonl\", mode=\"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if line.strip():  # Skip empty lines\n",
    "                goldens.append(json.loads(line))\n",
    "\n",
    "    samples: List[SingleTurnSample] = []\n",
    "    for golden in goldens:\n",
    "        single_turn_sample = SingleTurnSample(**golden)\n",
    "        samples.append(single_turn_sample)\n",
    "        \n",
    "    evaluation_dataset = EvaluationDataset(samples)\n",
    "except FileNotFoundError:\n",
    "    print(f\"File: `./datasets/{filepath}.jsonl` containing goldens not found!\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error parsing JSONL file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate required objects\n",
    "\n",
    "- **RAGAs** would require a **Large-Language-Model** and an **Embedding** one to be able to apply the **transformations** to the **knowledge graph**. For that purpose one must create **wrapper** objects for both of the models. `Langchain` and `llama-index` are both supported. \n",
    "\n",
    "- Additionally, a **configuration** can be used to modify the default behaviour of the framework. For example timeout values can be modified, maximum retries for failed operations and so on can be configured from the **RunConfig**.\n",
    "    - **NOTE**: depending on the LLM model and GPU you may need to modify the `timeout` value, otherwise you will stumble upon `TimeoutException`\n",
    "\n",
    "- Lastly, there's a single implementation in **RAGAs** for caching intermediate steps onto disk. To use it the **DiskCacheBackend** class can come in play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "from ragas import RunConfig, DiskCacheBackend\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "run_config = RunConfig(\n",
    "    timeout=86400,    # 24 hours on waiting for a single operation\n",
    "    max_retries=20,   # Max retries before giving up\n",
    "    max_wait=600,     # Max wait between retries\n",
    "    max_workers=4,    # Concurrent requests\n",
    "    log_tenacity=True # Print retry attempts\n",
    ")\n",
    "\n",
    "# This stores data generation and evaluation results locally on disk\n",
    "# When using it for the first time, it will create a .cache folder\n",
    "# When using it again, it will read from that folder and finish almost instantly\n",
    "cacher = DiskCacheBackend(cache_dir=\".cache\")\n",
    "\n",
    "load_dotenv(\"../../env/rag.env\")\n",
    "\n",
    "langchain_llm = LangchainLLMWrapper(\n",
    "    langchain_llm=ChatOllama(\n",
    "        model=os.getenv(\"CHAT_MODEL\"),\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        temperature=float(os.getenv(\"TEMPERATURE\")),\n",
    "        num_ctx=int(os.getenv(\"LLM_CONTEXT_WINDOW_TOKENS\")),\n",
    "        format=\"json\"\n",
    "    ),\n",
    "    run_config=run_config,\n",
    "    cache=cacher\n",
    ")\n",
    "\n",
    "langchain_embeddings = LangchainEmbeddingsWrapper(\n",
    "    embeddings=OllamaEmbeddings(\n",
    "        model=os.getenv(\"EMBEDDING_MODEL\"),\n",
    "        base_url=\"http://localhost:11434\"\n",
    "    ),\n",
    "    run_config=run_config,\n",
    "    cache=cacher\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "A metric is a quantatative measure used to evaluate the performance of a LLM-based application. Metrics help us assess how well an application as a whole or its individual components performs relative to the dataset used. With their help we can iterate towards a more *accurate*, *faithful* and *precise* LLM application.\n",
    "\n",
    "Metrics in **RAGAs** can be classified depending on the mechanism they evaluate the application as:\n",
    "\n",
    "- **LLM-based** - where a LLM is used to perform the evaluation. There might be more than one prompts submitted to the LLM to evaluate the application on a single metric. Furthermore, this type of metrics mimic a user, since they tend to be quite non-deterministic and unpredictable. \n",
    "\n",
    "- **Non-LLM based** - where no LLM is required or used to perform the evaluation. These metrics tend to be much faster and predictable than the previous type. However, in my opinion are maybe not the best for evaluating a RAG system where a single query can be submitted multiple times and each time a new response might be generated.\n",
    "\n",
    "The previously mentioned metric types can be further classified into **SingleTurnMetric** and **MultiTurnMetric** respectively. I would like to note again that in this project the **SingleTurnMetric** would be relevant and used.\n",
    "\n",
    "![Metric types in RAGAs](../../img/metrics_mindmap.webp \"Metrics RAGAs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Context Precision**\n",
    "\n",
    "> **TL;DR:** What fraction of the retrieved chunks are **actually relevant**?\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition**:\n",
    "\n",
    "**Context Precision** is a crucial metric for evaluating the **retrieval capabilities** of a RAG (Retrieval-Augmented Generation) system. Since the **LLM's response** depends heavily on the **context retrieved from the knowledge base**, it is essential to have a reliable retrieval mechanism that fetches **only relevant** information.  \n",
    "\n",
    "By achieving **high context precision**, the system ensures that the retrieved information is **more relevant** and **ranked higher**, leading to **more accurate responses** and **reduced hallucinations**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Formula:**\n",
    "For each retrieved chunk at **rank (k)**:\n",
    "\n",
    "**Precision@k** = $\\frac{\\text{Number of relevant chunks at rank k}}{\\text{Total number of retrieved chunks at rank k}} = \\frac{\\text{true positives@k}}{\\text{true positives@k + false positives@k}}$\n",
    "\n",
    "We then compute **Context Precision@K** as the **mean of all Precision@k values**, weighted by relevance:\n",
    "\n",
    "**Context Precision@K** = $\\frac{\\sum_{k=1}^{K} (\\text{Precision@k} \\times \\text{Relevance}(k))}{\\text{Total number of relevant chunks in top K results}}$\n",
    "\n",
    "Where:\n",
    "- **Precision@k** is the proportion of relevant chunks at rank (k).\n",
    "- **Relevance(k)**:\n",
    "\\begin{cases}\n",
    "1, & \\text{if the chunk at rank } k \\text{ is relevant} \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "- **K** is the total number of retrieved chunks, which are classified as relevant\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Calculation**\n",
    "Suppose our **retriever fetches 4 chunks**, and **2 of them** are relevant. We calculate **Precision@k** at each rank:\n",
    "\n",
    "| Rank (k) | Retrieved Chunk | Relevant? | Precision@k |\n",
    "|-------------|----------------|------------|-------------|\n",
    "| 1           | ✅ Relevant      | ✅ (1)      | \\( $\\frac{1}{1}$ = 1.00 \\) |\n",
    "| 2           | ❌ Not Relevant  | ❌ (0)      | \\( $\\frac{1}{2}$ = 0.50 \\) |\n",
    "| 3           | ✅ Relevant      | ✅ (1)      | \\( $\\frac{2}{3}$ = 0.67 \\) |\n",
    "| 4           | ❌ Not Relevant  | ❌ (0)      | \\( $\\frac{2}{4}$ = 0.50 \\) |\n",
    "\n",
    "Now, using the **Context Precision formula**:\n",
    "\n",
    "\n",
    "**Context Precision@4** = $\\frac{(1.00 \\times 1) + (0.50 \\times 0) + (0.67 \\times 1) + (0.50 \\times 0)}{2} = $\n",
    "\n",
    "= $\\frac{1.00 + 0 + 0.67 + 0}{2} = \\frac{1.67}{2}$ = 0.835\n",
    "\n",
    "Thus, **Context Precision@4 = 0.835**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why is This Useful?**\n",
    "\n",
    "- **Evaluates Retriever Quality** → Ensures retrieved chunks contain **relevant** information.\n",
    "\n",
    "- **Also evaluates the re-rankers capabilities** -> Ensures relevant chunks are ranked higher.\n",
    "\n",
    "- **Improves RAG Performance** → Helps **reduce hallucinations** and improves **LLM accuracy**, since relevant information would come first in the context.\n",
    "\n",
    "- **Higher values** for this metric would signify a good retriever, ranking **relevant chunks** high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import LLMContextPrecisionWithReference\n",
    "from prompts.metrics.custom_context_precision_prompt import MyContextPrecisionPrompt\n",
    "\n",
    "llm_context_precision = LLMContextPrecisionWithReference(\n",
    "    context_precision_prompt=MyContextPrecisionPrompt(),\n",
    "    max_retries=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Non-LLM Context Precision**\n",
    "\n",
    "> **TL;DR:** How well do the retrieved chunks semantically match the reference contexts, **without using an LLM**?\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition**:\n",
    "\n",
    "**Non-LLM Context Precision** evaluates the **retrieval accuracy** of a RAG (Retrieval-Augmented Generation) system **without relying on an LLM**. Instead, it leverages **string similarity** between the **retrieved** and **reference** contexts to determine relevance. In **Non-LLM Context Precision**, what matters is whether each retrieved chunk is relevant, regardless of which reference chunk it matches.\n",
    "\n",
    "This metric is useful when you want to **avoid LLM latency/costs** but still need a meaningful estimation of how precise your retriever is in fetching **semantically relevant chunks**.\n",
    "\n",
    "---\n",
    "\n",
    "### **How It Works**:\n",
    "\n",
    "1. For each **retrieved chunk**, compare it with **all reference chunks**.\n",
    "2. Use a **string metric** (**levenshtein** by default) to calculate a score for each (retrieved, reference) pair → forming a **cartesian product**.\n",
    "3. Keep the **maximum similarity score** for each retrieved chunk.\n",
    "4. Apply a **threshold** (e.g., 0.5) to determine if it’s **relevant** or not:\n",
    "   - Score $ \\geq $ threshold → 1 (relevant)\n",
    "   - Score $ \\lt $ threshold → 0 (not relevant)\n",
    "5. Compute **Context Precision** based on the binary verdicts using **average precision**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Formula:**\n",
    "\n",
    "Let `verdict_list` be a list of binary values where each element represents whether a retrieved chunk is considered **relevant (1)** or **not (0)**.\n",
    "\n",
    "The **Non-LLM Context Precision** is calculated as:\n",
    "\n",
    "**Context Precision@K** = $\\frac{\\sum_{k=1}^{K} \\left( \\frac{\\text{number relevant until k}}{k} \\times \\text{is\\_relevant}(k) \\right)}{\\text{Total number of relevant retrieved chunks}}$\n",
    "\n",
    "\n",
    "Where:\n",
    "- `is_relevant(k)` is 1 if the `k`-th chunk is relevant, 0 otherwise.\n",
    "- The numerator accumulates the precision at each rank **only when** the chunk is relevant.\n",
    "- The denominator is the total number of relevant retrieved chunks.\n",
    "\n",
    "This is equivalent to computing **average precision**, but with **relevance determined by similarity**, not LLM output.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Calculation**\n",
    "\n",
    "Assume the retriever returns 4 chunks, and we compute similarity scores against reference contexts as:\n",
    "\n",
    "| Retrieved Chunk | Max Similarity | Verdict (≥ 0.5?) |\n",
    "|------------------|----------------|------------------|\n",
    "| Chunk 1          | 0.92           | ✅ 1             |\n",
    "| Chunk 2          | 0.34           | ❌ 0             |\n",
    "| Chunk 3          | 0.66           | ✅ 1             |\n",
    "| Chunk 4          | 0.28           | ❌ 0             |\n",
    "\n",
    "From this, `verdict_list = [1, 0, 1, 0]`\n",
    "\n",
    "Now compute:\n",
    "\n",
    "**Precision@1** = $\\frac{1}{1} = 1.00$\n",
    "**Precision@3** = $\\frac{2}{3} = 0.67$\n",
    "\n",
    "\n",
    "Only include those positions where `verdict = 1`:\n",
    "\n",
    "**Context Precision@4** = $\\frac{(1.00 \\times 1) + (0.67 \\times 1)}{2} = \\frac{1.67}{2} = 0.835$\n",
    "\n",
    "---\n",
    "\n",
    "### **Why is This Useful?**\n",
    "\n",
    "- **No LLM dependency** → Ideal for fast, offline, or cheap evaluation pipelines.\n",
    "- **Evaluates Retriever Alone** → Helps isolate the retriever’s performance without LLM effects, avoiding LLM bias\n",
    "- **Threshold-tunable** → You can define what \"relevant\" means by adjusting the similarity threshold.\n",
    "- **Helpful during early-stage retriever tuning** before plugging in an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import NonLLMContextPrecisionWithReference\n",
    "\n",
    "non_llm_context_precision = NonLLMContextPrecisionWithReference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Context Recall**\n",
    "\n",
    "### TL;DR:\n",
    "> **How many of the relevant chunks were actually retrieved? Did we miss relevant chunks?**\n",
    "\n",
    "---\n",
    "\n",
    "### **Defition**:\n",
    "\n",
    "**Context Recall** measures how many of the **relevant documents (or pieces of information)** were successfully retrieved. This metric ensures that the retrieval process does not **miss important information** that could be used to generate a response. A **higher recall** means that fewer relevant documents were left out, making it crucial for applications that prioritize completeness over precision. Since recall focuses on **not missing relevant data**, it always requires a reference set for comparison.\n",
    "\n",
    "---\n",
    "\n",
    "### LLM-Based Context Recall\n",
    "\n",
    "This metric evaluates recall by analyzing the **claims** in the reference (expected) response and checking whether they can be attributed to the retrieved context. In an ideal scenario, **all claims** in the reference answer should be **supported** by the retrieved context.\n",
    "\n",
    "### **Formula**:\n",
    "\n",
    "**Context Recall** = $\\frac{\\text{Number of claims in the reference supported by the retrieved context}}{\\text{Total number of claims in the reference}}$\n",
    "\n",
    "A recall score **closer to 1** indicates that most relevant data has been successfully retrieved, while a **lower recall** means that key information was missed.\n",
    "\n",
    "### **Non-LLM-Based Context Recall**\n",
    "\n",
    "### **Definition**:\n",
    "\n",
    "**Non-LLM-Based Context Recall** measures how many of the **relevant documents (or pieces of information)** were successfully retrieved **without relying on a language model** for evaluation. This metric ensures that the retrieval process does not **miss important information** that could be used to generate a response. A **higher recall** means that fewer relevant documents were left out, making it crucial for applications that prioritize completeness over precision. Since recall focuses on **not missing relevant data**, it always requires a reference set for comparison.\n",
    "\n",
    "---\n",
    "\n",
    "### **How It Works**\n",
    "This metric calculates recall based on **string similarity** rather than using an LLM to assess the retrieved content. It follows these steps:\n",
    "\n",
    "1. **Extract Retrieved and Reference Contexts**  \n",
    "   - Uses `retrieved_contexts` (what was fetched) and `reference_contexts` (ground truth).  \n",
    "2. **Compute Similarity Scores**  \n",
    "   - Compares each **retrieved context** against each **reference context** using a similarity function.\n",
    "3. **Find the Best Match for Each Reference Context**  \n",
    "   - For each **reference context**, it selects the **retrieved context** with the **highest similarity score**.\n",
    "4. **Apply a Threshold (`0.5` by default)**  \n",
    "   - If the similarity score is **above the threshold**, the reference context is considered **retrieved successfully**.\n",
    "5. **Compute Recall Score**  \n",
    "   - The recall score is the proportion of reference contexts that have at least one matching retrieved context **above the threshold**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Formula**\n",
    "\n",
    "**Non-LLM-Based Context Recall** =  $\\frac{\\text{Number of reference contexts with a high-scoring retrieved match}}{\\text{Total number of reference contexts}} $\n",
    "\n",
    "Where:\n",
    "- A **retrieved match** is valid if its **similarity score > threshold**.\n",
    "- The **threshold** is configurable (default = `0.5`).\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Calculation**\n",
    "#### **Given:**\n",
    "- `retrieved_contexts = [\"text A\", \"text B\", \"text C\"]`\n",
    "- `reference_contexts = [\"text X\", \"text Y\"]`\n",
    "- **Similarity scores:**  \n",
    "  - `text A` ↔ `text X` = **0.7** ✅\n",
    "  - `text B` ↔ `text X` = **0.4** ❌\n",
    "  - `text C` ↔ `text X` = **0.6** ✅ (max = 0.7)\n",
    "  - `text A` ↔ `text Y` = **0.2** ❌\n",
    "  - `text B` ↔ `text Y` = **0.5** ✅\n",
    "  - `text C` ↔ `text Y` = **0.3** ❌ (max = 0.5)\n",
    "\n",
    "#### **Final Score Computation**\n",
    "- `text X` has a match (`0.7 > 0.5`) → ✅ **1**\n",
    "- `text Y` has a match (`0.5 == 0.5`) → ✅ **1**\n",
    "- **Score = (1+1) / 2 = 1.0 (100%)** ✅\n",
    "\n",
    "---\n",
    "\n",
    "### **Why is This Useful?**\n",
    "- **Evaluates Retriever Performance Without an LLM** → Ensures relevance using **string similarity**, avoiding model biases.\n",
    "- **Helps Optimize RAG Pipelines** → Higher recall ensures more **comprehensive** context retrieval.\n",
    "- **Works Well with Threshold-Based Comparisons** → Adjustable strictness for different applications.\n",
    "\n",
    "A recall score **closer to 1** means that most relevant contexts were retrieved, while a **lower recall** suggests that key information was missed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import LLMContextRecall, NonLLMContextRecall   \n",
    "from prompts.metrics.custom_context_recall_prompt import MyContextRecallPrompt\n",
    "\n",
    "# For each claim in the response, verify if it can be supported by the retrieved context\n",
    "llm_context_recall = LLMContextRecall(\n",
    "    context_recall_prompt=MyContextRecallPrompt(),\n",
    "    max_retries=20\n",
    ")\n",
    "\n",
    "non_llm_context_recall = NonLLMContextRecall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Context Entities Recall**\n",
    "\n",
    "### TL;DR:\n",
    "> **It is a measure of what fraction of entities are recalled from reference**\n",
    "\n",
    "---\n",
    "\n",
    "### Definition:\n",
    "`ContextEntityRecall` metric measures the recall of the retrieved context, based on the number of entities present in both `reference` and `retrieved_contexts`, relative to the total number of entities in the `reference` alone. In simple terms, it evaluates how well the retrieved contexts capture the entities from the original reference.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula:\n",
    "\n",
    "To compute this metric, we define two sets:\n",
    "\n",
    "- **RE**: The set of entities in the reference.\n",
    "- **RCE**: The set of entities in the retrieved contexts.\n",
    "\n",
    "We determine the number of entities common to both sets (**RCE** $\\cap$  **RE**) and divide it by the total number of entities in the reference (**RE**). The formula is:\n",
    "\n",
    "$ \\text{Context Entity Recall} = \\frac{\\text{Number of common entities between RCE and RE}}{\\text{Total number of entities in RE}} = \\frac{\\text{RCE } \\cap \\text{ RE}}{\\text{RE}} $\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Calculation**\n",
    "\n",
    "#### **Scenario:**\n",
    "\n",
    "We have a **reference** and two retrieved contexts:\n",
    "\n",
    "**Reference:**\n",
    "> \"The Taj Mahal is an ivory-white marble mausoleum on the right bank of the river Yamuna in the Indian city of Agra. It was commissioned in 1631 by the Mughal emperor Shah Jahan to house the tomb of his favorite wife, Mumtaz Mahal.\"\n",
    "\n",
    "**Retrieved Contexts:**\n",
    "\n",
    "- **High Entity Recall Context:**\n",
    "  > \"The Taj Mahal is a symbol of love and architectural marvel located in Agra, India. It was built by the Mughal emperor Shah Jahan in memory of his beloved wife, Mumtaz Mahal. The structure is renowned for its intricate marble work and beautiful gardens surrounding it.\"\n",
    "\n",
    "- **Low Entity Recall Context:**\n",
    "  > \"The Taj Mahal is an iconic monument in India. It is a UNESCO World Heritage Site and attracts millions of visitors annually. The intricate carvings and stunning architecture make it a must-visit destination.\"\n",
    "\n",
    "#### **Step 1: Extract Entities**\n",
    "\n",
    "Entities in the **Reference (RE)**:\n",
    "> \\[\"Taj Mahal\", \"Yamuna\", \"Agra\", \"1631\", \"Shah Jahan\", \"Mumtaz Mahal\"\\]\n",
    "\n",
    "Entities in **High Recall Context (RCE1)**:\n",
    "> \\[\"Taj Mahal\", \"Agra\", \"Shah Jahan\", \"Mumtaz Mahal\", \"India\"\\]\n",
    "\n",
    "Entities in **Low Recall Context (RCE2)**:\n",
    "> \\[\"Taj Mahal\", \"UNESCO\", \"India\"\\]\n",
    "\n",
    "#### **Step 2: Compute Context Entity Recall**\n",
    "\n",
    "For **High Recall Context (RCE1)**:\n",
    "$ \\text{Context Entity Recall} = \\frac{4}{6} = 0.67 $ \n",
    "\n",
    "For **Low Recall Context (RCE2)**:\n",
    "$ \\text{Context Entity Recall} = \\frac{1}{6} = 0.17 $\n",
    "\n",
    "Since the first context retains more entities from the reference, it has a **higher entity recall**, indicating it is **more comprehensive** in capturing the essential information.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Insights:**\n",
    "\n",
    "* **Higher Entity Recall Improves Answer Completeness:** If entities are important for context, a higher recall ensures critical information is included in the retrieved context.\n",
    "\n",
    "* **Useful for Fact-Heavy Applications:** Applications in **legal, medical, and historical domains** benefit significantly from high entity recall.\n",
    "\n",
    "* **Balances with Context Precision:** While high recall is beneficial, retrieving too many irrelevant entities can introduce noise. **Optimizing both recall and precision** is crucial for effective retrieval in RAG systems.\n",
    "\n",
    "* **Comparison of Retrieval Mechanisms:** If two retrieval mechanisms fetch different contexts, **Context Entity Recall** helps determine which one is better at preserving key entities from the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import ContextEntityRecall\n",
    "from prompts.metrics.custom_context_entities_recall_prompt import MyContextEntitiesRecallPrompt\n",
    "\n",
    "context_entity_recall = ContextEntityRecall(\n",
    "    context_entity_recall_prompt=MyContextEntitiesRecallPrompt(),\n",
    "    max_retries=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Noise Sensitivity**\n",
    "\n",
    "### TL;DR:\n",
    "> **How prone is the system to generating incorrect claims from retrieved contexts?**\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition**:\n",
    "**Noise Sensitivity** measures how often a system makes errors by providing incorrect responses when utilizing either relevant or irrelevant retrieved documents. This metric evaluates the robustness of a Retrieval-Augmented Generation (RAG) system against potentially misleading or noisy information.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conceptual Insight**:\n",
    "The metric fundamentally tests how easily an LLM can be \"tricked\" into generating factually incorrect responses. It distinguishes between two critical scenarios:\n",
    "\n",
    "1. **Relevant Context Noise**: \n",
    "   - More subtle and dangerous\n",
    "   - Noise is camouflaged within seemingly pertinent information\n",
    "   - High risk of inadvertently incorporating incorrect claims\n",
    "\n",
    "2. **Irrelevant Context Noise**:\n",
    "   - A robust LLM should completely resist this\n",
    "   - Contexts unrelated to the user's query\n",
    "   - Zero tolerance for incorporating unrelated information\n",
    "\n",
    "---\n",
    "\n",
    "### LLM-Based Noise Sensitivity\n",
    "\n",
    "The metric assesses noise sensitivity by decomposing the response and reference into individual claims, then analyzing:\n",
    "- Whether claims are correct according to the ground truth\n",
    "- Whether incorrect claims can be attributed to retrieved contexts (either relevant or irrelevant)\n",
    "\n",
    "---\n",
    "\n",
    "### **Calculation Approach**:\n",
    "\n",
    "1. **Decompose Statements**: Break down the reference and response into individual claims using an LLM evaluator\n",
    "2. **Context Verification**: Check if claims can be supported by retrieved contexts using NLI techniques\n",
    "3. **Identify Relevant Contexts**: Determine which contexts support the ground truth\n",
    "4. **Map Response Claims**: Map each claim in the response to contexts that support it\n",
    "5. **Identify Incorrect Claims**: Determine which response claims are unsupported by the ground truth\n",
    "6. **Calculate Sensitivity**: Compute the proportion of incorrect claims derived from contexts\n",
    "\n",
    "---\n",
    "\n",
    "### **Formula**:\n",
    "\n",
    "Let's define the following:\n",
    "- $S_r$ = Set of response statements\n",
    "- $S_i$ = Set of incorrect statements in the response (not supported by ground truth)\n",
    "- $C_r$ = Set of relevant contexts (contexts that support ground truth)\n",
    "- $C_i$ = Set of irrelevant contexts (contexts that don't support ground truth)\n",
    "- $f(s,c)$ = Function that returns 1 if statement $s$ can be inferred from context $c$, 0 otherwise\n",
    "\n",
    "For each statement $s$ in the response, we can define:\n",
    "- $\\text{relevant\\_faithful}(s) = \\max_{c \\in C_r} f(s,c)$ (1 if statement can be inferred from any relevant context)\n",
    "- $\\text{irrelevant\\_faithful}(s) = \\max_{c \\in C_i} f(s,c) \\cdot (1 - \\text{relevant\\_faithful}(s))$ (1 if statement can be inferred from irrelevant context and not from any relevant context)\n",
    "\n",
    "Then:\n",
    "\n",
    "**Relevant Mode Noise Sensitivity**:\n",
    "$$\\text{NS}_{\\text{relevant}} = \\frac{\\sum_{s \\in S_i} \\text{relevant\\_faithful}(s)}{|S_r|}$$\n",
    "\n",
    "**Irrelevant Mode Noise Sensitivity**:\n",
    "$$\\text{NS}_{\\text{irrelevant}} = \\frac{\\sum_{s \\in S_i} \\text{irrelevant\\_faithful}(s)}{|S_r|}$$\n",
    "\n",
    "Where $|S_r|$ is the total number of statements in the response.\n",
    "\n",
    "In plain English:\n",
    "- Relevant mode: Proportion of response statements that are both incorrect and supported by relevant contexts\n",
    "   - incorrect statement means that a there's a contradiction between a statement in the `response` and `reference`\n",
    "   - relevant context is a context, which supports information from the `reference`\n",
    "- Irrelevant mode: Proportion of response statements that are both incorrect and supported only by irrelevant contexts\n",
    "   - irrelevant context is a context, which doesn't support any statement in the `reference`\n",
    "\n",
    "A score **closer to 0** indicates better performance, suggesting:\n",
    "- Fewer incorrect claims\n",
    "- Less influence from noisy or irrelevant contexts\n",
    "- More robust response generation\n",
    "\n",
    "---\n",
    "\n",
    "### **Modes**:\n",
    "- **Relevant Mode** (default): Focuses on noise in relevant contexts\n",
    "- **Irrelevant Mode**: Analyzes noise from irrelevant retrieved documents\n",
    "\n",
    "---\n",
    "\n",
    "### **Evaluation Expectations**:\n",
    "- **Relevant Context**: Some noise tolerance, but should be minimal\n",
    "- **Irrelevant Context**: Virtually zero noise should be incorporated\n",
    "  - A high-quality LLM should completely disregard irrelevant information\n",
    "  - Any noise from irrelevant contexts indicates a significant vulnerability\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Scenario**:\n",
    "Consider a query about the Life Insurance Corporation of India (LIC):\n",
    "- **Question**: \"What is the Life Insurance Corporation of India (LIC) known for?\"\n",
    "- **Reference**: \"The Life Insurance Corporation of India (LIC) is the largest insurance company in India, established in 1956 through the nationalization of the insurance industry. It is known for managing a large portfolio of investments.\"\n",
    "- **Response**: \"The Life Insurance Corporation of India (LIC) is the largest insurance company in India, known for its vast portfolio of investments. LIC contributes to the financial stability of the country.\"\n",
    "- **Retrieved Contexts**: Mix of relevant information about LIC and irrelevant information about the Indian economy\n",
    "- **Noise Sensitivity Score**: 0.33 (one incorrect claim out of three total claims)\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Insights**:\n",
    "- Helps identify system vulnerabilities to hallucination\n",
    "- Provides a quantitative measure of response reliability\n",
    "- Distinguishes between subtle (relevant) and gross (irrelevant) information distortions\n",
    "- Serves as a critical component in comprehensive RAG system evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import NoiseSensitivity\n",
    "from prompts.metrics.faithfulness.custom_nli_generator_prompt import MyNLIStatementPrompt\n",
    "from prompts.metrics.faithfulness.custom_statement_generator_prompt import MyStatementGeneratorPrompt\n",
    "\n",
    "noise_sensitivity_mode_relevant = NoiseSensitivity(\n",
    "    nli_statements_prompt=MyNLIStatementPrompt(),\n",
    "    statement_generator_prompt=MyStatementGeneratorPrompt(),\n",
    "    max_retries=20\n",
    ")\n",
    "\n",
    "noise_sensitivity_mode_irrelevant = NoiseSensitivity(\n",
    "    mode=\"irrelevant\",\n",
    "    nli_statements_prompt=MyNLIStatementPrompt(),\n",
    "    statement_generator_prompt=MyStatementGeneratorPrompt(),\n",
    "    max_retries=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Response Relevancy**\n",
    "\n",
    "### TL;DR:\n",
    "> **How well does the answer address the original user query?**\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition**:\n",
    "**Response Relevancy** measures how closely an AI-generated response aligns with the original user input. It evaluates the answer's ability to directly and appropriately address the user's question, penalizing responses that are incomplete, off-topic, or include unnecessary details. It doesn't judge the **factual accuracy**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conceptual Insight**:\n",
    "The metric focuses on the semantic alignment between:\n",
    "- The original user query\n",
    "- The generated response\n",
    "\n",
    "Key evaluation criteria:\n",
    "1. **Direct Address**: Does the answer directly tackle the user's question?\n",
    "2. **Information Completeness**: Does the response provide sufficient information?\n",
    "3. **Topic Coherence**: Does the answer stay focused on the query's intent?\n",
    "\n",
    "---\n",
    "\n",
    "### **Calculation Approach**:\n",
    "\n",
    "1. **Question Generation**: \n",
    "   - Use the LLM to generate artificial questions based on the response\n",
    "   - Default is to create 3 variant questions\n",
    "   - These questions should capture the essence of the response\n",
    "\n",
    "2. **Semantic Similarity**:\n",
    "   - Compute cosine similarity between:\n",
    "     - Original user input embedding\n",
    "     - Embeddings of generated questions\n",
    "   - Measures how closely the questions match the original query\n",
    "\n",
    "3. **Scoring**:\n",
    "   - Average the cosine similarity scores\n",
    "   - Higher scores indicate better relevance\n",
    "   - Scores typically range between 0 and 1\n",
    "\n",
    "---\n",
    "\n",
    "### **Formula**:\n",
    "\n",
    "**Response Relevancy** = $\\frac{\\sum_{i=1}^{N} \\text{cosine\\_similarity}(E_{g_{i}}, E_{o})}{N}$ \n",
    "\n",
    "Where:\n",
    "- $E_{g_{i}}$: Embedding of the i-th generated question\n",
    "- $E_{o}$: Embedding of the original user input\n",
    "- N: Number of generated questions (default: 3)\n",
    "\n",
    "---\n",
    "\n",
    "### **Evaluation Expectations**:\n",
    "- **High Score (Close to 1)**: \n",
    "  - Answer directly addresses the query\n",
    "  - Comprehensive and focused response\n",
    "  - Minimal irrelevant information\n",
    "\n",
    "- **Low Score (Close to 0)**: \n",
    "  - Response is off-topic\n",
    "  - Incomplete or evasive answer\n",
    "  - Includes excessive unrelated details\n",
    "\n",
    "---\n",
    "\n",
    "### **Important Limitations**:\n",
    "- **No Factuality Check**: \n",
    "  - Measures relevance, not accuracy\n",
    "  - Does not verify the truthfulness of the response\n",
    "- **Embedding-Based**: \n",
    "  - Relies on semantic similarity\n",
    "  - May not catch nuanced relevance\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Scenario**:\n",
    "- **User Query**: \"When was the first Super Bowl?\"\n",
    "- **Good Response**: \"The first Super Bowl was held on Jan 15, 1967, between the Green Bay Packers and Kansas City Chiefs.\"\n",
    "- **Poor Response**: \"Football is a popular sport in the United States with many interesting historical moments.\"\n",
    "\n",
    "### **Key Insights**:\n",
    "- Helps evaluate response quality beyond simple keyword matching\n",
    "- Provides a quantitative measure of semantic alignment\n",
    "- Supports improving AI system's query understanding\n",
    "- Identifies potential issues with off-topic or unfocused responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import ResponseRelevancy\n",
    "from prompts.metrics.custom_response_relevance_prompt import MyResponseRelevancePrompt\n",
    "\n",
    "response_relevancy = ResponseRelevancy(\n",
    "    question_generation=MyResponseRelevancePrompt()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Faithfulness**\n",
    "\n",
    "### TL;DR:\n",
    "> **How factually consistent is the response with the retrieved context?**\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition**:\n",
    "The **Faithfulness** metric measures how factually consistent a response is with the retrieved context. It ranges from 0 to 1, with higher scores indicating better consistency.\n",
    "\n",
    "A response is considered faithful if all of its claims can be supported by the retrieved context.\n",
    "\n",
    "---\n",
    "\n",
    "### **Calculation Approach**:\n",
    "\n",
    "1. **Identify Claims**: \n",
    "   - Break down the response into individual statements\n",
    "   - Examine each claim systematically\n",
    "\n",
    "2. **Context Verification**:\n",
    "   - Check each claim to see if it can be inferred from the retrieved context\n",
    "   - Determine the support level of each statement\n",
    "\n",
    "3. **Scoring**:\n",
    "   - Compute the faithfulness score using the formula:\n",
    "     \n",
    "     $\\text{Faithfulness Score} = \\frac{\\text{Number of claims supported by the retrieved context}}{\\text{Total number of claims in the response}}$\n",
    "\n",
    "---\n",
    "\n",
    "### **Example**:\n",
    "\n",
    "**Question**: Where and when was Einstein born?\n",
    "\n",
    "**Context**: Albert Einstein (born 14 March 1879) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time.\n",
    "\n",
    "**High Faithfulness Answer**: Einstein was born in Germany on 14th March 1879.\n",
    "\n",
    "**Low Faithfulness Answer**: Einstein was born in Germany on 20th March 1879.\n",
    "\n",
    "#### Calculation Steps:\n",
    "- **Step 1**: Break the generated answer into individual statements\n",
    "- **Step 2**: Verify if each statement can be inferred from the given context\n",
    "- **Step 3**: Apply the faithfulness formula\n",
    "\n",
    "---\n",
    "\n",
    "### **Evaluation Expectations**:\n",
    "- **Perfect Score (1.0)**: \n",
    "  - All claims are directly supported by the context\n",
    "  - No extraneous or unsupported information\n",
    "\n",
    "- **Partial Score**: \n",
    "  - Some claims are supported\n",
    "  - Partial consistency with the retrieved context\n",
    "\n",
    "- **Low Score (Close to 0)**: \n",
    "  - Most claims cannot be verified\n",
    "  - Significant deviation from the original context\n",
    "\n",
    "---\n",
    "\n",
    "### **Advanced Verification**:\n",
    "**HHEM-2.1-Open** can be used as a classifier model to:\n",
    "- Detect hallucinations in LLM-generated text\n",
    "- Cross-check claims with the given context\n",
    "- Efficiently determine claim inferability\n",
    "- Avoid **biases** by not using `LLM-As-A-Judge`\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Insights**:\n",
    "- Measures the factual consistency of AI-generated responses\n",
    "- Helps identify potential hallucinations or fabrications\n",
    "- Provides a quantitative assessment of contextual alignment\n",
    "- Supports improving the reliability of AI-generated content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import Faithfulness\n",
    "from prompts.metrics.faithfulness.custom_nli_generator_prompt import MyNLIStatementPrompt\n",
    "from prompts.metrics.faithfulness.custom_statement_generator_prompt import MyStatementGeneratorPrompt\n",
    "\n",
    "faithfulness = Faithfulness(\n",
    "    nli_statements_prompt=MyNLIStatementPrompt(),\n",
    "    statement_generator_prompt=MyStatementGeneratorPrompt(),\n",
    "    max_retries=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Factual Correctness**\n",
    "\n",
    "### TL;DR:\n",
    "> **How factually consistent is a model's response with the ground truth?**\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition**:\n",
    "**Factual Correctness** measures the degree to which a model's `response` aligns with the `reference` by assessing the factual consistency between the two. It does this by breaking down both the response and reference into distinct claims and then determining whether these claims match using natural language inference (NLI). This metric helps evaluate how accurately the generated response retains factual integrity.\n",
    "\n",
    "The factual correctness score ranges from **0 to 1**, where higher values indicate better factual alignment.\n",
    "\n",
    "---\n",
    "\n",
    "### **Claim Breakdown and Classification**:\n",
    "This metric identifies three types of claims:\n",
    "- **True Positives (TP):** Claims that are present in both the response and the reference.\n",
    "- **False Positives (FP):** Claims that are in the response but not supported by the reference.\n",
    "- **False Negatives (FN):** Claims that are in the reference but missing from the response.\n",
    "\n",
    "---\n",
    "\n",
    "### **Calculation Modes**:\n",
    "The metric can be computed in three different modes:\n",
    "\n",
    "#### **Precision Mode:**\n",
    "> Measures how much of the information in the response is factually correct.\n",
    "\n",
    "$ \\text{Precision} = \\frac{TP}{TP + FP} $\n",
    "\n",
    "- High precision means the response avoids including unsupported claims.\n",
    "- Penalizes responses that introduce hallucinated information.\n",
    "\n",
    "#### **Recall Mode:**\n",
    "> Measures how much of the reference information is retained in the response.\n",
    "\n",
    "$ \\text{Recall} = \\frac{TP}{TP + FN} $\n",
    "\n",
    "- High recall means the response includes all important reference claims.\n",
    "- Penalizes responses that omit key factual details.\n",
    "\n",
    "#### **F1 Mode (Default):**\n",
    "> Balances precision and recall for a comprehensive factual correctness score.\n",
    "\n",
    "$ \\text{F1} = \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} $\n",
    "\n",
    "---\n",
    "\n",
    "### **Controlling the Number of Claims**\n",
    "Each sentence in the response and reference can be decomposed into multiple claims. The granularity of this decomposition is determined by **atomicity** and **coverage**.\n",
    "\n",
    "#### **Atomicity:**\n",
    "Controls how much a sentence is broken down:\n",
    "- **High Atomicity:** Breaks a sentence into fine-grained claims.\n",
    "- **Low Atomicity:** Keeps a sentence more intact with minimal decomposition.\n",
    "\n",
    "#### **Coverage:**\n",
    "Determines how much information is extracted:\n",
    "- **High Coverage:** Extracts all details from the original sentence.\n",
    "- **Low Coverage:** Focuses on key information, omitting minor details.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example of Atomicity and Coverage Adjustments**\n",
    "#### **High Atomicity & High Coverage:**\n",
    "```python\n",
    "scorer = FactualCorrectness(mode=\"precision\", atomicity=\"high\", coverage=\"high\")\n",
    "```\n",
    "**Original Sentence:**  \n",
    "> \"Marie Curie was a Polish and naturalized-French physicist and chemist who conducted pioneering research on radioactivity.\"\n",
    "\n",
    "**Decomposed Claims:**\n",
    "- \"Marie Curie was a Polish physicist.\"\n",
    "- \"Marie Curie was a naturalized-French physicist.\"\n",
    "- \"Marie Curie was a chemist.\"\n",
    "- \"Marie Curie conducted pioneering research on radioactivity.\"\n",
    "\n",
    "---\n",
    "\n",
    "### **Practical Application**\n",
    "- **High Atomicity & High Coverage** → Best for detailed fact-checking and claim extraction.\n",
    "- **Low Atomicity & Low Coverage** → Suitable for summarization or when only key facts are needed.\n",
    "\n",
    "This flexibility ensures the metric can be tailored to different levels of granularity based on the application.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways:**\n",
    "* Factual correctness evaluates the factual overlap between a response and a reference.  \n",
    "* It provides **precision**, **recall**, and **F1-score** options for measurement.  \n",
    "* Atomicity and coverage control the **granularity** of claim decomposition.  \n",
    "* Helps identify hallucinations (FP) and missing information (FN) in responses.  \n",
    "\n",
    "This metric is crucial for **evaluating Retrieval-Augmented Generation (RAG) systems** where factual consistency is a priority!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import FactualCorrectness\n",
    "from prompts.metrics.faithfulness.custom_nli_generator_prompt import MyNLIStatementPrompt\n",
    "\n",
    "factual_correctness = FactualCorrectness(\n",
    "    nli_prompt=MyNLIStatementPrompt()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Semantic Similarity**\n",
    "\n",
    "### TL;DR:\n",
    "> **How semantically similar are two texts?**\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition**:\n",
    "**Semantic Similarity** measures the degree of similarity between two texts. It uses an `embedding model` to convert the text into a high dimensional vector or so-called `embedding` and it computes the similarity using `cosine distance` for example between 2 such vectors. Two texts might contain very similar information semantically, however can be phrased in various ways. This metric determines if they are similar or close in meaning to each other regardless of the wording.\n",
    "\n",
    "---\n",
    "\n",
    "### **Approach**:\n",
    "Compute the emebedding of both the `response` and `reference`, then normalize the vectors and finally compute the score itself. The score will range between **0 to 1** with higher values indication higher similarity. This metric is great for assessing the `generator component` without requiring a LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import SemanticSimilarity\n",
    "\n",
    "semantic_similarity = SemanticSimilarity(\n",
    "    threshold=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, after all relevant metrics are initialized and customized as needed we can evaluate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ragas.evaluation import evaluate, EvaluationResult\n",
    "\n",
    "results: EvaluationResult = evaluate(\n",
    "    dataset=evaluation_dataset,\n",
    "    metrics=[\n",
    "        llm_context_precision,              # Metric 1\n",
    "        non_llm_context_precision,          # Metric 1\n",
    "        llm_context_recall,                 # Metric 2\n",
    "        non_llm_context_recall,             # Metric 2\n",
    "        context_entity_recall,              # Metric 3\n",
    "        noise_sensitivity_mode_relevant,    # Metric 4\n",
    "        noise_sensitivity_mode_irrelevant,  # Metric 4\n",
    "        response_relevancy,                 # Metric 5\n",
    "        faithfulness,                       # Metric 6\n",
    "        factual_correctness,                # Metric 7\n",
    "        semantic_similarity                  # Metric 8\n",
    "    ],\n",
    "    llm=langchain_llm,\n",
    "    embeddings=langchain_embeddings,\n",
    "    experiment_name=\"Evaluation of metrics with custom prompts\",\n",
    "    run_config=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Save results locally (optional)\n",
    "result_df: pd.DataFrame = results.to_pandas()\n",
    "result_df.to_csv(path='metrics_evaluation.csv', index=False)\n",
    "\n",
    "# Display metric scores\n",
    "results.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have supplied a RAGAs app token also upload to ragas.io (optional)\n",
    "results.upload()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
