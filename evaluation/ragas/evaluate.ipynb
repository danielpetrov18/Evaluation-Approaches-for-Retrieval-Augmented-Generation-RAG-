{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Retrieval-Augmented Generation (RAG) Applications: Importance and Impact  \n",
    "\n",
    "Evaluating RAG-based applications is crucial to ensure the reliability, relevance, and efficiency of their responses. Unlike using LLMs by themselves, RAG-based applications combine the power of Large-Language-Models with vector stores and retrieval components, which introduces complexity. Hence, evaluation of different components in RAG pipelines is crucial. \n",
    "\n",
    "#### Key Pitfalls in RAG Applications\n",
    "- **Hallucination**: Even with retrieved context, LLMs can still generate inaccurate or misleading responses.  \n",
    "- **Irrelevant Retrieval**: Poor retrieval results in the model relying on incomplete or incorrect context.  \n",
    "- **Latency & Efficiency**: Combining retrieval and generation can introduce delays, requiring performance optimization. \n",
    "\n",
    "#### Advantages of Proper Evaluation\n",
    "- **Improved Accuracy**: Ensures the factual correctness of responses by using a knowledge base.  \n",
    "- **Enhanced Relevance**: Helps refine retrieval mechanisms, ensuring responses are more context-aware.  \n",
    "- **Robustness & Adaptability**: Identifies failure points, enabling continuous improvements to model behavior.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "* Execute the script in the root of the `evaluation` folder.\n",
    "* If executing the script fails run: `chmod u+x setup.sh`.\n",
    "* It will install all required dependencies.\n",
    "* Finally, make sure you select it in the notebook by specifying `eval` as kernel.\n",
    "\n",
    "---\n",
    "\n",
    "*(OPTIONAL STEP)*\n",
    "\n",
    "**RAGAs** provides a cloud platform where a dataset and evaluation results can be stored and viewed.\n",
    "To use it follow this link: [RAGAs.io](https://app.ragas.io/).\n",
    "* Sign-up\n",
    "* Retrieve the **token**\n",
    "* Create a `.env` file with the following content:\n",
    "```bash\n",
    "RAGAS_APP_TOKEN=apt.......-9f6ed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the evaluation dataset\n",
    "\n",
    "According to **RAGAs** an **evaluation dataset** is a homogeneous collection of **data samples** with the sole purpose of measuring the capabilities and performance of an AI application.\n",
    "\n",
    "- **Structure**:\n",
    "\n",
    "    - Contains either **SingleTurnSample** or **MultiTurnSample** object instances, each of them representing a unique interaction between a **Persona** and the AI-system.\n",
    "\n",
    "    - **NOTE**: The dataset can contain **ONLY** a single type of samples. They cannot be mixed together into a single dataset.\n",
    "\n",
    "**Samples** represent a single unit of interaction with the underlying system. As mentioned they can be either **SingleTurnSample** or **MultiTurnSample**.\n",
    "\n",
    "For this project I focus solely on **SingleTurnSample** objects, since I'm evaluating independent queries, not multi-turn interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Dict\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EvaluationDataset, SingleTurnSample\n\u001b[1;32m      5\u001b[0m filepath: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease specify which dataset to evaluate (only the file name): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m goldens: List[Dict] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/eval/lib/python3.10/site-packages/ragas/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcache\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CacheInterface, DiskCacheBackend, cacher\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_schema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EvaluationDataset, MultiTurnSample, SingleTurnSample\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RunConfig\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/eval/lib/python3.10/site-packages/ragas/evaluation.py:20\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChainType, RagasTracer, new_group\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_schema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     EvaluationDataset,\n\u001b[1;32m     16\u001b[0m     EvaluationResult,\n\u001b[1;32m     17\u001b[0m     MultiTurnSample,\n\u001b[1;32m     18\u001b[0m     SingleTurnSample,\n\u001b[1;32m     19\u001b[0m )\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     21\u001b[0m     BaseRagasEmbeddings,\n\u001b[1;32m     22\u001b[0m     LangchainEmbeddingsWrapper,\n\u001b[1;32m     23\u001b[0m     embedding_factory,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExceptionInRunner\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexecutor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Executor\n",
      "File \u001b[0;32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/eval/lib/python3.10/site-packages/ragas/embeddings/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     BaseRagasEmbeddings,\n\u001b[1;32m      3\u001b[0m     HuggingfaceEmbeddings,\n\u001b[1;32m      4\u001b[0m     LangchainEmbeddingsWrapper,\n\u001b[1;32m      5\u001b[0m     LlamaIndexEmbeddingsWrapper,\n\u001b[1;32m      6\u001b[0m     embedding_factory,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhaystack_wrapper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HaystackEmbeddingsWrapper\n\u001b[1;32m     10\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseRagasEmbeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHaystackEmbeddingsWrapper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding_factory\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m ]\n",
      "File \u001b[0;32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/eval/lib/python3.10/site-packages/ragas/embeddings/base.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Embeddings\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpydantic_core\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CoreSchema, core_schema\n",
      "File \u001b[0;32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/eval/lib/python3.10/site-packages/langchain_openai/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI, ChatOpenAI\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAIEmbeddings, OpenAIEmbeddings\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAI, OpenAI\n\u001b[1;32m      5\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAzureOpenAIEmbeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m ]\n",
      "File \u001b[0;32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/eval/lib/python3.10/site-packages/langchain_openai/llms/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAI\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAzureOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/eval/lib/python3.10/site-packages/langchain_openai/llms/azure.py:13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Field, SecretStr, model_validator\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Self\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseOpenAI\n\u001b[1;32m     15\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mAzureOpenAI\u001b[39;00m(BaseOpenAI):\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1012\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:672\u001b[0m, in \u001b[0;36m_compile_bytecode\u001b[0;34m(data, name, bytecode_path, source_path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import List, Dict\n",
    "from ragas import EvaluationDataset, SingleTurnSample\n",
    "\n",
    "filepath: str = input(\"Please specify which dataset to evaluate (only the file name): \")\n",
    "\n",
    "goldens: List[Dict] = []\n",
    "try:\n",
    "    with open(file=f\"./datasets/{filepath}.jsonl\", mode=\"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if line.strip():  # Skip empty lines\n",
    "                goldens.append(json.loads(line))\n",
    "\n",
    "    samples: List[SingleTurnSample] = []\n",
    "    for golden in goldens:\n",
    "        single_turn_sample = SingleTurnSample(**golden)\n",
    "        samples.append(single_turn_sample)\n",
    "        \n",
    "    evaluation_dataset = EvaluationDataset(samples)\n",
    "except FileNotFoundError:\n",
    "    print(f\"File `./datasets/{filepath}.jsonl` containing goldens not found.\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error parsing JSONL file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "from ragas import RunConfig, DiskCacheBackend\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "run_config = RunConfig(\n",
    "    timeout=86400,    # 24 hours on waiting for a single operation\n",
    "    max_retries=20,   # Max retries before giving up\n",
    "    max_wait=600,     # Max wait between retries\n",
    "    max_workers=8,    # Concurrent requests\n",
    "    log_tenacity=True # Print retry attempts\n",
    ")\n",
    "\n",
    "# This stores data generation and evaluation results locally on disk\n",
    "# When using it for the first time, it will create a .cache folder\n",
    "# When using it again, it will read from that folder and finish almost instantly\n",
    "cacher = DiskCacheBackend(cache_dir=\".cache\")\n",
    "\n",
    "load_dotenv(\"../../env/rag.env\")\n",
    "\n",
    "langchain_llm = LangchainLLMWrapper(\n",
    "    langchain_llm=ChatOllama(\n",
    "        model=os.getenv(\"CHAT_MODEL\"),\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        temperature=float(os.getenv(\"TEMPERATURE\")),\n",
    "        num_ctx=int(os.getenv(\"LLM_CONTEXT_WINDOW_TOKENS\")),\n",
    "        format=\"json\"\n",
    "    ),\n",
    "    run_config=run_config,\n",
    "    cache=cacher\n",
    ")\n",
    "\n",
    "langchain_embeddings = LangchainEmbeddingsWrapper(\n",
    "    embeddings=OllamaEmbeddings(\n",
    "        model=os.getenv(\"EMBEDDING_MODEL\"),\n",
    "        base_url=\"http://localhost:11434\"\n",
    "    ),\n",
    "    run_config=run_config,\n",
    "    cache=cacher\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "A metric is quantatative measure used to evaluate the performance of a LLM-based application. Metrics help us assess how well an application as a whole or its individual components performs, with respect to the dataset we provide. With their help we can iterate towards a more *accurate*, *faithful* and *precise* LLM application.\n",
    "\n",
    "Metrics in **RAGAs** can be classified depending on the mechanism they evaluate the application as:\n",
    "\n",
    "- **LLM-based** - where a LLM is used to perform the evaluation. There might be more than one queries submitted to the LLM to evaluate the application on a single metric. Furthermore, this type of metrics mimic a user, since they tend to be quite non-deterministic and unpredictable. \n",
    "\n",
    "- **Non-LLM based** - where no LLM is required or used to perform the evaluation. These metrics tend to be much faster and predictable than the previous type. However, in my opinion are maybe not the best for evaluating a RAG system where a single query can be submitted multiple times and each time a new response might be generated.\n",
    "\n",
    "The previously mentioned metric types can be further classified into **SingleTurnMetric** and **MultiTurnMetric** respectively. I would like to note again that in this project the **SingleTurnMetric** would be relevant and used.\n",
    "\n",
    "---\n",
    "\n",
    "![Metric types in RAGAs](../../img/metrics_mindmap.webp \"Metrics RAGAs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Context Precision**\n",
    "\n",
    "> **TL;DR:** What fraction of the retrieved chunks are **actually relevant**?\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition**:\n",
    "\n",
    "**Context Precision** is a crucial metric for evaluating the **retrieval capabilities** of a RAG (Retrieval-Augmented Generation) system. Since the **LLM's response** depends heavily on the **context retrieved from the knowledge base**, it is essential to have a reliable retrieval mechanism that fetches **only relevant** information.  \n",
    "\n",
    "By achieving **high context precision**, the system ensures that the retrieved information is **more relevant** and **ranked higher**, leading to **more accurate responses** and **reduced hallucinations**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Formula:**\n",
    "For each retrieved chunk at **rank (k)**:\n",
    "\n",
    "**Precision@k** = $\\frac{\\text{Number of relevant chunks at rank k}}{\\text{Total number of retrieved chunks at rank k}} = \\frac{\\text{true positives @ k}}{\\text{true positives @ k + false positives @ k}}$\n",
    "\n",
    "We then compute **Context Precision@K** as the **mean of all Precision@k values**, weighted by relevance:\n",
    "\n",
    "**Context Precision@K** = $\\frac{\\sum_{k=1}^{K} \\text{Precision@k} \\times \\text{Relevance}(k)}{\\text{Total number of relevant chunks in top K results}}$\n",
    "\n",
    "Where:\n",
    "- **Precision@k** is the proportion of relevant chunks at rank (k).\n",
    "- **Relevance(k)** is a binary indicator\n",
    "    - 1 if the chunk at rank (k) is relevant\n",
    "    - 0 otherwise\n",
    "- **K** is the total number of retrieved chunks\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Calculation**\n",
    "Suppose our **retriever fetches 4 chunks**, and **2 of them** are relevant. We calculate **Precision@k** at each rank:\n",
    "\n",
    "| Rank \\( k \\) | Retrieved Chunk | Relevant? | Precision@k |\n",
    "|-------------|----------------|------------|-------------|\n",
    "| 1           | ✅ Relevant      | ✅ (1)      | \\( $\\frac{1}{1}$ = 1.00 \\) |\n",
    "| 2           | ❌ Not Relevant  | ❌ (0)      | \\( $\\frac{1}{2}$ = 0.50 \\) |\n",
    "| 3           | ✅ Relevant      | ✅ (1)      | \\( $\\frac{2}{3}$ = 0.67 \\) |\n",
    "| 4           | ❌ Not Relevant  | ❌ (0)      | \\( $\\frac{2}{4}$ = 0.50 \\) |\n",
    "\n",
    "Now, using the **Context Precision formula**:\n",
    "\n",
    "\n",
    "**Context Precision@4** = $\\frac{(1.00 \\times 1) + (0.50 \\times 0) + (0.67 \\times 1) + (0.50 \\times 0)}{2} = $\n",
    "\n",
    "= $\\frac{1.00 + 0 + 0.67 + 0}{2} = \\frac{1.67}{2}$ = 0.835\n",
    "\n",
    "Thus, **Context Precision@4 = 0.835**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why is This Useful?**\n",
    "\n",
    "- **Evaluates Retriever Quality** → Ensures retrieved chunks contain **relevant** information.\n",
    "\n",
    "- **Also evaluates the Re-ranker capabilities** -> Ensures relevant chunks are ranked higher.\n",
    "\n",
    "- **Improves RAG Performance** → Helps **reduce hallucinations** and improves **LLM accuracy**.\n",
    "\n",
    "- **Higher values** for this metric would signify a good retriever, ranking **relevant chunks** high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    LLMContextPrecisionWithReference,\n",
    "    LLMContextPrecisionWithoutReference,\n",
    "    NonLLMContextPrecisionWithReference\n",
    ")\n",
    "from prompts.custom_context_precision_prompt import MyContextPrecisionPrompt\n",
    "\n",
    "# For each piece of context, the LLM determines whether or not it is relevant for answering the input \n",
    "# with respect to the expected output\n",
    "llm_context_precision_with_re = LLMContextPrecisionWithReference(\n",
    "    context_precision_prompt=MyContextPrecisionPrompt(),\n",
    "    max_retries=5\n",
    ")\n",
    "\n",
    "# Same metric, but instead of reference the actual response is used\n",
    "llm_context_precision_without_re = LLMContextPrecisionWithoutReference(\n",
    "    context_precision_prompt=MyContextPrecisionPrompt(),    \n",
    "    max_retries=5\n",
    ")\n",
    "\n",
    "# This particular version uses a distance measure to determine the semantic similarity\n",
    "# between chunks from the retrieved context and reference context.\n",
    "# It builds a cartesian product coupling each reference context with context from the retrieved nodes.\n",
    "# For each couple a score is computed and the maximum is taken.\n",
    "# Finally, the metric removes all scores that do not exceed the threshold and the Context@K is computed using the \n",
    "# already known formula. The only difference here is that no LLM is actually used.\n",
    "context_precision_with_reference = NonLLMContextPrecisionWithReference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Context Recall**\n",
    "\n",
    "### TL;DR:\n",
    "> **How many of the relevant chunks were actually retrieved? Did we miss relevant chunks?**\n",
    "\n",
    "---\n",
    "\n",
    "### **Defition**:\n",
    "\n",
    "**Context Recall** measures how many of the **relevant documents (or pieces of information)** were successfully retrieved. This metric ensures that the retrieval process does not **miss important information** that could be used to generate a response. A **higher recall** means that fewer relevant documents were left out, making it crucial for applications that prioritize completeness over precision. Since recall focuses on **not missing relevant data**, it always requires a reference set for comparison.\n",
    "\n",
    "---\n",
    "\n",
    "### LLM-Based Context Recall\n",
    "\n",
    "This metric evaluates recall by analyzing the **claims** in the reference (expected) response and checking whether they can be attributed to the retrieved context. In an ideal scenario, **all claims** in the reference answer should be **supported** by the retrieved context.\n",
    "\n",
    "### **Formula**:\n",
    "\n",
    "**Context Recall** = $\\frac{\\text{Number of claims in the reference supported by the retrieved context}}{\\text{Total number of claims in the reference}}$\n",
    "\n",
    "A recall score **closer to 1** indicates that most relevant data has been successfully retrieved, while a **lower recall** means that key information was missed.\n",
    "\n",
    "### **Non-LLM-Based Context Recall**\n",
    "\n",
    "### **Definition**:\n",
    "\n",
    "**Non-LLM-Based Context Recall** measures how many of the **relevant documents (or pieces of information)** were successfully retrieved **without relying on a language model** for evaluation. This metric ensures that the retrieval process does not **miss important information** that could be used to generate a response. A **higher recall** means that fewer relevant documents were left out, making it crucial for applications that prioritize completeness over precision. Since recall focuses on **not missing relevant data**, it always requires a reference set for comparison.\n",
    "\n",
    "---\n",
    "\n",
    "### **How It Works**\n",
    "This metric calculates recall based on **string similarity** rather than using an LLM to assess the retrieved content. It follows these steps:\n",
    "\n",
    "1. **Extract Retrieved and Reference Contexts**  \n",
    "   - Uses `retrieved_contexts` (what was fetched) and `reference_contexts` (ground truth).  \n",
    "2. **Compute Similarity Scores**  \n",
    "   - Compares each **retrieved context** against each **reference context** using a similarity function.\n",
    "3. **Find the Best Match for Each Reference Context**  \n",
    "   - For each **reference context**, it selects the **retrieved context** with the **highest similarity score**.\n",
    "4. **Apply a Threshold (`0.5` by default)**  \n",
    "   - If the similarity score is **above the threshold**, the reference context is considered **retrieved successfully**.\n",
    "5. **Compute Recall Score**  \n",
    "   - The recall score is the proportion of reference contexts that have at least one matching retrieved context **above the threshold**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Formula**\n",
    "\n",
    "**Non-LLM-Based Context Recall** =  $\\frac{\\text{Number of reference contexts with a high-scoring retrieved match}}{\\text{Total number of reference contexts}} $\n",
    "\n",
    "Where:\n",
    "- A **retrieved match** is valid if its **similarity score > threshold**.\n",
    "- The **threshold** is configurable (default = `0.5`).\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Calculation**\n",
    "#### **Given:**\n",
    "- `retrieved_contexts = [\"text A\", \"text B\", \"text C\"]`\n",
    "- `reference_contexts = [\"text X\", \"text Y\"]`\n",
    "- **Similarity scores:**  \n",
    "  - `text A` ↔ `text X` = **0.7** ✅\n",
    "  - `text B` ↔ `text X` = **0.4** ❌\n",
    "  - `text C` ↔ `text X` = **0.6** ✅ (max = 0.7)\n",
    "  - `text A` ↔ `text Y` = **0.2** ❌\n",
    "  - `text B` ↔ `text Y` = **0.5** ✅\n",
    "  - `text C` ↔ `text Y` = **0.3** ❌ (max = 0.5)\n",
    "\n",
    "#### **Final Score Computation**\n",
    "- `text X` has a match (`0.7 > 0.5`) → ✅ **1**\n",
    "- `text Y` has a match (`0.5 == 0.5`) → ✅ **1**\n",
    "- **Score = (1+1) / 2 = 1.0 (100%)** ✅\n",
    "\n",
    "---\n",
    "\n",
    "### **Why is This Useful?**\n",
    "- **Evaluates Retriever Performance Without an LLM** → Ensures relevance using **string similarity**, avoiding model biases.\n",
    "- **Helps Optimize RAG Pipelines** → Higher recall ensures more **comprehensive** context retrieval.\n",
    "- **Works Well with Threshold-Based Comparisons** → Adjustable strictness for different applications.\n",
    "\n",
    "A recall score **closer to 1** means that most relevant contexts were retrieved, while a **lower recall** suggests that key information was missed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import LLMContextRecall, NonLLMContextRecall   \n",
    "from prompts.custom_context_recall_prompt import MyContextRecallPrompt\n",
    "\n",
    "# For each claim in the response, verify if it can be supported by the retrieved context\n",
    "llm_context_recall = LLMContextRecall(\n",
    "    context_recall_prompt=MyContextRecallPrompt(),\n",
    "    max_retries=5\n",
    ")\n",
    "\n",
    "context_recall = NonLLMContextRecall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Context Entities Recall**\n",
    "\n",
    "### TL;DR:\n",
    "> **It is a measure of what fraction of entities are recalled from reference**\n",
    "\n",
    "---\n",
    "\n",
    "### Definition:\n",
    "`ContextEntityRecall` metric measures the recall of the retrieved context, based on the number of entities present in both `reference` and `retrieved_contexts`, relative to the total number of entities in the `reference`. In simple terms, it evaluates how well the retrieved contexts capture the entities from the original reference.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula:\n",
    "\n",
    "To compute this metric, we define two sets:\n",
    "\n",
    "- **RE**: The set of entities in the reference.\n",
    "- **RCE**: The set of entities in the retrieved contexts.\n",
    "\n",
    "We determine the number of entities common to both sets (**RCE** $\\cap$  **RE**) and divide it by the total number of entities in the reference (**RE**). The formula is:\n",
    "\n",
    "$ \\text{Context Entity Recall} = \\frac{\\text{Number of common entities between RCE and RE}}{\\text{Total number of entities in RE}} = \\frac{\\text{RCE } \\cap \\text{ RE}}{\\text{RE}} $\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Calculation**\n",
    "\n",
    "#### **Scenario:**\n",
    "\n",
    "We have a **reference** and two retrieved contexts:\n",
    "\n",
    "**Reference:**\n",
    "> \"The Taj Mahal is an ivory-white marble mausoleum on the right bank of the river Yamuna in the Indian city of Agra. It was commissioned in 1631 by the Mughal emperor Shah Jahan to house the tomb of his favorite wife, Mumtaz Mahal.\"\n",
    "\n",
    "**Retrieved Contexts:**\n",
    "\n",
    "- **High Entity Recall Context:**\n",
    "  > \"The Taj Mahal is a symbol of love and architectural marvel located in Agra, India. It was built by the Mughal emperor Shah Jahan in memory of his beloved wife, Mumtaz Mahal. The structure is renowned for its intricate marble work and beautiful gardens surrounding it.\"\n",
    "\n",
    "- **Low Entity Recall Context:**\n",
    "  > \"The Taj Mahal is an iconic monument in India. It is a UNESCO World Heritage Site and attracts millions of visitors annually. The intricate carvings and stunning architecture make it a must-visit destination.\"\n",
    "\n",
    "#### **Step 1: Extract Entities**\n",
    "\n",
    "Entities in the **Reference (RE)**:\n",
    "> \\[\"Taj Mahal\", \"Yamuna\", \"Agra\", \"1631\", \"Shah Jahan\", \"Mumtaz Mahal\"\\]\n",
    "\n",
    "Entities in **High Recall Context (RCE1)**:\n",
    "> \\[\"Taj Mahal\", \"Agra\", \"Shah Jahan\", \"Mumtaz Mahal\", \"India\"\\]\n",
    "\n",
    "Entities in **Low Recall Context (RCE2)**:\n",
    "> \\[\"Taj Mahal\", \"UNESCO\", \"India\"\\]\n",
    "\n",
    "#### **Step 2: Compute Context Entity Recall**\n",
    "\n",
    "For **High Recall Context (RCE1)**:\n",
    "$ \\text{Context Entity Recall} = \\frac{4}{6} = 0.67 $ \n",
    "\n",
    "For **Low Recall Context (RCE2)**:\n",
    "$ \\text{Context Entity Recall} = \\frac{1}{6} = 0.17 $\n",
    "\n",
    "Since the first context retains more entities from the reference, it has a **higher entity recall**, indicating it is **more comprehensive** in capturing the essential information.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Insights:**\n",
    "\n",
    "* **Higher Entity Recall Improves Answer Completeness:** If entities are important for context, a higher recall ensures critical information is included in the retrieved context.\n",
    "\n",
    "* **Useful for Fact-Heavy Applications:** Applications in **legal, medical, and historical domains** benefit significantly from high entity recall.\n",
    "\n",
    "* **Balances with Context Precision:** While high recall is beneficial, retrieving too many irrelevant entities can introduce noise. **Optimizing both recall and precision** is crucial for effective retrieval in RAG systems.\n",
    "\n",
    "* **Comparison of Retrieval Mechanisms:** If two retrieval mechanisms fetch different contexts, **Context Entity Recall** helps determine which one is better at preserving key entities from the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import ContextEntityRecall\n",
    "from prompts.custom_context_entities_recall_prompt import MyContextEntitiesRecallPrompt\n",
    "\n",
    "context_entity_recall = ContextEntityRecall(\n",
    "    context_entity_recall_prompt=MyContextEntitiesRecallPrompt(),\n",
    "    max_retries=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Noise Sensitivity**\n",
    "\n",
    "### TL;DR:\n",
    "> **How prone is the system to generating incorrect claims from retrieved contexts?**\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition**:\n",
    "**Noise Sensitivity** measures how often a system makes errors by providing incorrect responses when utilizing either relevant or irrelevant retrieved documents. This metric evaluates the robustness of a Retrieval-Augmented Generation (RAG) system against potentially misleading or noisy information.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conceptual Insight**:\n",
    "The metric fundamentally tests how easily an LLM can be \"tricked\" into generating factually incorrect responses. It distinguishes between two critical scenarios:\n",
    "\n",
    "1. **Relevant Context Noise**: \n",
    "   - More subtle and dangerous\n",
    "   - Noise is camouflaged within seemingly pertinent information\n",
    "   - High risk of inadvertently incorporating incorrect claims\n",
    "\n",
    "2. **Irrelevant Context Noise**:\n",
    "   - A robust LLM should completely resist this\n",
    "   - Contexts unrelated to the user's query\n",
    "   - Zero tolerance for incorporating unrelated information\n",
    "\n",
    "---\n",
    "\n",
    "### LLM-Based Noise Sensitivity\n",
    "\n",
    "The metric assesses noise sensitivity by decomposing the response and reference into individual claims, then analyzing:\n",
    "- Whether claims are correct according to the ground truth\n",
    "- Whether incorrect claims can be attributed to retrieved contexts (either relevant or irrelevant)\n",
    "\n",
    "---\n",
    "\n",
    "### **Calculation Approach**:\n",
    "\n",
    "1. **Decompose Statements**: Break down the reference and response into individual claims using an LLM evaluator\n",
    "2. **Context Verification**: Check if claims can be supported by retrieved contexts using NLI techniques\n",
    "3. **Identify Relevant Contexts**: Determine which contexts support the ground truth\n",
    "4. **Map Response Claims**: Map each claim in the response to contexts that support it\n",
    "5. **Identify Incorrect Claims**: Determine which response claims are unsupported by the ground truth\n",
    "6. **Calculate Sensitivity**: Compute the proportion of incorrect claims derived from contexts\n",
    "\n",
    "---\n",
    "\n",
    "### **Formula**:\n",
    "\n",
    "Let's define the following:\n",
    "- $S_r$ = Set of response statements\n",
    "- $S_i$ = Set of incorrect statements in the response (not supported by ground truth)\n",
    "- $C_r$ = Set of relevant contexts (contexts that support ground truth)\n",
    "- $C_i$ = Set of irrelevant contexts (contexts that don't support ground truth)\n",
    "- $f(s,c)$ = Function that returns 1 if statement $s$ can be inferred from context $c$, 0 otherwise\n",
    "\n",
    "For each statement $s$ in the response, we can define:\n",
    "- $\\text{relevant\\_faithful}(s) = \\max_{c \\in C_r} f(s,c)$ (1 if statement can be inferred from any relevant context)\n",
    "- $\\text{irrelevant\\_faithful}(s) = \\max_{c \\in C_i} f(s,c) \\cdot (1 - \\text{relevant\\_faithful}(s))$ (1 if statement can be inferred from irrelevant context and not from any relevant context)\n",
    "\n",
    "Then:\n",
    "\n",
    "**Relevant Mode Noise Sensitivity**:\n",
    "$$\\text{NS}_{\\text{relevant}} = \\frac{\\sum_{s \\in S_i} \\text{relevant\\_faithful}(s)}{|S_r|}$$\n",
    "\n",
    "**Irrelevant Mode Noise Sensitivity**:\n",
    "$$\\text{NS}_{\\text{irrelevant}} = \\frac{\\sum_{s \\in S_i} \\text{irrelevant\\_faithful}(s)}{|S_r|}$$\n",
    "\n",
    "Where $|S_r|$ is the total number of statements in the response.\n",
    "\n",
    "In plain English:\n",
    "- Relevant mode: Proportion of response statements that are both incorrect and supported by relevant contexts\n",
    "   - incorrect statement means that a there's a contradiction between a statement in the `response` and `reference`\n",
    "   - relevant context is a context, which supports information from the `reference`\n",
    "- Irrelevant mode: Proportion of response statements that are both incorrect and supported only by irrelevant contexts\n",
    "   - irrelevant context is a context, which doesn't support any statement in the `reference`\n",
    "\n",
    "A score **closer to 0** indicates better performance, suggesting:\n",
    "- Fewer incorrect claims\n",
    "- Less influence from noisy or irrelevant contexts\n",
    "- More robust response generation\n",
    "\n",
    "---\n",
    "\n",
    "### **Modes**:\n",
    "- **Relevant Mode** (default): Focuses on noise in relevant contexts\n",
    "- **Irrelevant Mode**: Analyzes noise from irrelevant retrieved documents\n",
    "\n",
    "---\n",
    "\n",
    "### **Evaluation Expectations**:\n",
    "- **Relevant Context**: Some noise tolerance, but should be minimal\n",
    "- **Irrelevant Context**: Virtually zero noise should be incorporated\n",
    "  - A high-quality LLM should completely disregard irrelevant information\n",
    "  - Any noise from irrelevant contexts indicates a significant vulnerability\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Scenario**:\n",
    "Consider a query about the Life Insurance Corporation of India (LIC):\n",
    "- **Question**: \"What is the Life Insurance Corporation of India (LIC) known for?\"\n",
    "- **Reference**: \"The Life Insurance Corporation of India (LIC) is the largest insurance company in India, established in 1956 through the nationalization of the insurance industry. It is known for managing a large portfolio of investments.\"\n",
    "- **Response**: \"The Life Insurance Corporation of India (LIC) is the largest insurance company in India, known for its vast portfolio of investments. LIC contributes to the financial stability of the country.\"\n",
    "- **Retrieved Contexts**: Mix of relevant information about LIC and irrelevant information about the Indian economy\n",
    "- **Noise Sensitivity Score**: 0.33 (one incorrect claim out of three total claims)\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Insights**:\n",
    "- Helps identify system vulnerabilities to hallucination\n",
    "- Provides a quantitative measure of response reliability\n",
    "- Distinguishes between subtle (relevant) and gross (irrelevant) information distortions\n",
    "- Serves as a critical component in comprehensive RAG system evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import NoiseSensitivity\n",
    "from prompts.faithfulness.custom_nli_generator_prompt import MyNLIStatementPrompt\n",
    "from prompts.faithfulness.custom_statement_generator_prompt import MyStatementGeneratorPrompt\n",
    "\n",
    "noise_sensitivity_mode_relevant = NoiseSensitivity(\n",
    "    nli_statements_prompt=MyNLIStatementPrompt(),\n",
    "    statement_generator_prompt=MyStatementGeneratorPrompt(),\n",
    "    max_retries=5\n",
    ")\n",
    "\n",
    "noise_sensitivity_mode_irrelevant = NoiseSensitivity(\n",
    "    mode=\"irrelevant\",\n",
    "    nli_statements_prompt=MyNLIStatementPrompt(),\n",
    "    statement_generator_prompt=MyStatementGeneratorPrompt(),\n",
    "    max_retries=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Response Relevancy**\n",
    "\n",
    "### TL;DR:\n",
    "> **How well does the answer address the original user query?**\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition**:\n",
    "**Response Relevancy** measures how closely an AI-generated response aligns with the original user input. It evaluates the answer's ability to directly and appropriately address the user's question, penalizing responses that are incomplete, off-topic, or include unnecessary details. It doesn't judge the **factual accuracy**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conceptual Insight**:\n",
    "The metric focuses on the semantic alignment between:\n",
    "- The original user query\n",
    "- The generated response\n",
    "\n",
    "Key evaluation criteria:\n",
    "1. **Direct Address**: Does the answer directly tackle the user's question?\n",
    "2. **Information Completeness**: Does the response provide sufficient information?\n",
    "3. **Topic Coherence**: Does the answer stay focused on the query's intent?\n",
    "\n",
    "---\n",
    "\n",
    "### **Calculation Approach**:\n",
    "\n",
    "1. **Question Generation**: \n",
    "   - Use the LLM to generate artificial questions based on the response\n",
    "   - Default is to create 3 variant questions\n",
    "   - These questions should capture the essence of the response\n",
    "\n",
    "2. **Semantic Similarity**:\n",
    "   - Compute cosine similarity between:\n",
    "     - Original user input embedding\n",
    "     - Embeddings of generated questions\n",
    "   - Measures how closely the questions match the original query\n",
    "\n",
    "3. **Scoring**:\n",
    "   - Average the cosine similarity scores\n",
    "   - Higher scores indicate better relevance\n",
    "   - Scores typically range between 0 and 1\n",
    "\n",
    "---\n",
    "\n",
    "### **Formula**:\n",
    "\n",
    "**Response Relevancy** = $\\frac{\\sum_{i=1}^{N} \\text{cosine\\_similarity}(E_{g_{i}}, E_{o})}{N}$ \n",
    "\n",
    "Where:\n",
    "- $E_{g_{i}}$: Embedding of the i-th generated question\n",
    "- $E_{o}$: Embedding of the original user input\n",
    "- N: Number of generated questions (default: 3)\n",
    "\n",
    "---\n",
    "\n",
    "### **Evaluation Expectations**:\n",
    "- **High Score (Close to 1)**: \n",
    "  - Answer directly addresses the query\n",
    "  - Comprehensive and focused response\n",
    "  - Minimal irrelevant information\n",
    "\n",
    "- **Low Score (Close to 0)**: \n",
    "  - Response is off-topic\n",
    "  - Incomplete or evasive answer\n",
    "  - Includes excessive unrelated details\n",
    "\n",
    "---\n",
    "\n",
    "### **Important Limitations**:\n",
    "- **No Factuality Check**: \n",
    "  - Measures relevance, not accuracy\n",
    "  - Does not verify the truthfulness of the response\n",
    "- **Embedding-Based**: \n",
    "  - Relies on semantic similarity\n",
    "  - May not catch nuanced relevance\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Scenario**:\n",
    "- **User Query**: \"When was the first Super Bowl?\"\n",
    "- **Good Response**: \"The first Super Bowl was held on Jan 15, 1967, between the Green Bay Packers and Kansas City Chiefs.\"\n",
    "- **Poor Response**: \"Football is a popular sport in the United States with many interesting historical moments.\"\n",
    "\n",
    "### **Key Insights**:\n",
    "- Helps evaluate response quality beyond simple keyword matching\n",
    "- Provides a quantitative measure of semantic alignment\n",
    "- Supports improving AI system's query understanding\n",
    "- Identifies potential issues with off-topic or unfocused responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import ResponseRelevancy\n",
    "from prompts.custom_response_relevance_prompt import MyResponseRelevancePrompt\n",
    "\n",
    "response_relevancy = ResponseRelevancy(\n",
    "    question_generation=MyResponseRelevancePrompt()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Faithfulness**\n",
    "\n",
    "### TL;DR:\n",
    "> **How factually consistent is the response with the retrieved context?**\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition**:\n",
    "The **Faithfulness** metric measures how factually consistent a response is with the retrieved context. It ranges from 0 to 1, with higher scores indicating better consistency.\n",
    "\n",
    "A response is considered faithful if all of its claims can be supported by the retrieved context.\n",
    "\n",
    "---\n",
    "\n",
    "### **Calculation Approach**:\n",
    "\n",
    "1. **Identify Claims**: \n",
    "   - Break down the response into individual statements\n",
    "   - Examine each claim systematically\n",
    "\n",
    "2. **Context Verification**:\n",
    "   - Check each claim to see if it can be inferred from the retrieved context\n",
    "   - Determine the support level of each statement\n",
    "\n",
    "3. **Scoring**:\n",
    "   - Compute the faithfulness score using the formula:\n",
    "     \n",
    "     $\\text{Faithfulness Score} = \\frac{\\text{Number of claims supported by the retrieved context}}{\\text{Total number of claims in the response}}$\n",
    "\n",
    "---\n",
    "\n",
    "### **Example**:\n",
    "\n",
    "**Question**: Where and when was Einstein born?\n",
    "\n",
    "**Context**: Albert Einstein (born 14 March 1879) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time.\n",
    "\n",
    "**High Faithfulness Answer**: Einstein was born in Germany on 14th March 1879.\n",
    "\n",
    "**Low Faithfulness Answer**: Einstein was born in Germany on 20th March 1879.\n",
    "\n",
    "#### Calculation Steps:\n",
    "- **Step 1**: Break the generated answer into individual statements\n",
    "- **Step 2**: Verify if each statement can be inferred from the given context\n",
    "- **Step 3**: Apply the faithfulness formula\n",
    "\n",
    "---\n",
    "\n",
    "### **Evaluation Expectations**:\n",
    "- **Perfect Score (1.0)**: \n",
    "  - All claims are directly supported by the context\n",
    "  - No extraneous or unsupported information\n",
    "\n",
    "- **Partial Score**: \n",
    "  - Some claims are supported\n",
    "  - Partial consistency with the retrieved context\n",
    "\n",
    "- **Low Score (Close to 0)**: \n",
    "  - Most claims cannot be verified\n",
    "  - Significant deviation from the original context\n",
    "\n",
    "---\n",
    "\n",
    "### **Advanced Verification**:\n",
    "**HHEM-2.1-Open** can be used as a classifier model to:\n",
    "- Detect hallucinations in LLM-generated text\n",
    "- Cross-check claims with the given context\n",
    "- Efficiently determine claim inferability\n",
    "- Avoid **biases** by not using `LLM-As-A-Judge`\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Insights**:\n",
    "- Measures the factual consistency of AI-generated responses\n",
    "- Helps identify potential hallucinations or fabrications\n",
    "- Provides a quantitative assessment of contextual alignment\n",
    "- Supports improving the reliability of AI-generated content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import Faithfulness\n",
    "from prompts.faithfulness.custom_nli_generator_prompt import MyNLIStatementPrompt\n",
    "from prompts.faithfulness.custom_statement_generator_prompt import MyStatementGeneratorPrompt\n",
    "\n",
    "faithfulness = Faithfulness(\n",
    "    nli_statements_prompt=MyNLIStatementPrompt(),\n",
    "    statement_generator_prompt=MyStatementGeneratorPrompt(),\n",
    "    max_retries=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Factual Correctness**\n",
    "\n",
    "### TL;DR:\n",
    "> **How factually consistent is a model's response with the ground truth?**\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition**:\n",
    "**Factual Correctness** measures the degree to which a model's `response` aligns with the `reference` by assessing the factual consistency between the two. It does this by breaking down both the response and reference into distinct claims and then determining whether these claims match using natural language inference (NLI). This metric helps evaluate how accurately the generated response retains factual integrity.\n",
    "\n",
    "The factual correctness score ranges from **0 to 1**, where higher values indicate better factual alignment.\n",
    "\n",
    "---\n",
    "\n",
    "### **Claim Breakdown and Classification**:\n",
    "This metric identifies three types of claims:\n",
    "- **True Positives (TP):** Claims that are present in both the response and the reference.\n",
    "- **False Positives (FP):** Claims that are in the response but not supported by the reference.\n",
    "- **False Negatives (FN):** Claims that are in the reference but missing from the response.\n",
    "\n",
    "---\n",
    "\n",
    "### **Calculation Modes**:\n",
    "The metric can be computed in three different modes:\n",
    "\n",
    "#### **Precision Mode:**\n",
    "> Measures how much of the information in the response is factually correct.\n",
    "\n",
    "$ \\text{Precision} = \\frac{TP}{TP + FP} $\n",
    "\n",
    "- High precision means the response avoids including unsupported claims.\n",
    "- Penalizes responses that introduce hallucinated information.\n",
    "\n",
    "#### **Recall Mode:**\n",
    "> Measures how much of the reference information is retained in the response.\n",
    "\n",
    "$ \\text{Recall} = \\frac{TP}{TP + FN} $\n",
    "\n",
    "- High recall means the response includes all important reference claims.\n",
    "- Penalizes responses that omit key factual details.\n",
    "\n",
    "#### **F1 Mode (Default):**\n",
    "> Balances precision and recall for a comprehensive factual correctness score.\n",
    "\n",
    "$ \\text{F1} = \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} $\n",
    "\n",
    "---\n",
    "\n",
    "### **Controlling the Number of Claims**\n",
    "Each sentence in the response and reference can be decomposed into multiple claims. The granularity of this decomposition is determined by **atomicity** and **coverage**.\n",
    "\n",
    "#### **Atomicity:**\n",
    "Controls how much a sentence is broken down:\n",
    "- **High Atomicity:** Breaks a sentence into fine-grained claims.\n",
    "- **Low Atomicity:** Keeps a sentence more intact with minimal decomposition.\n",
    "\n",
    "#### **Coverage:**\n",
    "Determines how much information is extracted:\n",
    "- **High Coverage:** Extracts all details from the original sentence.\n",
    "- **Low Coverage:** Focuses on key information, omitting minor details.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example of Atomicity and Coverage Adjustments**\n",
    "#### **High Atomicity & High Coverage:**\n",
    "```python\n",
    "scorer = FactualCorrectness(mode=\"precision\", atomicity=\"high\", coverage=\"high\")\n",
    "```\n",
    "**Original Sentence:**  \n",
    "> \"Marie Curie was a Polish and naturalized-French physicist and chemist who conducted pioneering research on radioactivity.\"\n",
    "\n",
    "**Decomposed Claims:**\n",
    "- \"Marie Curie was a Polish physicist.\"\n",
    "- \"Marie Curie was a naturalized-French physicist.\"\n",
    "- \"Marie Curie was a chemist.\"\n",
    "- \"Marie Curie conducted pioneering research on radioactivity.\"\n",
    "\n",
    "---\n",
    "\n",
    "### **Practical Application**\n",
    "- **High Atomicity & High Coverage** → Best for detailed fact-checking and claim extraction.\n",
    "- **Low Atomicity & Low Coverage** → Suitable for summarization or when only key facts are needed.\n",
    "\n",
    "This flexibility ensures the metric can be tailored to different levels of granularity based on the application.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways:**\n",
    "* Factual correctness evaluates the factual overlap between a response and a reference.  \n",
    "* It provides **precision**, **recall**, and **F1-score** options for measurement.  \n",
    "* Atomicity and coverage control the **granularity** of claim decomposition.  \n",
    "* Helps identify hallucinations (FP) and missing information (FN) in responses.  \n",
    "\n",
    "This metric is crucial for **evaluating Retrieval-Augmented Generation (RAG) systems** where factual consistency is a priority!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import FactualCorrectness\n",
    "from prompts.faithfulness.custom_nli_generator_prompt import MyNLIStatementPrompt\n",
    "\n",
    "factual_correctness = FactualCorrectness(\n",
    "    nli_prompt=MyNLIStatementPrompt()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Answer Correctness**\n",
    "\n",
    "### TL;DR:  \n",
    "> **How well does a model’s response align with the ground truth in terms of factual correctness and completeness?**\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition**:  \n",
    "**Answer Correctness** evaluates how accurately a model-generated response corresponds to a given ground truth. It does this by breaking down both texts into discrete factual statements and determining whether these statements match. The metric assesses correctness through a combination of **factual alignment** and **semantic similarity**. Its based on the `Factual Correctness` metric and it adds `Semantic Similarity` to it. So its like an enhanced version of `Factual Correctness`.\n",
    "\n",
    "The correctness score ranges from **0 to 1**, where higher values indicate better alignment with the ground truth.\n",
    "\n",
    "---\n",
    "\n",
    "### **Statement Breakdown and Classification**:  \n",
    "To assess correctness, the response is decomposed into factual statements, which are then categorized as follows:\n",
    "\n",
    "- **True Positives (TP):** Statements in the response that are also in the ground truth.\n",
    "- **False Positives (FP):** Statements in the response that are not supported by the ground truth.\n",
    "- **False Negatives (FN):** Statements in the ground truth that are missing from the response.\n",
    "\n",
    "---\n",
    "\n",
    "### **Calculation Formula**:  \n",
    "The **F1 Score** used for Answer Correctness is defined as:\n",
    "\n",
    "$ F1 = \\frac{|TP|}{|TP| + 0.5 \\times (|FP| + |FN|)} $ \n",
    "\n",
    "- This formula ensures a balance between **precision (avoiding hallucinated facts)** and **recall (retaining key information).**\n",
    "- The **0.5 weight on FP and FN** prevents excessive penalization while ensuring factual accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### **Computation Steps**:  \n",
    "1. **Extract factual statements** from both the response and the ground truth.  \n",
    "2. **Classify statements** into TP, FP, and FN using a **Correctness Classifier** based on natural language inference (NLI).  \n",
    "3. **Compute factual correctness** using the F1-like score.  \n",
    "4. **Compute semantic similarity** if slight wording variations are acceptable.  \n",
    "5. **Combine both scores** using predefined weight factors (default: factual correctness = 0.75, semantic similarity = 0.25).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Computation Modes**:  \n",
    "\n",
    "#### **Factual Alignment Mode (Default)**  \n",
    "> Focuses on exact factual correctness using the TP, FP, and FN classification.  \n",
    "\n",
    "F1 = $\\frac{|TP|}{|TP| + 0.5 \\times (|FP| + |FN|)} $\n",
    "\n",
    "- Ensures that incorrect statements (FP) and missing information (FN) reduce the score.\n",
    "\n",
    "#### **Semantic Similarity Mode**  \n",
    "> Measures overall alignment by incorporating semantic closeness between response and ground truth.  \n",
    "\n",
    "- Uses **embeddings-based similarity scoring** (e.g., cosine similarity).\n",
    "- Allows for flexible wording but still ensures factual correctness.\n",
    "\n",
    "#### **Weighted Combination Mode**  \n",
    "> A hybrid approach balancing factual alignment and semantic similarity.  \n",
    "\n",
    "\n",
    "Score = $ w_1 \\times \\text{Factual Correctness} + w_2 \\times \\text{Semantic Similarity} $\n",
    "\n",
    "(Default: **\\( w_1 = 0.75, w_2 = 0.25 \\)**)\n",
    "\n",
    "---\n",
    "\n",
    "### **Controlling Granularity of Statements**  \n",
    "\n",
    "The metric supports **customizing the granularity** of extracted statements using:  \n",
    "\n",
    "#### **Atomicity:**  \n",
    "- **High Atomicity** → Extracts fine-grained claims from a sentence.  \n",
    "- **Low Atomicity** → Retains larger sentence-level claims.  \n",
    "\n",
    "#### **Coverage:**  \n",
    "- **High Coverage** → Extracts all possible claims from a response.  \n",
    "- **Low Coverage** → Focuses only on the key facts.  \n",
    "\n",
    "This flexibility ensures adaptability for different evaluation needs.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example of Atomicity and Coverage Adjustments**  \n",
    "#### **High Atomicity & High Coverage Example:**\n",
    "```python\n",
    "scorer = AnswerCorrectness(atomicity=\"high\", coverage=\"high\")\n",
    "```\n",
    "**Original Sentence:**  \n",
    "> \"The Eiffel Tower was designed by Gustave Eiffel and completed in 1889.\"  \n",
    "\n",
    "**Decomposed Claims:**  \n",
    "- \"The Eiffel Tower was designed by Gustave Eiffel.\"  \n",
    "- \"The Eiffel Tower was completed in 1889.\"  \n",
    "\n",
    "---\n",
    "\n",
    "### **Practical Applications**  \n",
    "- **High Atomicity & High Coverage** → Best for **fact-checking and evaluation** in RAG applications.  \n",
    "- **Low Atomicity & Low Coverage** → Suitable for **summary-level correctness**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways:**  \n",
    "* Measures factual correctness and completeness in generated answers.  \n",
    "* Uses **True Positives (TP), False Positives (FP), and False Negatives (FN)** for scoring.  \n",
    "* Provides an **F1-like score** for correctness evaluation.  \n",
    "* Supports **semantic similarity** as an additional scoring mode.  \n",
    "* Can be adjusted for **granularity (atomicity) and coverage** to control claim decomposition.  \n",
    "\n",
    "This metric is essential for **evaluating Retrieval-Augmented Generation (RAG) systems**, ensuring factually grounded responses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import AnswerCorrectness\n",
    "from prompts.custom_correctness_classifier_prompt import MyCorrectnessClassifier\n",
    "from prompts.faithfulness.custom_statement_generator_prompt import MyStatementGeneratorPrompt\n",
    "\n",
    "# Note you can modify the weights to put more emphasis on factual correctness or semantic similarity\n",
    "answer_correctness = AnswerCorrectness(\n",
    "    correctness_prompt=MyCorrectnessClassifier(),\n",
    "    statement_generator_prompt=MyStatementGeneratorPrompt(),\n",
    "    max_retries=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, after all relevant metrics are initialized and customized as needed we can evaluate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ragas.evaluation import evaluate, EvaluationResult\n",
    "\n",
    "results: EvaluationResult = evaluate(\n",
    "    dataset=evaluation_dataset,\n",
    "    metrics=[\n",
    "        llm_context_precision_with_re,      # Metric 1\n",
    "        llm_context_precision_without_re,   # Metric 1\n",
    "        context_precision_with_reference,   # Metric 1\n",
    "        llm_context_recall,                 # Metric 2\n",
    "        context_recall,                     # Metric 2\n",
    "        context_entity_recall,              # Metric 3\n",
    "        noise_sensitivity_mode_relevant,    # Metric 4\n",
    "        noise_sensitivity_mode_irrelevant,  # Metric 4\n",
    "        response_relevancy,                 # Metric 5\n",
    "        faithfulness,                       # Metric 6\n",
    "        factual_correctness,                # Metric 7\n",
    "        answer_correctness                  # Metric 8\n",
    "    ],\n",
    "    llm=langchain_llm,\n",
    "    embeddings=langchain_embeddings,\n",
    "    experiment_name=\"Evaluation of metrics with custom prompts\",\n",
    "    run_config=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Save results locally (optional)\n",
    "result_df: pd.DataFrame = results.to_pandas()\n",
    "result_df.to_csv(path='metrics_evaluation.csv', index=False)\n",
    "\n",
    "# Display metric scores\n",
    "results.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have supplied a RAGAs app token also upload to ragas.io (optional)\n",
    "results.upload()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
