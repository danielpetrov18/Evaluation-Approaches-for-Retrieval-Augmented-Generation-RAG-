{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance of High-Quality Test Dataset for RAG Systems\n",
    "\n",
    "### Purpose\n",
    "Test sets are critical for:\n",
    "- Accurately measuring RAG system performance\n",
    "- Identifying system strengths and weaknesses\n",
    "- Guiding continuous improvement\n",
    "\n",
    "### Key Evaluation Dimensions\n",
    "1. **Retrieval Effectiveness**: Assess how well relevant context is retrieved\n",
    "2. **Generation Quality**: Evaluate the accuracy and coherence of generated responses\n",
    "3. **Contextual Relevance**: Measure how well the system understands and integrates retrieved information\n",
    "\n",
    "### Evaluation Goals\n",
    "- Benchmark system performance\n",
    "- Detect hallucinations\n",
    "- Validate generalization capabilities\n",
    "- Simulate real-world query complexity\n",
    "\n",
    "### Best Practices\n",
    "- Use diverse query types\n",
    "- Cover multiple domains\n",
    "- Include edge cases\n",
    "- Create reproducible test scenarios\n",
    "- Contains enough number of samples to derive statistically significant conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Retrieve data:\n",
    "* The data can be a dataset from `huggingface` or any other platform.\n",
    "\n",
    "* Alternatively, files available on disk - pdf, md, etc.\n",
    "\n",
    "* One can also use `AsyncHtmlLoader` from `langchain` to scrape from the internet.\n",
    "    - **Careful when performing web scraping to not violate any terms and conditions!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'data'...\n",
      "remote: Enumerating objects: 14, done.\u001b[K\n",
      "remote: Total 14 (delta 0), reused 0 (delta 0), pack-reused 14 (from 1)\u001b[K\n",
      "Unpacking objects: 100% (14/14), 16.16 KiB | 2.31 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://huggingface.co/datasets/explodinggradients/ragas-airline-dataset data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**:\n",
    "* For the generation of testdata I use **DeepEval**, however this notebook can also be used.\n",
    "* Make sure you install the requirements first if you want to test the notebook.\n",
    "    * To do so run the `setup.sh` in the parent folder.\n",
    "* Make sure you select the proper environment as your kernel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load data into document objects\n",
    "\n",
    "I prefer to use `langchain`, however `llama-index` is also a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "path = \"data/\"\n",
    "loader = DirectoryLoader(\n",
    "    path,\n",
    "    glob=\"**/*.md\",\n",
    "    exclude=\"README.md\"\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Construct knowledge graph\n",
    "\n",
    "- A **knowledge graph** is a fundamental concept when it comes to **RAGAs** and using its capabilities for **automatic data generation**.\n",
    "\n",
    "- A **knowledge graph** consists of **Node** objects at first, which represent **documents** - their content and additional metadata.\n",
    "\n",
    "- Thereafter, one can enrich the graph by **applying various transformations** to it and **relationships get built**, which express some kind of connection between Node objects. The transformations can be applied only through the use of **Extractors** and or **RelationshipBuilder** objects. They serve as a way to gather relevant data from the documents depending on the type of extractor and this way to logically connect 2 or more nodes together.\n",
    "\n",
    "- This graph then is used to generate so called **scenarios** and can also be used to generate **personas** to arrive at the test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset.graph import (\n",
    "    Node,\n",
    "    NodeType,\n",
    "    KnowledgeGraph,\n",
    ")\n",
    "\n",
    "kg = KnowledgeGraph()\n",
    "\n",
    "for doc in docs:\n",
    "    kg.add(\n",
    "        Node(\n",
    "            type=NodeType.DOCUMENT,\n",
    "            properties={\"page_content\": doc.page_content, \"document_metadata\": doc.metadata}\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Instantiate required objects\n",
    "\n",
    "- **RAGAs** would require a **Large-Language-Model** and an **Embedding** one to be able to apply the **transformations** to the **knowledge graph**. For that purpose one must create **wrapper** objects for both of the models. `Langchain` and `llama-index` are both supported. \n",
    "\n",
    "- Additionally, a **configuration** can be used to modify the default behaviour of the framework. For example timeout values can be modified, maximum retries for failed operations and so on can be configured from the **RunConfig**.\n",
    "    - **NOTE**: since llama3.1:8b is not particularly efficient you may need to modify the `timeout` value.\n",
    "\n",
    "- Lastly, there's a single implementation in **RAGAs** for caching intermediate steps onto disk. To use it the **DiskCacheBackend** class can come in play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "from ragas import (\n",
    "    RunConfig,\n",
    "    DiskCacheBackend\n",
    ")\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "run_config = RunConfig(\n",
    "    timeout=14400, # This may need to be much higher depending on the GPU\n",
    "    max_retries=15,\n",
    "    max_wait=30\n",
    ")\n",
    "\n",
    "cacher = DiskCacheBackend(cache_dir=\".cache\")\n",
    "\n",
    "load_dotenv(\"../../env/rag.env\")\n",
    "chat_model = os.getenv(\"CHAT_MODEL\")\n",
    "embedding_model = os.getenv(\"EMBEDDING_MODEL\")\n",
    "temperature = float(os.getenv(\"TEMPERATURE\"))\n",
    "num_ctx = int(os.getenv(\"LLM_CONTEXT_WINDOW_TOKENS\"))\n",
    "\n",
    "ollama_llm = OllamaLLM(\n",
    "    model=chat_model,\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=temperature,\n",
    "    num_ctx=num_ctx,\n",
    "    format=\"json\"\n",
    ")\n",
    "\n",
    "ollama_embeddings = OllamaEmbeddings(\n",
    "    model=embedding_model,\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "langchain_llm = LangchainLLMWrapper(\n",
    "    langchain_llm=ollama_llm,\n",
    "    run_config=run_config,\n",
    "    cache=cacher\n",
    ")\n",
    "\n",
    "langchain_embeddings = LangchainEmbeddingsWrapper(\n",
    "    embeddings=ollama_embeddings,\n",
    "    run_config=run_config,\n",
    "    cache=cacher\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create the transformation pipeline\n",
    "\n",
    "The sequence of transformations:\n",
    "\n",
    "1. HeadlinesExtractor and HeadlinesSplitter\n",
    "    - This step is going to ensure that longer documents are split into logical sections\n",
    "\n",
    "2. Named Entity Recognition (NER) & Theme Extraction  \n",
    "    - NERExtractor identifies and extracts named entities (e.g., people, organizations, locations).  \n",
    "    - ThemesExtractor detects overarching topics/themes in each chunk.\n",
    "\n",
    "3. Summary Extraction and Summary Embedding Extraction\n",
    "    - Relevant for the MultiHopAbstractQuerySynthesizer\n",
    "\n",
    "4. Extraction of key phrases\n",
    "    - Gain additional insights into documents\n",
    "\n",
    "5. NEROverlapBuilder and CosineSimilarityBuilder\n",
    "    - Used to group nodes containing similar entities\n",
    "    - Group semantically close nodes by their summary\n",
    "\n",
    "6. Parallel Processing for Efficiency\n",
    "    - Certain transformations run in parallel to improve performance\n",
    "\n",
    "-> Final Outcome:\n",
    "    - A structured set of document transformations that extract valuable information\n",
    "    - Used to enrich the knowledge graph for further generation of scenarios and finally samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.transforms.engine import Parallel\n",
    "from ragas.testset.transforms.extractors.llm_based import (\n",
    "    HeadlinesExtractor,\n",
    "    SummaryExtractor,\n",
    "    NERExtractor,\n",
    "    ThemesExtractor,\n",
    "    KeyphrasesExtractor,\n",
    ")\n",
    "from ragas.testset.transforms.relationship_builders import (\n",
    "    OverlapScoreBuilder,\n",
    "    CosineSimilarityBuilder\n",
    ")\n",
    "from ragas.testset.transforms.splitters import HeadlineSplitter\n",
    "from ragas.testset.transforms.extractors.embeddings import EmbeddingExtractor\n",
    "\n",
    "headline_extractor = HeadlinesExtractor(\n",
    "    llm=langchain_llm,\n",
    "    max_num=10\n",
    ")\n",
    "\n",
    "headline_splitter = HeadlineSplitter(\n",
    "    max_tokens=1500\n",
    ")\n",
    "\n",
    "summary_extractor = SummaryExtractor(\n",
    "    llm=langchain_llm\n",
    ")\n",
    "\n",
    "ner_extractor = NERExtractor(\n",
    "    llm=langchain_llm,\n",
    "    max_num_entities=20\n",
    ")\n",
    "\n",
    "themes_extractor = ThemesExtractor(\n",
    "    llm=langchain_llm,\n",
    "    max_num_themes=20\n",
    ")\n",
    "\n",
    "summary_emb_extractor = EmbeddingExtractor(\n",
    "    property_name=\"summary_embedding\",\n",
    "    embed_property_name=\"summary\",\n",
    "    embedding_model=langchain_embeddings,\n",
    ")\n",
    "\n",
    "keyphrase_extractor = KeyphrasesExtractor(\n",
    "    llm=langchain_llm,\n",
    "    max_num=15\n",
    ")\n",
    "\n",
    "ner_overlap_sim = OverlapScoreBuilder(\n",
    "    threshold=0.01\n",
    ")\n",
    "\n",
    "cosine_sim_builder = CosineSimilarityBuilder(\n",
    "    property_name=\"summary_embedding\",\n",
    "    new_property_name=\"summary_similarity\",\n",
    "    threshold=0.7\n",
    ")\n",
    "\n",
    "transforms = [\n",
    "    headline_extractor,\n",
    "    headline_splitter,\n",
    "    summary_extractor,\n",
    "    Parallel(\n",
    "        summary_emb_extractor,\n",
    "        themes_extractor,\n",
    "        ner_extractor,\n",
    "        keyphrase_extractor,\n",
    "    ),\n",
    "    Parallel(\n",
    "        cosine_sim_builder, \n",
    "        ner_overlap_sim\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Apply the transformations to the knowledge graph\n",
    "\n",
    "In the section below the `apply_transforms` is going to apply all the previously defined transformations enriching the `knowledge graph` in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying SummaryExtractor:   0%|          | 0/23 [00:00<?, ?it/s] Property 'summary' already exists in node '0d39a9'. Skipping!\n",
      "Property 'summary' already exists in node 'b23be3'. Skipping!\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor, KeyphrasesExtractor]:   0%|          | 0/92 [00:00<?, ?it/s]Property 'summary_embedding' already exists in node '0d39a9'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'b23be3'. Skipping!\n",
      "Property 'themes' already exists in node '0d39a9'. Skipping!\n",
      "Property 'themes' already exists in node 'b23be3'. Skipping!\n",
      "Property 'entities' already exists in node '0d39a9'. Skipping!\n",
      "Property 'entities' already exists in node 'b23be3'. Skipping!\n",
      "Property 'keyphrases' already exists in node '0d39a9'. Skipping!\n",
      "Property 'keyphrases' already exists in node 'b23be3'. Skipping!\n",
      "                                                                                                                                 \r"
     ]
    }
   ],
   "source": [
    "from ragas.testset.transforms import apply_transforms\n",
    "\n",
    "apply_transforms(\n",
    "    kg,\n",
    "    transforms,\n",
    "    run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Generating personas\n",
    "\n",
    "- A **Persona** is an entity/role which interacts with the system. **Personas** provide context and perspective, ensuring that **generated queries are natural, user-specific, and diverse**.\n",
    "\n",
    "- `Example: a Senior DevOps engineer, a Junior Data Scientist, a Marketing Manager in the context of an IT company `\n",
    "\n",
    "- **Persona** object consists of a **name** and a **description**.\n",
    "    \n",
    "    - The name is used to identify the persona and the description is used to describe the role of the persona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.persona import Persona\n",
    "\n",
    "persona_first_time_flier = Persona(\n",
    "    name=\"First Time Flier\",\n",
    "    role_description=\"Is flying for the first time and may feel anxious. Needs clear guidance on flight procedures, safety protocols, and what to expect throughout the journey.\",\n",
    ")\n",
    "\n",
    "persona_frequent_flier = Persona(\n",
    "    name=\"Frequent Flier\",\n",
    "    role_description=\"Travels regularly and values efficiency and comfort. Interested in loyalty programs, express services, and a seamless travel experience.\",\n",
    ")\n",
    "\n",
    "persona_angry_business_flier = Persona(\n",
    "    name=\"Angry Business Flier\",\n",
    "    role_description=\"Demands top-tier service and is easily irritated by any delays or issues. Expects immediate resolutions and is quick to express frustration if standards are not met.\",\n",
    ")\n",
    "\n",
    "persona_traveler_with_medical_needs = Persona(\n",
    "    name=\"Traveler with Medical Needs\",\n",
    "    role_description=\"Has specific medical requirements and needs information about carrying medications, requesting medical clearance, and accessing in-flight medical assistance. Concerned about potential health issues during travel.\",\n",
    ")\n",
    "\n",
    "persona_family_traveling_with_children = Persona(\n",
    "    name=\"Family with Children\",\n",
    "    role_description=\"Traveling with young children, including occasionally unaccompanied minors. Requires information about special services for families, meal options for kids, entertainment options, and how to manage multiple reservations efficiently.\",\n",
    ")\n",
    "\n",
    "personas = [\n",
    "    persona_first_time_flier,\n",
    "    persona_frequent_flier,\n",
    "    persona_angry_business_flier,\n",
    "    persona_traveler_with_medical_needs,\n",
    "    persona_family_traveling_with_children\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate query types \n",
    "\n",
    "- There are two main types of queries in **RAGAs**:\n",
    "    \n",
    "    - **SingleHopQuery** where the **context** relevant for answering a question lies in a **single document/chunk**\n",
    "\n",
    "    - **MultiHopQuery** where the **context** relevant for answering a question lies in **multiple documents/chunks**\n",
    "\n",
    "- Additionally, for each of those queries there's a **Specific** or **Abstract** query variant:\n",
    "    \n",
    "    - **Specific** one which pertains to a **fact**. \n",
    "\n",
    "        - `Example: When did WW1 break out? (Can be precisely answered, there's no room for guessing/interpretation)`\n",
    "    \n",
    "    - **Abstract** one which is more about testing the **reasoning** capabilities of the LLM. \n",
    "\n",
    "        - `Example: Why did WW1 break out? (There's room for interpretation in this case)`\n",
    "\n",
    "**Synthesizers** are responsible for **converting enriched nodes and personas into queries**. They achieve this by **selecting a node property (e.g., \"entities\" or \"keyphrases\"), pairing it with a persona, style, and query length**, and then using a LLM to generate a query-answer pair based on the content of the node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import typing as t\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from langchain_core.callbacks import Callbacks\n",
    "\n",
    "from ragas.testset.graph import (\n",
    "    Node,\n",
    "    KnowledgeGraph,\n",
    ")\n",
    "from ragas.prompt import PydanticPrompt\n",
    "from ragas.testset.synthesizers.prompts import (\n",
    "    ThemesPersonasInput,\n",
    "    ThemesPersonasMatchingPrompt,\n",
    ")\n",
    "from ragas.testset.synthesizers.single_hop import (\n",
    "    SingleHopScenario,\n",
    "    SingleHopQuerySynthesizer,\n",
    ")\n",
    "\n",
    "# Doesn't filter based on document type but solely on the property\n",
    "@dataclass\n",
    "class MySingleHopSpecificQuerySynthesizer(SingleHopQuerySynthesizer):\n",
    "    name: str = \"single_hop_specifc_query_synthesizer\"\n",
    "    theme_persona_matching_prompt: PydanticPrompt = ThemesPersonasMatchingPrompt()\n",
    "    property_name: str = \"entities\"\n",
    "\n",
    "    def get_node_clusters(self, knowledge_graph: KnowledgeGraph) -> t.List[Node]:\n",
    "        \"\"\"\n",
    "        Get all nodes that contain the specified property (`entities`), regardless of type.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            node\n",
    "            for node in knowledge_graph.nodes\n",
    "            if node.get_property(self.property_name) is not None\n",
    "        ]\n",
    "\n",
    "    async def _generate_scenarios(\n",
    "        self,\n",
    "        n: int,\n",
    "        knowledge_graph: KnowledgeGraph,\n",
    "        persona_list: t.List[Persona],\n",
    "        callbacks: Callbacks,\n",
    "    ) -> t.List[SingleHopScenario]:\n",
    "        \"\"\"\n",
    "        Generates a list of scenarios based on all nodes that have the `entities` property.\n",
    "        \"\"\"\n",
    "\n",
    "        nodes = self.get_node_clusters(knowledge_graph)\n",
    "        if len(nodes) == 0:\n",
    "            raise ValueError(\"No nodes found with the `entities` property.\")\n",
    "        samples_per_node = int(np.ceil(n / len(nodes)))\n",
    "\n",
    "        scenarios = []\n",
    "        for node in nodes:\n",
    "            if len(scenarios) >= n:\n",
    "                break\n",
    "            themes = node.properties.get(self.property_name, [\"\"])\n",
    "            prompt_input = ThemesPersonasInput(themes=themes, personas=persona_list)\n",
    "            persona_concepts = await self.theme_persona_matching_prompt.generate(\n",
    "                data=prompt_input, llm=self.llm, callbacks=callbacks\n",
    "            )\n",
    "            base_scenarios = self.prepare_combinations(\n",
    "                node,\n",
    "                themes,\n",
    "                personas=persona_list,\n",
    "                persona_concepts=persona_concepts.mapping,\n",
    "            )\n",
    "            scenarios.extend(self.sample_combinations(base_scenarios, samples_per_node))\n",
    "\n",
    "        return scenarios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.synthesizers.multi_hop.specific import MultiHopSpecificQuerySynthesizer\n",
    "from ragas.testset.synthesizers.multi_hop.abstract import MultiHopAbstractQuerySynthesizer\n",
    "\n",
    "single_hop_specific_entities = MySingleHopSpecificQuerySynthesizer(\n",
    "    llm=langchain_llm,\n",
    "    property_name=\"entities\"\n",
    ")\n",
    "\n",
    "single_hop_specific_keyphrases = MySingleHopSpecificQuerySynthesizer(\n",
    "    llm=langchain_llm,\n",
    "    property_name=\"keyphrases\"\n",
    ")\n",
    "\n",
    "multi_hop_specific_entities = MultiHopSpecificQuerySynthesizer(\n",
    "    llm=langchain_llm\n",
    ")\n",
    "\n",
    "multi_hop_abstract_entities = MultiHopAbstractQuerySynthesizer(\n",
    "    llm=langchain_llm\n",
    ")\n",
    "\n",
    "query_distribution = [\n",
    "    (single_hop_specific_entities, 0.25),\n",
    "    (single_hop_specific_keyphrases, 0.25),\n",
    "    (multi_hop_specific_entities, 0.25),\n",
    "    (multi_hop_abstract_entities, 0.25)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Generate the samples\n",
    "\n",
    "### Definition of Evaluation Sample\n",
    "\n",
    "An evaluation sample is a single structured data instance that is used to assess and measure the performance of your LLM application in specific scenarios. It represents a single unit of interaction or a specific use case that the AI application is expected to handle. In Ragas, evaluation samples are represented using the `SingleTurnSample` and `MultiTurnSample` classes.\n",
    "\n",
    "### SingleTurnSample\n",
    "\n",
    "`SingleTurnSample` represents a single-turn interaction between a user, LLM, and expected results for evaluation. It is suitable for evaluations that involve a single question and answer pair, possibly with additional context or reference information.\n",
    "\n",
    "This type of sample is ideal for straightforward question-answering scenarios where a user asks a single question and expects a direct response.\n",
    "\n",
    "### MultiTurnSample\n",
    "\n",
    "`MultiTurnSample` represents a multi-turn interaction between Human, AI and optionally a Tool and expected results for evaluation. It is suitable for representing conversational agents in more complex interactions for evaluation.\n",
    "\n",
    "In `MultiTurnSample`, the `user_input` attribute represents a sequence of messages that collectively form a multi-turn conversation between a human user and an AI system. These messages are instances of the classes `HumanMessage`, `AIMessage`, and `ToolMessage`.\n",
    "\n",
    "This type of sample is designed for evaluating more complex conversational flows where multiple turns of dialogue occur, potentially involving tool usage for gathering additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Scenarios: 100%|██████████| 4/4 [1:02:56<00:00, 944.13s/it] \n",
      "Generating Samples: 100%|██████████| 52/52 [18:05<00:00, 20.88s/it]\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "generator = TestsetGenerator(\n",
    "    langchain_llm,\n",
    "    langchain_embeddings,\n",
    "    kg,\n",
    "    personas\n",
    ")\n",
    "\n",
    "dataset = generator.generate(\n",
    "    testset_size=50,\n",
    "    query_distribution=query_distribution,\n",
    "    num_personas=5,\n",
    "    run_config=run_config,\n",
    "    with_debugging_logs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Ingest data into R2R if not already\n",
    "\n",
    "* **Note:** you can also use the frontend at `http://localhost:8501` after starting the application.\n",
    "* Alternatively, a script will also do the job:\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "from r2r import R2RClient, R2RException\n",
    "\n",
    "client = R2RClient(\n",
    "    base_url=\"http://localhost:7272\", # Maybe different for you\n",
    "    timeout=600\n",
    ")\n",
    "\n",
    "dir_path = Path(\"data\")\n",
    "for item in dir_path.iterdir():\n",
    "    if item.is_file() and item.suffix == '.md' and item.name != \"README.md\":\n",
    "        try:\n",
    "            client.documents.create(\n",
    "                file_path=str(item),\n",
    "                ingestion_mode=\"custom\",\n",
    "                run_with_orchestration=True   \n",
    "            )\n",
    "            print(f\"Ingested file: {item.name}\")\n",
    "        except R2RException as r2re:\n",
    "            print(f\"Couldn't ingest file: {item.name} due to {str(r2re)}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Add missing information to dataset using R2R\n",
    "\n",
    "* Add the `actual response`\n",
    "* Add the `retrieved context`\n",
    "\n",
    "**EXAMPLE:**\n",
    "\n",
    "```python\n",
    "from r2r import R2RClient, R2RException\n",
    "\n",
    "client = R2RClient(\n",
    "    base_url=\"http://localhost:7272\",\n",
    "    timeout=600\n",
    ")\n",
    "\n",
    "# These need to be ideally the same configurations as the ones used in the backend for R2R\n",
    "# This way we could try to reproduce the same results as in the backend or as close as possible\n",
    "search_settings = {\n",
    "    \"use_semantic_search\": True,\n",
    "    \"limit\": 5,\n",
    "    \"offset\": 0,\n",
    "    \"include_metadatas\": False,\n",
    "    \"search_strategy\": \"vanilla\",\n",
    "}\n",
    "    \n",
    "rag_generation_config = {\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 1,\n",
    "    \"max_tokens_to_sample\": 512\n",
    "}\n",
    "\n",
    "template = \"\"\" \n",
    "## Task:\n",
    "\n",
    "Answer the given query ONLY using the provided context. Keep your answer short and concise.\n",
    "\n",
    "### Guidelines:\n",
    "- Strictly limit responses to 2-3 sentences whenever possible.\n",
    "- If a longer answer is necessary, make it as brief as possible, focusing only on relevant details.\n",
    "- Merge lists/enumerations into a single coherent sentence using conjunctions or commas.\n",
    "- Do NOT reference line numbers or list items from the context.\n",
    "- If the provided context lacks sufficient information, explicitly inform the user that the answer cannot be determined.\n",
    "- NEVER generate an answer beyond the given context — do not speculate or infer missing details.\n",
    "- Do NOT use external knowledge; rely only on the retrieved context.\n",
    "\n",
    "---\n",
    "\n",
    "### Query:\n",
    "{query}\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "### Reminder:  \n",
    "- Keep the response short and factual.  \n",
    "- If the context lacks the answer, say so explicitly.  \n",
    "- Do NOT generate an answer beyond the provided information.  \n",
    "\n",
    "## Response:\n",
    "\"\"\"\n",
    "    \n",
    "final_dataset = dataset # Make sure to use a different variable if something goes wrong\n",
    "for i, sample in enumerate(final_dataset.samples):\n",
    "    try:\n",
    "        # Submit a query using the randomly generated question by RAGAs\n",
    "        response = client.retrieval.rag(\n",
    "            query=sample.eval_sample.user_input,\n",
    "            search_mode=\"custom\",\n",
    "            search_settings=search_settings,\n",
    "            rag_generation_config=rag_generation_config,\n",
    "            task_prompt=template\n",
    "        ).results\n",
    "\n",
    "        llm_response = response.completion\n",
    "        retrieved_context_txt = [chunk.text for chunk in response.search_results.chunk_search_results]\n",
    "        \n",
    "        final_dataset.samples[i].eval_sample.response = llm_response\n",
    "        final_dataset.samples[i].eval_sample.retrieved_contexts = retrieved_context_txt\n",
    "        \n",
    "        print(f\"Added data to sample: {i + 1} out of {len(final_dataset.samples)}\")\n",
    "        \n",
    "    except R2RException as r2re:\n",
    "        print(f\"Something went wrong when submitting query: {sample.eval_sample.user_input} due to {str(r2re)}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Save the dataset\n",
    "\n",
    "Save locally if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_jsonl(\"dataset.jsonl\")\n",
    "dataset.to_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Upload to the cloud (Optional)\n",
    "\n",
    "* To upload the data on **app.ragas.io** make sure you:\n",
    "    * First create an account\n",
    "    * Get an **API key**\n",
    "    * Finally, create a `.env` file in the parent folder like so and export it in your notebook:\n",
    "\n",
    "```bash\n",
    "RAGAS_APP_TOKEN=apt.1234a-......-9dfew\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testset uploaded! View at https://app.ragas.io/dashboard/alignment/testset/b0eb0e65-8914-4910-a103-8dab699927b3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://app.ragas.io/dashboard/alignment/testset/b0eb0e65-8914-4910-a103-8dab699927b3'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() # This will load the token\n",
    "\n",
    "dataset.upload()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
