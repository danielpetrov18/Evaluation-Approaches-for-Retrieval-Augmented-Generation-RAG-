{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e77b42c8",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "* The **Opik** platform can either be hosted locally in a container or a cluster: [local hosting](https://www.comet.com/docs/opik/self-host/overview)\n",
    "* Alternatively, there's a cloud based platform free of charge: [cloud version](https://www.comet.com/signup).\n",
    "    * When using the cloud version: an API key and a workspace need to be specified.\n",
    "    * Similarly to **DeepEval**, the cloud version of **Opik** has an intuitive UI, where datasets, evaluations results and more can be stored.\n",
    "    * For the cloud based approach make sure to add the following to your `.env` file in the parent folder:\n",
    "    ```bash\n",
    "        OPIK_API_KEY=<your-api-key>\n",
    "        OPIK_WORKSPACE=<your-workspace>\n",
    "        OPIK_PROJECT_NAME=<project-name> # Setting this will automatically log traces for the project (Optional)\n",
    "        OPIK_USAGE_REPORT_ENABLED=false  # Disable telemetry (Optional)\n",
    "        # By default creates a file called ~/.opik.config (on Linux) if it doesn't exist\n",
    "        OPIK_CONFIG_PATH=<filepath-to-your-config-file>  # Overwrite the file location (Optional)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcb76d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Opik is already configured. You can check the settings by viewing the config file at /home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/opik/.opik.config\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import opik\n",
    "from dotenv import load_dotenv\n",
    "from opik.exceptions import ConfigurationError\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "try:\n",
    "    # If you're using the locally hosted version set the `use_local=True and provide the url`\n",
    "    opik.configure(\n",
    "        api_key=os.getenv(\"OPIK_API_KEY\"),\n",
    "        workspace=os.getenv(\"OPIK_WORKSPACE\")\n",
    "    )\n",
    "except (ConfigurationError, ConnectionError) as ce:\n",
    "    print(f\"Error occurred: {ce}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fafd36d",
   "metadata": {},
   "source": [
    "### LLM and tracing\n",
    "\n",
    "* **Opik** uses **OpenAI** as the LLM-provider by default. To overwrite that create a `LiteLLMChatModel` instance with the model you want to use and specify your [input parameters](https://docs.litellm.ai/docs/completion/input).\n",
    "* If you want to add tracing capabilities so that all calls to `litellm` are traced to the **Opik** platform create the `OpikLogger` and set the `litellm` callbacks (Optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f00f3621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import litellm\n",
    "from dotenv import load_dotenv\n",
    "from opik.evaluation.models import LiteLLMChatModel\n",
    "from litellm.integrations.opik.opik import OpikLogger\n",
    "\n",
    "load_dotenv(\"../../env/rag.env\")\n",
    "\n",
    "# https://docs.litellm.ai/docs/completion/input\n",
    "eval_model = LiteLLMChatModel(\n",
    "    model_name=f\"ollama/{os.getenv(\"CHAT_MODEL\")}\",\n",
    "    temperature=float(os.getenv(\"TEMPERATURE\")),\n",
    "    top_p=float(os.getenv(\"TOP_P\")),\n",
    "    response_format={\n",
    "        \"type\": \"json_object\"\n",
    "    },\n",
    "    api_base=\"http://localhost:11434\",\n",
    "    num_retries=3,\n",
    ")\n",
    "\n",
    "# This will trace all calls submitted to the LLM to Opik (Optional)\n",
    "opik_logger = OpikLogger()\n",
    "litellm.callbacks = [opik_logger]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9354cff4",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "For an evaluation/experiment in **Opik** the following things are required:\n",
    "- a dataset\n",
    "    - **Opik** supports datasets, which are a collection of key value pairs.\n",
    "    - Datasets can be created and deleted.\n",
    "- an evaluation task\n",
    "    - maps a dataset item/sample to a dictionary object, which is submitted as a parameter to the `score` method if a metric.\n",
    "- a set of metrics\n",
    "    - the ones of relevance for the project are the LLM-as-ajudge one\n",
    "    - additionally, one can overwrite the `BaseMetric` class for a custom metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a12d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from typing import Dict, Any, List\n",
    "from opik import Dataset\n",
    "from opik.api_objects.dataset.rest_operations import ApiError\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# Create an `Opik`` client for interacting with the platform\n",
    "opik_client = opik.Opik(\n",
    "    project_name=os.getenv(\"OPIK_PROJECT_NAME\"),\n",
    "    workspace=os.getenv(\"OPIK_WORKSPACE\"),\n",
    "    api_key=os.getenv(\"OPIK_API_KEY\"),\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Fetch the dataset\n",
    "    opik_dataset: Dataset = opik_client.get_dataset(name=os.getenv(\"DATASET_ALIAS\"))\n",
    "except ApiError as ae:\n",
    "    # If not available fetch it from `DeepEval`\n",
    "    # Convert it into a list of Opik Dataset Items and upload to `Opik`\n",
    "    from deepeval.dataset import EvaluationDataset\n",
    "    from deepeval import login_with_confident_api_key\n",
    "    \n",
    "    print(f\"{ae.status_code}: {ae.body['errors']}\")\n",
    "    print(f\"Fetching from DeepEval and then uploading on the Opik Platform\")\n",
    "    \n",
    "    login_with_confident_api_key(os.getenv(\"DEEPEVAL_API_KEY\"))\n",
    "    deepeval_dataset = EvaluationDataset()\n",
    "    deepeval_dataset.pull(\n",
    "        alias=os.getenv(\"DATASET_ALIAS\"),\n",
    "        auto_convert_goldens_to_test_cases=True\n",
    "    )\n",
    "    \n",
    "    opik_dataset: Dataset = opik_client.create_dataset(\n",
    "        name=os.getenv(\"DATASET_ALIAS\"),\n",
    "        description=\"Evaluation dataset from DeepEval\"\n",
    "    )\n",
    "    \n",
    "    opik_dataset_items: List[Dict[str, Any]] = [vars(test_case) for test_case in deepeval_dataset.test_cases]\n",
    "    opik_dataset.insert(opik_dataset_items)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1857dccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opik.api_objects.dataset.dataset_item import DatasetItem\n",
    "\n",
    "# This function is used during evaluation\n",
    "# For each item in the dataset, this function will be called\n",
    "# The output of the function is a dictionary containing the relevant parameters for the metrics\n",
    "def evaluation_task(item: DatasetItem) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"input\": item['input'],\n",
    "        \"output\": item['actual_output'],\n",
    "        \"expected_output\": item['expected_output'],\n",
    "        \"context\": item['context']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e92d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opik.evaluation.metrics import Hallucination\n",
    "from opik.evaluation.metrics.llm_judges.hallucination.template import generate_query # TODO: Override the prompt template\n",
    "\n",
    "hallucination_metric = Hallucination(\n",
    "    model=eval_model,\n",
    "    project_name=os.getenv(\"OPIK_PROJECT_NAME\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8582c010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation:   0%|          | 0/48 [00:00<?, ?it/s]OPIK: Started logging traces to the \"Evaluation Approaches for RAG\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=019691d7-a282-716b-a3b2-a145dc28568b&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n",
      "Evaluation: 100%|██████████| 48/48 [31:18<00:00, 39.14s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ DeepEval Dataset (48 samples) ────╮\n",
       "│                                    │\n",
       "│ <span style=\"font-weight: bold\">Total time:       </span> 00:31:19        │\n",
       "│ <span style=\"font-weight: bold\">Number of samples:</span> 48              │\n",
       "│                                    │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">hallucination_metric: 0.3021 (avg)</span> │\n",
       "│                                    │\n",
       "╰────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─ DeepEval Dataset (48 samples) ────╮\n",
       "│                                    │\n",
       "│ \u001b[1mTotal time:       \u001b[0m 00:31:19        │\n",
       "│ \u001b[1mNumber of samples:\u001b[0m 48              │\n",
       "│                                    │\n",
       "│ \u001b[1;32mhallucination_metric: 0.3021 (avg)\u001b[0m │\n",
       "│                                    │\n",
       "╰────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Uploading results to Opik <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Uploading results to Opik \u001b[33m...\u001b[0m \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">View the results <a href=\"https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=019691d7-9e4f-7cb2-8ffe-f0f289678664&dataset_id=019691c4-bccb-753b-8eba-1f818e3d504c&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\" target=\"_blank\">in your Opik dashboard</a>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "View the results \u001b]8;id=728789;https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=019691d7-9e4f-7cb2-8ffe-f0f289678664&dataset_id=019691c4-bccb-753b-8eba-1f818e3d504c&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\u001b\\in your Opik dashboard\u001b]8;;\u001b\\.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from opik.evaluation import evaluate\n",
    "from opik.evaluation.evaluation_result import EvaluationResult\n",
    "\n",
    "eval_res: EvaluationResult = evaluate(\n",
    "    dataset=opik_dataset,\n",
    "    task=evaluation_task,\n",
    "    scoring_metrics=[hallucination_metric],\n",
    "    experiment_name=\"First evaluation ever using Opik\",\n",
    "    project_name=os.getenv(\"OPIK_PROJECT_NAME\"),\n",
    "    #experiment_config={}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
