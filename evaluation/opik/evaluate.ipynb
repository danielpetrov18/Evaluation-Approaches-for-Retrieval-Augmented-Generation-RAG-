{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e77b42c8",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "* The **Opik** platform can either be hosted locally in a container or a cluster: [local hosting](https://www.comet.com/docs/opik/self-host/overview)\n",
    "* Alternatively, there's a cloud based platform free of charge: [cloud version](https://www.comet.com/signup).\n",
    "    * When using the cloud version: an API key and a workspace need to be specified.\n",
    "    * Similarly to **DeepEval**, the cloud version of **Opik** has an intuitive UI, where datasets, evaluations results and more can be stored.\n",
    "    * For the cloud based approach make sure to add the following to your `.env` file in the parent folder:\n",
    "    ```bash\n",
    "        OPIK_API_KEY=<your-api-key>\n",
    "        OPIK_WORKSPACE=<your-workspace>\n",
    "        # Setting this will automatically log traces for the project (Optional)\n",
    "        OPIK_PROJECT_NAME=<project-name>\n",
    "        OPIK_USAGE_REPORT_ENABLED=false  # Disable telemetry (Optional)\n",
    "        # By default creates a file called ~/.opik.config (on Linux) if it doesn't exist\n",
    "        OPIK_CONFIG_PATH=<filepath-to-your-config-file>  # Overwrite the file location (Optional)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcb76d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Opik is already configured. You can check the settings by viewing the config file at /home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/opik/.opik.config\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import opik\n",
    "from typing import Final\n",
    "from dotenv import load_dotenv\n",
    "from opik.exceptions import ConfigurationError\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "OPIK_API_KEY: Final[str] = os.getenv(\"OPIK_API_KEY\")\n",
    "OPIK_WORKSPACE: Final[str] = os.getenv(\"OPIK_WORKSPACE\")\n",
    "OPIK_PROJECT_NAME: Final[str] = os.getenv(\"OPIK_PROJECT_NAME\")\n",
    "\n",
    "try:\n",
    "    # If you're using the locally hosted version set the `use_local=True and provide the url`\n",
    "    opik.configure(\n",
    "        api_key=OPIK_API_KEY,\n",
    "        workspace=OPIK_WORKSPACE\n",
    "    )\n",
    "except (ConfigurationError, ConnectionError) as ce:\n",
    "    print(f\"Error occurred: {ce}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fafd36d",
   "metadata": {},
   "source": [
    "### LLM and tracing\n",
    "\n",
    "* **Opik** uses **OpenAI** as the LLM-provider by default. To overwrite that create a `LiteLLMChatModel` instance with the model you want to use and specify your [input parameters](https://docs.litellm.ai/docs/completion/input).\n",
    "* If you want to add tracing capabilities so that all calls to `litellm` are traced to the **Opik** platform create the `OpikLogger` and set the `litellm` callbacks (Optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f00f3621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import litellm\n",
    "from dotenv import load_dotenv\n",
    "from opik.evaluation.models import LiteLLMChatModel\n",
    "from litellm.integrations.opik.opik import OpikLogger\n",
    "\n",
    "load_dotenv(\"../../env/rag.env\")\n",
    "\n",
    "# https://docs.litellm.ai/docs/completion/input\n",
    "eval_model = LiteLLMChatModel(\n",
    "    model_name=f\"ollama/{os.getenv(\"CHAT_MODEL\")}\",\n",
    "    temperature=float(os.getenv(\"TEMPERATURE\")),\n",
    "    top_p=float(os.getenv(\"TOP_P\")),\n",
    "    response_format={\n",
    "        \"type\": \"json_object\"\n",
    "    },\n",
    "    api_base=\"http://localhost:11434\",\n",
    "    num_retries=3,\n",
    ")\n",
    "\n",
    "# This will trace all calls submitted to the LLM to Opik (Optional)\n",
    "opik_logger = OpikLogger()\n",
    "litellm.callbacks = [opik_logger]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9354cff4",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "For an evaluation/experiment in **Opik** the following things are required:\n",
    "- an experiment is a single evaluation of the LLM application\n",
    "    - during an experiment all items of the dataset get iterated on\n",
    "    - an experiment consists of two main components:\n",
    "        - configuration:\n",
    "            - one can store key-value-pairs unique to each experiment to track and compare different evaluation runs and verify, which set of hyperparameters yields the best performance\n",
    "        - experiment items:\n",
    "            - individual items from a dataset\n",
    "            - they get evaluated using the specified metrics and receive a trace, score and additional metadata\n",
    "- a dataset\n",
    "    - **Opik** supports datasets, which are a collection samples, that the LLM application will be evaluated on.\n",
    "    - Datasets can be created and deleted.\n",
    "- an evaluation task\n",
    "    - receives a dataset item as input and returns a dictionary, that contains all required parameters by a metric\n",
    "- a set of metrics\n",
    "    - the ones of relevance for the project are the LLM-as-ajudge one\n",
    "    - additionally, one can overwrite the `BaseMetric` class for a custom metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a12d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from typing import Dict, Any, List\n",
    "from opik import Dataset\n",
    "from opik.api_objects.dataset.rest_operations import ApiError\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "DATASET_ALIAS: Final[str] = os.getenv(\"DATASET_ALIAS\")\n",
    "\n",
    "# Create an `Opik` client for interacting with the platform\n",
    "opik_client = opik.Opik(\n",
    "    project_name=OPIK_PROJECT_NAME,\n",
    "    workspace=OPIK_WORKSPACE,\n",
    "    api_key=OPIK_API_KEY,\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Fetch the dataset\n",
    "    opik_dataset: Dataset = opik_client.get_dataset(name=DATASET_ALIAS)\n",
    "except ApiError as ae:\n",
    "    # If not available fetch it from `DeepEval`\n",
    "    # Convert it into a list of Opik Dataset Items and upload to `Opik`\n",
    "    # Alternatively: \n",
    "    #   opik_dataset.read_jsonl_from_file(\"path/to/file.jsonl\")\n",
    "    #   opik_dataset.insert_from_pandas(dataframe=df)\n",
    "\n",
    "    from deepeval.dataset import EvaluationDataset\n",
    "    from deepeval import login_with_confident_api_key\n",
    "    \n",
    "    print(f\"{ae.status_code}: {ae.body['errors']}\")\n",
    "    print(f\"Fetching from DeepEval and then uploading on the Opik Platform\")\n",
    "    \n",
    "    login_with_confident_api_key(os.getenv(\"DEEPEVAL_API_KEY\"))\n",
    "    deepeval_dataset = EvaluationDataset()\n",
    "    deepeval_dataset.pull(\n",
    "        alias=DATASET_ALIAS,\n",
    "        auto_convert_goldens_to_test_cases=True\n",
    "    )\n",
    "    \n",
    "    opik_dataset: Dataset = opik_client.create_dataset(\n",
    "        name=DATASET_ALIAS,\n",
    "        description=\"Evaluation dataset from DeepEval\"\n",
    "    )\n",
    "    \n",
    "    opik_dataset_items: List[Dict[str, Any]] = [vars(test_case) for test_case in deepeval_dataset.test_cases]\n",
    "    opik_dataset.insert(opik_dataset_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73af85ba",
   "metadata": {},
   "source": [
    "### Evaluation task\n",
    "\n",
    "The whole purpose of this function is so that one can compute the `actual output` at runtime. However, since we already have a full dataset ready, we can just map each dataset item to a dictionary. **Opik** follows a very similar concept like in **DeepEval** where the usage of **Golden**s is encouraged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1857dccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opik.api_objects.dataset.dataset_item import DatasetItem\n",
    "\n",
    "# This function is used during evaluation\n",
    "# For each item in the dataset, this function will be called\n",
    "# The output of the function is a dictionary containing the relevant parameters for the metrics\n",
    "def evaluation_task(item: DatasetItem) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"input\": item['input'],\n",
    "        \"output\": item['actual_output'],\n",
    "        \"expected_output\": item['expected_output'],\n",
    "        \"context\": item['retrieval_context'],\n",
    "        \"reference_context\": item['context']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d935ccb9",
   "metadata": {},
   "source": [
    "### Answer Relevance\n",
    "\n",
    "The following metric evaluates the pertinence of the `actual_output` with respect to the `input`. Missing or off-topic information will be penalized. The goal is to achieve a fully pertinent answer with no redundancies. Unlike **RAGAs** or **DeepEval** this framework evaluates in a different way. **RAGAs** takes the `actual_output` and generates *hypothetical questions* and then the average of the semantic similarity between them and the original questions is computed. **DeepEval** uses a multi-step approach, where the `actual_output` is first decomposed into statements, verdicts are then computed and then the final score is derived. **Opik** uses a single template to evaluate the `relevancy` of the answer, so it would be important to be able to have control over the prompt template. For that reason I've created a custom metric, which works the same way as the default one, however it provides a way to manually overwrite the template to test different ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70d3d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom.my_answer_relevance import MyAnswerRelevance\n",
    "\n",
    "# Do have in mind that you can also overwrite the `few shot examples` with your custom ones\n",
    "# Be sure to set the `few_shot_examples_no_context` to your custom list of examples, since my custom metric doesn't use context\n",
    "my_answer_relevance_metric = MyAnswerRelevance(\n",
    "    model=eval_model,\n",
    "    project_name=OPIK_PROJECT_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858a87ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b33861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e92d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opik.evaluation.metrics.llm_judges.answer_relevance.metric import AnswerRelevance\n",
    "from opik.evaluation.metrics.llm_judges.context_precision.metric import ContextPrecision\n",
    "from opik.evaluation.metrics.llm_judges.context_recall.metric import ContextRecall\n",
    "from opik.evaluation.metrics.llm_judges.factuality.metric import Factuality\n",
    "from opik.evaluation.metrics.llm_judges.usefulness.metric import Usefulness\n",
    "from opik.evaluation.metrics.llm_judges.hallucination.metric import Hallucination\n",
    "\n",
    "answer_relevance_metric = AnswerRelevance(\n",
    "    model=eval_model,\n",
    "    require_context=False,\n",
    "    project_name=OPIK_PROJECT_NAME\n",
    ")\n",
    "\n",
    "context_precision_metric = ContextPrecision(\n",
    "    model=eval_model,\n",
    "    project_name=OPIK_PROJECT_NAME\n",
    ")\n",
    "\n",
    "context_recall_metric = ContextRecall(\n",
    "    model=eval_model,\n",
    "    project_name=OPIK_PROJECT_NAME\n",
    ")\n",
    "\n",
    "factuality_metric = Factuality(\n",
    "    model=eval_model,\n",
    "    project_name=OPIK_PROJECT_NAME\n",
    ")\n",
    "\n",
    "usefulness_metric = Usefulness(\n",
    "    model=eval_model,\n",
    "    project_name=OPIK_PROJECT_NAME\n",
    ")\n",
    "\n",
    "hallucination_metric = Hallucination(\n",
    "    model=eval_model,\n",
    "    project_name=OPIK_PROJECT_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8582c010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation:   0%|          | 0/48 [00:00<?, ?it/s]OPIK: Started logging traces to the \"Evaluation Approaches for RAG\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=019691d7-a282-716b-a3b2-a145dc28568b&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n",
      "Evaluation: 100%|██████████| 48/48 [31:18<00:00, 39.14s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ DeepEval Dataset (48 samples) ────╮\n",
       "│                                    │\n",
       "│ <span style=\"font-weight: bold\">Total time:       </span> 00:31:19        │\n",
       "│ <span style=\"font-weight: bold\">Number of samples:</span> 48              │\n",
       "│                                    │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">hallucination_metric: 0.3021 (avg)</span> │\n",
       "│                                    │\n",
       "╰────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─ DeepEval Dataset (48 samples) ────╮\n",
       "│                                    │\n",
       "│ \u001b[1mTotal time:       \u001b[0m 00:31:19        │\n",
       "│ \u001b[1mNumber of samples:\u001b[0m 48              │\n",
       "│                                    │\n",
       "│ \u001b[1;32mhallucination_metric: 0.3021 (avg)\u001b[0m │\n",
       "│                                    │\n",
       "╰────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Uploading results to Opik <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Uploading results to Opik \u001b[33m...\u001b[0m \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">View the results <a href=\"https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=019691d7-9e4f-7cb2-8ffe-f0f289678664&dataset_id=019691c4-bccb-753b-8eba-1f818e3d504c&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\" target=\"_blank\">in your Opik dashboard</a>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "View the results \u001b]8;id=728789;https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=019691d7-9e4f-7cb2-8ffe-f0f289678664&dataset_id=019691c4-bccb-753b-8eba-1f818e3d504c&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\u001b\\in your Opik dashboard\u001b]8;;\u001b\\.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from opik.evaluation import evaluate\n",
    "from opik.evaluation.evaluation_result import EvaluationResult\n",
    "\n",
    "# There's a function called `evaluate_experiment`, which allows to add a metric to an already existing experiment.\n",
    "# This doesn't re-run the evaluation for the already existing metric scores.\n",
    "# https://www.comet.com/docs/opik/evaluation/update_existing_experiment\n",
    "\n",
    "eval_res: EvaluationResult = evaluate(\n",
    "    dataset=opik_dataset,\n",
    "    task=evaluation_task,\n",
    "    scoring_metrics=[\n",
    "        answer_relevance_metric,\n",
    "        context_precision_metric,\n",
    "        context_recall_metric,\n",
    "        factuality_metric,\n",
    "        usefulness_metric,\n",
    "        hallucination_metric\n",
    "    ],\n",
    "    experiment_name=\"First evaluation ever using Opik\",\n",
    "    project_name=os.getenv(\"OPIK_PROJECT_NAME\"),\n",
    "    experiment_config={\n",
    "        \"model\": os.getenv(\"CHAT_MODEL\")\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
