{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e77b42c8",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "* The **Opik** platform can either be hosted locally in a container or a cluster: [local hosting](https://www.comet.com/docs/opik/self-host/overview)\n",
    "\n",
    "* Alternatively, there's a cloud based platform free of charge: [cloud version](https://www.comet.com/signup).\n",
    "\n",
    "    * When using the cloud version: an API key and a workspace need to be specified.\n",
    "\n",
    "    * Similarly to **DeepEval**, the cloud version of **Opik** has an intuitive UI, where datasets, evaluations results and more can be stored.\n",
    "\n",
    "    * For the cloud based approach make sure to add the following to your `.env` file in the parent folder:\n",
    "    ```bash\n",
    "        OPIK_API_KEY=<your-api-key>\n",
    "        OPIK_WORKSPACE=<your-workspace>\n",
    "        # Setting this will automatically log traces for the project (Optional)\n",
    "        OPIK_PROJECT_NAME=<project-name>\n",
    "        OPIK_USAGE_REPORT_ENABLED=false  # Disable telemetry (Optional)\n",
    "        # By default creates a file called ~/.opik.config (on Linux) if it doesn't exist\n",
    "        OPIK_CONFIG_PATH=<filepath-to-your-config-file>  # Overwrite the file location (Optional)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcb76d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Opik is already configured. You can check the settings by viewing the config file at /home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/opik_eval/.opik.config\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import opik\n",
    "from typing import Final, Type\n",
    "from dotenv import load_dotenv\n",
    "from opik.exceptions import ConfigurationError\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "OPIK_API_KEY: Final[Type[str]] = str(os.getenv(\"OPIK_API_KEY\"))\n",
    "OPIK_WORKSPACE: Final[Type[str]] = str(os.getenv(\"OPIK_WORKSPACE\"))\n",
    "OPIK_PROJECT_NAME: Final[Type[str]] = str(os.getenv(\"OPIK_PROJECT_NAME\"))\n",
    "\n",
    "try:\n",
    "    # If you're using the locally hosted version set the `use_local=True and provide the url`\n",
    "    opik.configure(\n",
    "        api_key=OPIK_API_KEY,\n",
    "        workspace=OPIK_WORKSPACE\n",
    "    )\n",
    "except (ConfigurationError, ConnectionError) as ce:\n",
    "    print(f\"Error occurred: {str(ce)}. Please try again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fafd36d",
   "metadata": {},
   "source": [
    "### LLM and tracing\n",
    "\n",
    "* **Opik** uses **OpenAI** as the LLM-provider by default. To overwrite that create a `LiteLLMChatModel` instance with the model you want to use and specify your [input parameters](https://docs.litellm.ai/docs/completion/input).\n",
    "\n",
    "* If you want to add tracing capabilities so that all calls to `litellm` are traced to the **Opik** platform create the `OpikLogger` and set the `litellm` callbacks (Optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f3621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import litellm\n",
    "from dotenv import load_dotenv\n",
    "from opik.evaluation.models import LiteLLMChatModel\n",
    "from litellm.integrations.opik.opik import OpikLogger\n",
    "\n",
    "load_dotenv(\"../../env/rag.env\")\n",
    "\n",
    "CHAT_MODEL: Final[Type[str]] = str(os.getenv(\"CHAT_MODEL\"))\n",
    "\n",
    "# https://docs.litellm.ai/docs/completion/input\n",
    "eval_model = LiteLLMChatModel(\n",
    "    model_name=f\"ollama_chat/{CHAT_MODEL}\",\n",
    "    temperature=0.0,\n",
    "    response_format={\n",
    "        \"type\": \"json_object\"\n",
    "    },\n",
    "    api_base=\"http://localhost:11434\",\n",
    "    num_retries=3,\n",
    ")\n",
    "\n",
    "# This will trace all calls submitted to the LLM to Opik (Optional)\n",
    "opik_logger = OpikLogger()\n",
    "litellm.callbacks = [opik_logger]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9354cff4",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "For an evaluation/experiment in **Opik** the following is required:\n",
    "\n",
    "- an experiment:\n",
    "    - a single evaluation of the LLM application\n",
    "   \n",
    "    - during an experiment all items of the dataset get iterated on\n",
    "   \n",
    "    - an experiment consists of two main components:\n",
    "   \n",
    "        - configuration:\n",
    "            - one can store key-value-pairs unique to each experiment to track and compare different evaluation runs and verify, which set of hyperparameters yields the best performance\n",
    "\n",
    "        - experiment items:\n",
    "            - individual items from a dataset, which consist of `input`, `response`, `expected response` and `context`\n",
    "            - they get evaluated using the specified metrics and receive a trace, score and additional metadata\n",
    "\n",
    "- a dataset:\n",
    "    - **Opik** supports datasets, which are a collection of samples, that the LLM application will be evaluated on.\n",
    "\n",
    "- an evaluation task:\n",
    "    - a function that maps dataset items to a dictionary\n",
    "    - receives a dataset item as input and returns a dictionary, that contains all required parameters by a metric\n",
    "\n",
    "- a set of metrics\n",
    "    - quantatative measure, which helps us assess the performance of the LLM application\n",
    "    - the ones of relevance for the project are the LLM-as-a-judge one (LLM evaluates the experiment)\n",
    "    - additionally, one can overwrite the `BaseMetric` class for a custom metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a12d36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Created a \"test_1\" dataset at https://www.comet.com/opik/api/v1/session/redirect/datasets/?dataset_id=0196d8cf-637c-7f50-921d-51293f0f4a8f&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "from opik import Dataset\n",
    "from opik.api_objects.dataset.rest_operations import ApiError\n",
    "\n",
    "# Create an `Opik` client for interacting with the platform\n",
    "opik_client = opik.Opik(\n",
    "    project_name=OPIK_PROJECT_NAME,\n",
    "    workspace=OPIK_WORKSPACE,\n",
    "    api_key=OPIK_API_KEY,\n",
    ")\n",
    "\n",
    "# Enter the dataset id without the extension. \n",
    "# The datasets would be stored under `/ragas_eval/datasets`\n",
    "dataset_id: Final[Type[str]] = input(\"Enter the dataset id (Ex: dataset_1): \")\n",
    "dataset_filepath: Final[Type[str]] = f\"../ragas_eval/datasets/{dataset_id}.jsonl\"\n",
    "\n",
    "try:\n",
    "    # Fetch the dataset\n",
    "    dataset: Dataset = opik_client.get_dataset(name=dataset_id)\n",
    "except ApiError as ae:    \n",
    "    dataset: Dataset = opik_client.create_dataset(\n",
    "        name=dataset_id,\n",
    "        description=\"Evaluation dataset from DeepEval\"\n",
    "    )\n",
    "    \n",
    "    dataset.read_jsonl_from_file(\n",
    "        file_path=dataset_filepath,\n",
    "        keys_mapping={\n",
    "            # From RAGAs to Opik mappings\n",
    "            \"user_input\": \"input\",\n",
    "            \"response\": \"output\",\n",
    "            \"reference\": \"expected_output\",\n",
    "            \"retrieved_contexts\": \"context\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73af85ba",
   "metadata": {},
   "source": [
    "### Evaluation task\n",
    "\n",
    "The whole purpose of this function is so that one can compute the `actual output` at runtime. However, since we already have a full dataset ready, we can just map each dataset item to a dictionary. **Opik** follows a very similar concept like in **DeepEval** where the usage of **Golden**s is encouraged. This means that the actual output can be generated at `evaluation time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1857dccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opik.api_objects.dataset.dataset_item import DatasetItem\n",
    "\n",
    "# This function is used during evaluation\n",
    "# For each item in the dataset, this function will be called\n",
    "# The output of the function is a dictionary containing the relevant parameters for the metrics\n",
    "def evaluation_task(item: DatasetItem) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"input\": item['input'],\n",
    "        \"output\": item['output'],\n",
    "        \"expected_output\": item['expected_output'],\n",
    "        \"context\": item['context']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7045b183",
   "metadata": {},
   "source": [
    "### Hallucination\n",
    "\n",
    "This metric verifies if the `output` generated relative to the `input` is based on information from the `context`. This is the equivalent to the **Faithfulness** metric we know from **DeepEval** and **RAGAs**, however instead of trying to maximize the score, we would like to minimize it. As with all metrics in **Opik**, by default the evaluation takes place by submitting a singular prompt to the LLM without breaking the process into multiple steps. If we compare the approach to **DeepEval or RAGAs** we would find out that the other frameworks first decompose the output and then the context (in the case of **DeepEval**) and based on that the final verdict is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87581d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom.hallucination.metric import Hallucination\n",
    "\n",
    "hallucination = Hallucination(\n",
    "    model=eval_model,\n",
    "    project_name=OPIK_PROJECT_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d935ccb9",
   "metadata": {},
   "source": [
    "### Answer Relevance\n",
    "\n",
    "The following metric evaluates the pertinence of the `actual_output` with respect to the `input`. Missing or off-topic information will be penalized. The goal is to achieve a fully relevant answer with no redundancies. Unlike **RAGAs** or **DeepEval** this framework evaluates in a different way. **RAGAs** takes the `actual_output` and generates *hypothetical questions* and then the average of the semantic similarity between them and the original questions is computed. **DeepEval** uses a multi-step approach, where the `actual_output` is first decomposed into statements, verdicts are then computed and then the final score is derived. **Opik** uses a single template to evaluate the `relevancy` of the answer, so it would be important to be able to have control over the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450894d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom.answer_relevancy.metric import AnswerRelevance\n",
    "\n",
    "answer_relevance = AnswerRelevance(\n",
    "    model=eval_model,\n",
    "    project_name=OPIK_PROJECT_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381aff75",
   "metadata": {},
   "source": [
    "### Context Precision\n",
    "\n",
    "Context precision is a metric, which evaluates the retriever component in a RAG application. The idea is that a high context precision would translate into retrieving relevant information and ranking it high so that the user input could be augmented with relevant context maximizing the relevance and completenss of the LLM output. Unlike **RAGAs** and **DeepEval** there's no complex multi-step approach here, we just submit a single prompt template so it's worth playing around with the prompt itself to get the best possible results. You can also overwrite the examples I've provided in the metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0ad589",
   "metadata": {},
   "source": [
    "### Context Recall\n",
    "\n",
    "This metric is mostly about evaluating the `retriever` and in particular to verify if all the required information from the knowledge base has been retrieved. The retriever tends to exert a greater influence over the final answer of the RAG pipeline, so its crucial to have a metric, which tests exactly that. To test the `recall` of the pipeline we submit a single prompt and instruct the LLM to determine for each statement in the `expected output`, whether or not it is supported by a node in the context. The final formula is:\n",
    "* **Context Recall** = $\\frac{\\text{number statements supported by context}}{\\text{total number statements in the expected output}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abc6418",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8582c010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from opik.evaluation import evaluate\n",
    "from opik.evaluation.evaluation_result import EvaluationResult\n",
    "\n",
    "test_id: Final[Type[str]] = int(input(\"Enter the test id (Ex: 1: \"))\n",
    "df: pd.DataFrame = pd.read_csv(\"../../experiments.csv\")\n",
    "\n",
    "row = df[df['test_id'] == test_id].iloc[0]\n",
    "\n",
    "TOP_K = int(row['top_k'])                     # Number of top-k retrieved contexts during the generation\n",
    "MAX_TOKENS = int(row['max_tokens_to_sample']) # Tokens used when generating the response\n",
    "CHUNK_SIZE = int(row['chunk_size'])           # Size of the chunk used to split the context during the generation\n",
    "CHUNK_OVERLAP = int(row['chunk_overlap'])     # Overlap of the chunk used to split the context during the generation\n",
    "MODEL = str(row['chat_model'])                # The model used to generate the response, not the evaluation model\n",
    "TEMPERATURE = float(row['temperature'])       # Temperature used to generate the response\n",
    "DESCRIPTION = str(row.get('description'))\n",
    "\n",
    "eval_res: EvaluationResult = evaluate(\n",
    "    dataset=dataset,\n",
    "    task=evaluation_task,\n",
    "    scoring_metrics=[\n",
    "        hallucination,\n",
    "        answer_relevance\n",
    "    ],\n",
    "    experiment_name=f\"{dataset_id}_evaluation\",\n",
    "    project_name=OPIK_PROJECT_NAME,\n",
    "    experiment_config={\n",
    "        \"top_k\": TOP_K,\n",
    "        \"max_tokens\": MAX_TOKENS,\n",
    "        \"chunk_size\": CHUNK_SIZE,\n",
    "        \"chunk_overlap\": CHUNK_OVERLAP,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"model\": MODEL,\n",
    "        \"description\": DESCRIPTION\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb13f0f",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c5d5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_res.test_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
