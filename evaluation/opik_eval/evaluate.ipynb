{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e77b42c8",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "* The **Opik** platform can either be hosted locally in a container or a cluster: [local hosting](https://www.comet.com/docs/opik/self-host/overview)\n",
    "\n",
    "* Alternatively, there's a cloud based platform free of charge: [cloud version](https://www.comet.com/signup).\n",
    "    * When using the cloud version: an API key and a workspace need to be specified.\n",
    "    * Those can be retrieved in the platform. \n",
    "    * Similarly to **DeepEval**, the cloud version of **Opik** has an intuitive UI, where datasets, evaluations results and more can be stored.\n",
    "    * For the cloud based approach make sure to add the following to your `.env` file in the parent folder:\n",
    "    ```bash\n",
    "        OPIK_API_KEY=<your-api-key>\n",
    "        OPIK_WORKSPACE=<your-workspace>\n",
    "        # Setting this will automatically log traces for the project (Optional)\n",
    "        OPIK_PROJECT_NAME=<project-name>\n",
    "        OPIK_USAGE_REPORT_ENABLED=false  # Disable telemetry (Optional)\n",
    "        # By default creates a file called ~/.opik.config (on Linux) if it doesn't exist\n",
    "        OPIK_CONFIG_PATH=<filepath-to-your-config-file>  # Overwrite the file location (Optional)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd2eb6c-1395-4bb3-b15e-29cf37167dbd",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a326343-4628-4095-bd8f-1a5fe69538b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from typing import Final, Dict, Any\n",
    "from distutils.util import strtobool\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import opik\n",
    "from opik import Dataset\n",
    "from opik.exceptions import ConfigurationError\n",
    "from opik.evaluation import evaluate\n",
    "from opik.evaluation.models import LiteLLMChatModel\n",
    "from opik.evaluation.evaluation_result import EvaluationResult\n",
    "from opik.api_objects.dataset.dataset_item import DatasetItem\n",
    "from opik.api_objects.dataset.rest_operations import ApiError\n",
    "\n",
    "# Uncomment for tracing. In my case there were some errors\n",
    "# import litellm\n",
    "# from litellm.integrations.opik.opik import OpikLogger\n",
    "\n",
    "# Assuming you have created a file.\n",
    "# This will not be in the project by default since it contains sensitive data.\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# Retrieve all the evaluation parameters and other ones relevant for experiments\n",
    "load_dotenv(\"../../env/rag.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce649c7-b206-466d-85de-90d41df36295",
   "metadata": {},
   "source": [
    "### Configure Opik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb76d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPIK_API_KEY: Final[str] = str(os.getenv(\"OPIK_API_KEY\"))\n",
    "OPIK_WORKSPACE: Final[str] = str(os.getenv(\"OPIK_WORKSPACE\"))\n",
    "OPIK_PROJECT_NAME: Final[str] = str(os.getenv(\"OPIK_PROJECT_NAME\"))\n",
    "\n",
    "try:\n",
    "    # If you're using the locally hosted version set the `use_local=True and provide the url`\n",
    "    opik.configure(\n",
    "        api_key=OPIK_API_KEY,\n",
    "        workspace=OPIK_WORKSPACE,\n",
    "        \n",
    "    )\n",
    "except (ConfigurationError, ConnectionError) as ce:\n",
    "    raise Exception(f\"Error occurred: {str(ce)}. Please try again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fafd36d",
   "metadata": {},
   "source": [
    "### LLM and tracing\n",
    "\n",
    "* **Opik** uses **OpenAI** as the LLM-provider by default. To overwrite that create a `LiteLLMChatModel` instance with the model you want to use and specify your [input parameters](https://docs.litellm.ai/docs/completion/input).\n",
    "* If you want to add tracing capabilities so that all calls to `litellm` are traced to the **Opik** platform create the `OpikLogger` and set the `litellm` callbacks (Optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f3621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for all experiments\n",
    "EVALUATION_MODEL: Final[str] = str(os.getenv(\"EVALUATION_MODEL\"))\n",
    "EVALUATION_TEMPERATURE: Final[float] = float(os.getenv(\"EVALUATION_TEMPERATURE\"))\n",
    "\n",
    "# https://docs.litellm.ai/docs/completion/input\n",
    "eval_model = LiteLLMChatModel(\n",
    "    model_name=f\"ollama_chat/{EVALUATION_MODEL}\",\n",
    "    temperature=EVALUATION_TEMPERATURE,\n",
    "    response_format={\n",
    "        \"type\": \"json_object\" # Make sure this is set, since some metrics require JSON output\n",
    "    },\n",
    "    api_base=\"http://localhost:11434\", # Might differ in your case\n",
    "    num_retries=3,\n",
    ")\n",
    "\n",
    "# This will trace all calls submitted to the LLM to Opik (Optional)\n",
    "# I'm getting errors so I don't use it.\n",
    "# opik_logger = OpikLogger()\n",
    "# litellm.callbacks = [opik_logger]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9354cff4",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "For an evaluation/experiment in **Opik** the following is required:\n",
    "\n",
    "- an experiment:\n",
    "    - a single evaluation of the LLM application\n",
    "    - during an experiment all items of the dataset get iterated on\n",
    "    - an experiment consists of two main components:\n",
    "        - configuration:\n",
    "            - one can store key-value pairs unique to each experiment to track and compare different evaluation runs and verify, which set of hyperparameters yields the best performance\n",
    "\n",
    "        - experiment items:\n",
    "            - individual items from a dataset, which consist of `input`, `response`, `expected response` and `context`\n",
    "            - they get evaluated using the specified metrics and receive a trace, score and additional metadata\n",
    "\n",
    "- a dataset:\n",
    "    - **Opik** supports datasets, which are a collection of samples, that the LLM application will be evaluated on.\n",
    "\n",
    "- an evaluation task:\n",
    "    - a function that maps dataset items to a dictionary\n",
    "    - receives a dataset item as input and returns a dictionary, that contains all required parameters by a metric\n",
    "\n",
    "- a set of metrics\n",
    "    - quantatative measures, which helps us assess the performance of the LLM application\n",
    "    - additionally, one can overwrite the `BaseMetric` class for a custom metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a12d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an `Opik` client for interacting with the platform\n",
    "opik_client = opik.Opik(\n",
    "    project_name=OPIK_PROJECT_NAME,\n",
    "    workspace=OPIK_WORKSPACE,\n",
    "    api_key=OPIK_API_KEY,\n",
    ")\n",
    "\n",
    "# Enter the dataset id without the extension. \n",
    "# The datasets would be stored under `../ragas_eval/datasets`.\n",
    "# Alternatively provide your filepath.\n",
    "experiment_id: Final[int] = int(input(\"Enter the experiment id (Ex: 1): \"))\n",
    "dataset_filepath: Final[str] = f\"../ragas_eval/datasets/{experiment_id}_dataset.jsonl\"\n",
    "\n",
    "try:\n",
    "    # Fetch the dataset\n",
    "    dataset: Dataset = opik_client.get_dataset(name=f\"{experiment_id}_dataset\")\n",
    "except ApiError as ae: \n",
    "    # If it doesn't exist already, just create it\n",
    "    dataset: Dataset = opik_client.create_dataset(\n",
    "        name=f\"{experiment_id}_dataset\",\n",
    "        description=f\"Evaluation dataset [{experiment_id}]\"\n",
    "    )\n",
    "    \n",
    "    dataset.read_jsonl_from_file(\n",
    "        file_path=dataset_filepath,\n",
    "        keys_mapping={\n",
    "            # From RAGAs to Opik mappings\n",
    "            # Alternatively, map it from your fields to the ones expected by Opik\n",
    "            # The ones expected by Opik are on the right-hand side\n",
    "            \"user_input\": \"input\",\n",
    "            \"response\": \"output\",\n",
    "            \"reference\": \"expected_output\",\n",
    "            \"retrieved_contexts\": \"context\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73af85ba",
   "metadata": {},
   "source": [
    "### Evaluation task\n",
    "\n",
    "The whole purpose of this function is so that one can compute the `actual output` at runtime. So if you have a custom **RAG** pipeline, you could take each individual input and compute the output and get the context at evaluation time. However, since we already have a full dataset ready, we can just map each dataset item to a dictionary. **Opik** follows a very similar concept like in **DeepEval** where the usage of **Golden**s is encouraged. This means that the actual output can be generated at `evaluation time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1857dccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used during evaluation\n",
    "# For each item in the dataset, this function will be called\n",
    "# The output of the function is a dictionary containing the relevant parameters for the metrics\n",
    "def evaluation_task(item: DatasetItem) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"input\": item['input'],\n",
    "        \"output\": item['output'],\n",
    "        \"expected_output\": item['expected_output'],\n",
    "        \"context\": item['context']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7045b183",
   "metadata": {},
   "source": [
    "### Hallucination\n",
    "\n",
    "This metric verifies if the `output` generated relative to the `input` is based on information from the `context`. Cases where statements contain **contradictory** or **made-up** information are penalized.\n",
    "\n",
    "This is the equivalent to the **Faithfulness** metric we know from **DeepEval** and **RAGAs**, however instead of trying to maximize the score, we would like to minimize it.\n",
    "\n",
    "The evaluation takes place by submitting a singular prompt to the LLM, providing instructions and examples. Since we expect a JSON output, we provide a schema and parse the output to retrieve the score. \n",
    "\n",
    "The metric expects:\n",
    "- `input`\n",
    "- `output`\n",
    "- `context`\n",
    "\n",
    "**DeepEval** first decomposes both the `output` and `context` into statements and then it checks for hallucinations. The final score is: **Faithfulness** = $ \\frac{\\text{number of attributed statements in the response}}{\\text{all statements in the response}} $.\n",
    "\n",
    "**RAGAs** works in a similar fashion to **DeepEval**, however the `context` doesn't get decomposed into so called `truths`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87581d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom.hallucination.metric import Hallucination\n",
    "\n",
    "hallucination = Hallucination(\n",
    "    model=eval_model,\n",
    "    project_name=OPIK_PROJECT_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d935ccb9",
   "metadata": {},
   "source": [
    "### Answer Relevance\n",
    "\n",
    "The following metric evaluates the pertinence of the `output` with respect to the `input`. Missing or off-topic information will be penalized. The goal is to achieve a complete answer, which is on-topic amd addresses the `input` directly.\n",
    "\n",
    "The goal is to maximize this metric, following 3 dimensions:\n",
    "- completeness\n",
    "- conciseness\n",
    "- relevance\n",
    "\n",
    "**RAGAs** takes the `output` and generates *hypothetical questions* and then the average of the semantic similarity between them and the original questions is computed. There's no `LLM-as-a-judge` in this case.\n",
    "\n",
    "**DeepEval** uses a multi-step approach, where the `output` is first decomposed into statements, verdicts are then computed and then the final score is derived using the formula: **Answer Relevancy** = $ \\frac{\\text{relevant statements in the response}}{\\text{all statements in the response}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450894d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom.answer_relevancy.metric import AnswerRelevance\n",
    "\n",
    "answer_relevance = AnswerRelevance(\n",
    "    model=eval_model,\n",
    "    project_name=OPIK_PROJECT_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381aff75",
   "metadata": {},
   "source": [
    "### Context Precision\n",
    "\n",
    "Context precision is a metric, which evaluates the retriever component in a RAG application.\n",
    "\n",
    "The idea is that a **high** context precision would translate into **retrieving relevant information and ranking it high**, so that the user input could be augmented with relevant context maximizing the relevance and completeness of the LLM output.\n",
    "\n",
    "The higher the context is ranked, the better since it's going to appear earlier in the prompt and this way the LLM could utilize it better avoiding the [Lost In The Middle](https://arxiv.org/abs/2307.03172) problem.\n",
    "\n",
    "### Formula\n",
    "* WCP = $ \\frac{\\sum_{k=1}^{K}(\\text{Precision@k} \\times v_{k})}{\\text{Total number of relevant items in top K results}} $\n",
    "\n",
    "* `K` is the `limit` or number of documents fetched by the retriever\n",
    "\n",
    "* $ v_{k} $ is either 1 or 0, depending on the relevance of the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a034a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom.context_precision.metric import ContextPrecision\n",
    "\n",
    "context_precision = ContextPrecision(\n",
    "    model=eval_model,\n",
    "    project_name=OPIK_PROJECT_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0ad589",
   "metadata": {},
   "source": [
    "### Context Recall\n",
    "\n",
    "This metric is mostly about evaluating the `retriever`s ability to retrieve all information, that would be required to address a particular `input`. The retriever tends to exert a greater influence over the final answer of the RAG pipeline, so its crucial to have a metric, which tests exactly that.\n",
    "\n",
    "This metric first decomposes the `expected output` into individual statements, thereafter we classify each of them as either:\n",
    "- attributed (supported by the `context`)\n",
    "- not-attributed (not supported by the `context`)\n",
    "\n",
    "The final formula is:\n",
    "* **Context Recall** = $\\frac{\\text{number statements supported by context}}{\\text{total number statements in the expected output}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc6a8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom.context_recall.metric import ContextRecall\n",
    "\n",
    "context_recall = ContextRecall(\n",
    "    model=eval_model,\n",
    "    project_name=OPIK_PROJECT_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88d4a53",
   "metadata": {},
   "source": [
    "### Mean Reciprocal Rank\n",
    "\n",
    "This metric evaluates the `retrievers` effectiveness, by measuring how well it can rank the first relevant document based on relevance.\n",
    "\n",
    "In this project, we need to evaluate a RAG pipeline. When we retrieve data/context it can either be relevant or not. For this metric I compare each individual context that is retrieved with the `expected output`. If the semantic similarity score exceeds a predefined `threshold` then that context is deemed relevant. So we just take the multiplicative inverse of the rank of the first relevant document. \n",
    "\n",
    "To compute the `MRR` for the full dataset we need to average out all the scores for each entry in our dataset. This metric is `deterministic`, i.e. on each execution we should get identical results, with small margin of error.\n",
    "\n",
    "* **MRR** = $\\frac{1}{K} \\times \\sum_{k=1}^{K}(\\frac{1}{\\text{rank}_{i}})$\n",
    "\n",
    "* `K` is the limit or number of documents that were retrieved\n",
    "\n",
    "* `i` is the rank of the position of the first document deemed relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdde3fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom.mean_reciprocal_rank.reciprocal_rank.metric import ReciprocalRank\n",
    "from custom.mean_reciprocal_rank.metric import MeanReciprocalRank\n",
    "\n",
    "# For each data entry we compute the reciprocal rank individually\n",
    "reciprocal_ranks = [\n",
    "    ReciprocalRank(\n",
    "      project_name=OPIK_PROJECT_NAME  \n",
    "    ) \n",
    "    for _ in range(len(dataset.get_items()))\n",
    "]\n",
    "\n",
    "# This aggregate metric will then compute the average of all scores to get the MRR\n",
    "mrr = MeanReciprocalRank(\n",
    "    name=\"MRR\",\n",
    "    metrics=reciprocal_ranks,\n",
    "    project_name=OPIK_PROJECT_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abc6418",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e704e8-115d-41bd-9851-02a7c8b226f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df: pd.DataFrame = pd.read_csv(\"../../experiments.csv\")\n",
    "\n",
    "row = df[df['test_id'] == experiment_id].iloc[0]\n",
    "\n",
    "TOP_K: int = int(row['top_k'])                     # Number of top-k retrieved contexts during the generation\n",
    "MAX_TOKENS: int = int(row['max_tokens_to_sample']) # Tokens used when generating the response\n",
    "CHUNK_SIZE: int = int(row['chunk_size'])           # Size of the chunk used to split the context during the generation\n",
    "CHUNK_OVERLAP: int = int(row['chunk_overlap'])     # Overlap of the chunk used to split the context during the generation\n",
    "MODEL: str = str(row['chat_model'])                # The model used to generate the response, not the evaluation model\n",
    "TEMPERATURE: float = float(row['temperature'])     # Temperature used to generate the response\n",
    "DESCRIPTION: str = str(row.get('description'))\n",
    "\n",
    "use_vanilla_rag: bool = bool(\n",
    "    strtobool(os.getenv(\"VANILLA_RAG\"))\n",
    ")\n",
    "\n",
    "if use_vanilla_rag:\n",
    "    RAG_APPROACH: str = \"vanilla\"\n",
    "else:\n",
    "    RAG_APPROACH: str = \"query_fusion\" # RAG-Fusion\n",
    "\n",
    "print(f\"\"\"\n",
    "Evaluating experiment {experiment_id}:\n",
    "\n",
    "top-k = {TOP_K}\n",
    "max tokens = {MAX_TOKENS}\n",
    "chunk size = {CHUNK_SIZE}\n",
    "chunk overlap = {CHUNK_OVERLAP}\n",
    "model = {MODEL}                \n",
    "temperature = {TEMPERATURE}\n",
    "description = {DESCRIPTION.strip()}\n",
    "RAG approach = {RAG_APPROACH}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f257751-ceb3-40eb-b636-8761237c9424",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8582c010",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_res: EvaluationResult = evaluate(\n",
    "    dataset=dataset,\n",
    "    task=evaluation_task,\n",
    "    scoring_metrics=[\n",
    "        hallucination,\n",
    "        answer_relevance,\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        mrr\n",
    "    ],\n",
    "    experiment_name=f\"{experiment_id}_evaluation\",\n",
    "    project_name=OPIK_PROJECT_NAME,\n",
    "    experiment_config={\n",
    "        \"top_k\": TOP_K,\n",
    "        \"max_tokens\": MAX_TOKENS,\n",
    "        \"chunk_size\": CHUNK_SIZE,\n",
    "        \"chunk_overlap\": CHUNK_OVERLAP,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"model\": MODEL,\n",
    "        \"description\": DESCRIPTION,\n",
    "        \"rag_approach\": RAG_APPROACH\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb13f0f",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c5d5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "results = {}\n",
    "\n",
    "# You can choose to store the resuls as you prefer\n",
    "# I use this approach since it's easy to aggregate and get the average for each metric\n",
    "for test_result in eval_res.test_results:\n",
    "    for score_result in test_result.score_results:\n",
    "        metric_name: str = score_result.name\n",
    "        \n",
    "        if metric_name not in results:\n",
    "            results[metric_name] = []\n",
    "        \n",
    "        results[metric_name].append({\n",
    "            'value': score_result.value,\n",
    "            'reason': score_result.reason\n",
    "        })\n",
    "\n",
    "# Save to JSON\n",
    "with open(f'./res/{experiment_id}_evaluation.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791a9b45-3621-48c4-8e8f-6c0ebcacf9c6",
   "metadata": {},
   "source": [
    "Check out the average for each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ff2955",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    scores = {}\n",
    "    with open(file=f\"./res/{experiment_id}_evaluation.json\", mode=\"r\") as f:\n",
    "        scores = json.load(f)\n",
    "        \n",
    "        for metric, results in scores.items():\n",
    "            avg_metric = sum([result['value'] for result in results]) / len(results)\n",
    "            print(f\"{metric}: {avg_metric:.4f}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No evaluation results found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
