{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2fbUqr7LdHa",
        "outputId": "5472ddc0-b155-4fea-c942-53b895b416d0"
      },
      "source": [
        "This notebook will not be required at all for the most part. If at any time the regular notebook hangs, it usually means there's a timeout error. So you can use this notebook in `Google Colab`.\n",
        "\n",
        "- `curl -fsSL https://ollama.com/install.sh | sh`\n",
        "- `OLLAMA_KEEP_ALIVE=\"3h\" OLLAMA_CONTEXT_LENGTH=\"${LLM_CONTEXT_WINDOW_TOKENS:-16000}\" ollama serve &`\n",
        "- `ollama pull mxbai-embed-large && ollama pull llama3.1:8b-instruct-q4_1`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repository\n",
        "!git clone https://github.com/danielpetrov18/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-wUezoQLiaz",
        "outputId": "ed130d07-0c3c-4cdf-c18a-f44a39a52755"
      },
      "outputs": [],
      "source": [
        "# Switch into the deepeval evaluation folder\n",
        "%cd Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/deepeval_eval/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAYxtWMNLjuf",
        "outputId": "3b064a5e-9040-44bd-b93f-e06da5011359"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip3 install deepeval==3.2.1 python-dotenv==1.1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krhBFh2JLn3j",
        "outputId": "d6a12413-9d71-4608-9e2e-e609da176418"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from typing import (\n",
        "    Final, List, Dict, Any, Union, Optional\n",
        ")\n",
        "from multiprocessing import Process, Queue\n",
        "\n",
        "from pydantic import BaseModel\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata\n",
        "\n",
        "from deepeval import evaluate\n",
        "from deepeval.metrics import (\n",
        "    AnswerRelevancyMetric,\n",
        "    FaithfulnessMetric,\n",
        "    ContextualPrecisionMetric,\n",
        "    ContextualRecallMetric,\n",
        "    ContextualRelevancyMetric\n",
        ")\n",
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval.models.llms import OllamaModel\n",
        "from deepeval.dataset import EvaluationDataset\n",
        "from deepeval import login_with_confident_api_key\n",
        "from deepeval.evaluate.configs import (\n",
        "    AsyncConfig, CacheConfig, DisplayConfig, ErrorConfig\n",
        ")\n",
        "from deepeval.evaluate.evaluate import EvaluationResult\n",
        "\n",
        "# Custom prompts for the DeepEval metrics\n",
        "from prompts.custom_faithfulness_prompt import MyFaithfulnessTemplate\n",
        "from prompts.custom_answer_relevancy_prompt import MyAnswerRelevancyTemplate\n",
        "from prompts.custom_contextual_recall_prompt import MyContextualRecallTemplate\n",
        "from prompts.custom_contextual_precision_prompt import MyContextualPrecisionTemplate\n",
        "from prompts.custom_contextual_relevancy_prompt import MyContextualRelevancyTemplate\n",
        "\n",
        "# Loads all the RAG parameters like chunk size, chunk overlap, temperature, etc.\n",
        "load_dotenv(\"../../env/rag.env\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "t5_DjEhiLoQl",
        "outputId": "30701f41-0ee7-4c96-da02-bb86f7be92f4"
      },
      "outputs": [],
      "source": [
        "DEEPEVAL_API_KEY: str = userdata.get(\"DEEPEVAL_API_KEY\")\n",
        "os.environ[\"DEEPEVAL_API_KEY\"] = DEEPEVAL_API_KEY\n",
        "\n",
        "if os.getenv(\"DEEPEVAL_API_KEY\"):\n",
        "    deepeval_api_key: str = os.getenv(\"DEEPEVAL_API_KEY\")\n",
        "\n",
        "    # You should get a message letting you know you are logged-in.\n",
        "    login_with_confident_api_key(deepeval_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zj5kJgNFL0QB"
      },
      "outputs": [],
      "source": [
        "# By default DeepEval doesn't provide a way to set a timeout\n",
        "# Certain operations can block indefinitely\n",
        "# To avoid this I provide this custom solution\n",
        "\n",
        "class TimeoutOllamaModel(OllamaModel):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: Optional[str] = None,\n",
        "        base_url: Optional[str] = None,\n",
        "        temperature: float = 0,\n",
        "        timeout: Optional[int] = 1800,  # seconds\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(model, base_url, temperature, **kwargs)\n",
        "        self.timeout = timeout\n",
        "\n",
        "    def _run_chat(self, prompt: str, schema: Optional[BaseModel], q: Queue):\n",
        "        try:\n",
        "            chat_model = self.load_model()\n",
        "            response = chat_model.chat(\n",
        "                model=self.model_name,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                format=schema.model_json_schema() if schema else None,\n",
        "                options={\"temperature\": self.temperature},\n",
        "            )\n",
        "            q.put(response.message.content)\n",
        "        except Exception as e:\n",
        "            q.put(e)\n",
        "\n",
        "    def generate(self, prompt: str, schema: Optional[BaseModel] = None):\n",
        "        from multiprocessing import Queue\n",
        "\n",
        "        q = Queue()\n",
        "        p = Process(target=self._run_chat, args=(prompt, schema, q))\n",
        "        p.start()\n",
        "        p.join(self.timeout)\n",
        "\n",
        "        if p.is_alive():\n",
        "            p.terminate()\n",
        "            p.join()\n",
        "            print(f\"Ollama generation exceeded timeout of {self.timeout} seconds\")\n",
        "            raise TimeoutError(f\"Ollama generation exceeded timeout of {self.timeout} seconds\")\n",
        "\n",
        "        result = q.get()\n",
        "        if isinstance(result, Exception):\n",
        "            raise result\n",
        "\n",
        "        return (\n",
        "            schema.model_validate_json(result) if schema else result,\n",
        "            0.0,\n",
        "        )\n",
        "\n",
        "EVALUATION_MODEL: Final[str] = os.getenv(\"EVALUATION_MODEL\")\n",
        "\n",
        "eval_model = TimeoutOllamaModel(\n",
        "  model=EVALUATION_MODEL,\n",
        "  timeout=600, # 10 minutes\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4xXz4R5L1Bv",
        "outputId": "f74735b7-1250-4287-ac24-94bb1f60dbbc"
      },
      "outputs": [],
      "source": [
        "# The datasets are located under `../datasets`\n",
        "experiment_id: int = int(input(\"Please specify experiment_id (Ex. 1): \"))\n",
        "\n",
        "ragas_samples: List[Dict[str, Any]] = []\n",
        "try:\n",
        "    # Your filepath may vary\n",
        "    # Alternatively, you can pull a dataset from ConfidentAI\n",
        "    with open(\n",
        "        file=f\"../datasets/{experiment_id}_dataset.jsonl\",\n",
        "        mode=\"r\",\n",
        "        encoding=\"utf-8\"\n",
        "    ) as file:\n",
        "        for line in file:\n",
        "            if line.strip():  # Skip empty lines\n",
        "                ragas_samples.append(json.loads(line))\n",
        "\n",
        "    # Convert from JSON to LLMTestCase.\n",
        "    # Have in mind that RAGAs and DeepEval use similar names for the same parameters.\n",
        "    # We are mapping the RAGAs parameters to DeepEval ones.\n",
        "    test_cases: List[LLMTestCase] = []\n",
        "    for ragas_sample in ragas_samples:\n",
        "        test_case = LLMTestCase(\n",
        "            input=ragas_sample[\"user_input\"],\n",
        "            actual_output=ragas_sample[\"response\"],\n",
        "            expected_output=ragas_sample[\"reference\"],\n",
        "            retrieval_context=ragas_sample[\"retrieved_contexts\"],\n",
        "            context=ragas_sample[\"reference_contexts\"],\n",
        "        )\n",
        "        test_cases.append(test_case)\n",
        "\n",
        "    # The fully loaded and ready for evaluation dataset\n",
        "    evaluation_dataset = EvaluationDataset(test_cases=test_cases)\n",
        "except FileNotFoundError:\n",
        "    raise Exception(\n",
        "        f\"File: `../datasets/{experiment_id}_dataset.jsonl` containing test cases not found!\"\n",
        "    )\n",
        "except json.JSONDecodeError as e:\n",
        "    raise Exception (\n",
        "        f\"Error parsing JSONL file: {str(e)}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtPxd88lL5TH"
      },
      "outputs": [],
      "source": [
        "answer_relevancy = AnswerRelevancyMetric(\n",
        "    model=eval_model,\n",
        "    include_reason=False,\n",
        "    evaluation_template=MyAnswerRelevancyTemplate,\n",
        ")\n",
        "\n",
        "faithfulness = FaithfulnessMetric(\n",
        "    model=eval_model,\n",
        "    include_reason=False,\n",
        "    evaluation_template=MyFaithfulnessTemplate,\n",
        ")\n",
        "\n",
        "contextual_precision = ContextualPrecisionMetric(\n",
        "    model=eval_model,\n",
        "    include_reason=False,\n",
        "    evaluation_template=MyContextualPrecisionTemplate\n",
        ")\n",
        "\n",
        "contextual_recall = ContextualRecallMetric(\n",
        "    model=eval_model,\n",
        "    include_reason=False,\n",
        "    evaluation_template=MyContextualRecallTemplate\n",
        ")\n",
        "\n",
        "contextual_relevancy = ContextualRelevancyMetric(\n",
        "    model=eval_model,\n",
        "    include_reason=False,\n",
        "    evaluation_template=MyContextualRelevancyTemplate\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQPrjDmnL6gg"
      },
      "outputs": [],
      "source": [
        "# https://www.deepeval.com/docs/evaluation-flags-and-configs\n",
        "async_conf = AsyncConfig(\n",
        "    run_async=False,\n",
        ")\n",
        "\n",
        "cache_conf = CacheConfig(\n",
        "    write_cache=True,\n",
        "    use_cache=True\n",
        ")\n",
        "\n",
        "display_conf = DisplayConfig(\n",
        "    show_indicator=True,\n",
        "    print_results=True,\n",
        "    verbose_mode=False\n",
        ")\n",
        "\n",
        "error_conf = ErrorConfig(\n",
        "    ignore_errors=True,\n",
        "    skip_on_missing_params=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "431bfa2b87d347ba9c248c23fba83cfb",
            "967c4a1c047a4ef29f5408acdebe7c51"
          ]
        },
        "id": "Ojzrm1EGL8W1",
        "outputId": "3bb1fc24-ed26-4b2d-f505-deb70bfaae4c"
      },
      "outputs": [],
      "source": [
        "results: EvaluationResult = evaluate(\n",
        "    test_cases=evaluation_dataset.test_cases,\n",
        "    metrics=[\n",
        "        answer_relevancy,\n",
        "        faithfulness,\n",
        "        contextual_precision,\n",
        "        contextual_recall,\n",
        "        contextual_relevancy\n",
        "    ],\n",
        "    #hyperparameters={},\n",
        "    identifier=f\"{experiment_id}_experiment\",\n",
        "    async_config=async_conf,\n",
        "    cache_config=cache_conf,\n",
        "    display_config=display_conf,\n",
        "    error_config=error_conf\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEjBLwNAL-pk"
      },
      "outputs": [],
      "source": [
        "# Extract the scores\n",
        "scores: Dict[str, List[Dict[str, Union[str, float]]]] = {} # Metric name -> list of test case name and score\n",
        "\n",
        "for test_result in results.test_results: # Iterate over the test cases\n",
        "    for metric in test_result.metrics_data: # Iterate over the metrics\n",
        "        # Get the test case number\n",
        "        test_case: int = int(test_result.name.split(\"_\")[-1]) # Make sure to convert to int, so we can sort it properly\n",
        "        if metric.name in scores:\n",
        "            scores[metric.name].append({ \"test_case\": test_case, \"score\": metric.score })\n",
        "        else:\n",
        "            scores[metric.name] = [{ \"test_case\": test_case, \"score\": metric.score }]\n",
        "\n",
        "# Sort properly, since DeepEval doesn't evaluate in the original order\n",
        "for metric_name in scores:\n",
        "    scores[metric_name] = sorted(scores[metric_name], key=lambda x: x[\"test_case\"])\n",
        "\n",
        "# Make sure the directory exists\n",
        "os.makedirs(\"./res\", exist_ok=True)\n",
        "\n",
        "# Store on disk\n",
        "with open(f\"./res/{experiment_id}_eval.jsonl\", \"w\") as f:\n",
        "    for metric_name, metric_scores in scores.items():\n",
        "        record = {\n",
        "            \"metric\": metric_name,\n",
        "            \"scores\": metric_scores  # This is the sorted list of {\"test_case\": ..., \"score\": ...}\n",
        "        }\n",
        "        f.write(json.dumps(record) + \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "431bfa2b87d347ba9c248c23fba83cfb": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_967c4a1c047a4ef29f5408acdebe7c51",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluating 52 test case(s) sequentially <span style=\"color: #11ff00; text-decoration-color: #11ff00\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━</span> <span style=\"color: #00e5ff; text-decoration-color: #00e5ff\"> 98%</span> <span style=\"color: #5703ff; text-decoration-color: #5703ff\">2:00:46</span>\n</pre>\n",
                  "text/plain": "Evaluating 52 test case(s) sequentially \u001b[38;2;17;255;0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;17;255;0m╸\u001b[0m\u001b[38;5;237m━\u001b[0m \u001b[38;2;0;229;255m 98%\u001b[0m \u001b[38;2;87;3;255m2:00:46\u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "967c4a1c047a4ef29f5408acdebe7c51": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
