{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a synthetic dataset using DeepEval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthesizer\n",
    "\n",
    "This object can be used to generate **Golden** instances, which consist out of **input**, **expected output** and **context**. It uses a LLM to come up with random input values based on a context and thereafter tries to enhance those, by making them more complex and realistic through evolutions.\n",
    "\n",
    "For a comprehensive guide on understanding how this object works please refer here: [Synthesizer](https://www.confident-ai.com/blog/the-definitive-guide-to-synthetic-data-generation-using-llms)\n",
    "\n",
    "### Summary\n",
    "\n",
    "I will try to summarize the most important information:\n",
    "\n",
    "* It uses a **LLM to come-up with a comprehensive dataset** much faster than a human can\n",
    "\n",
    "* The process starts with the LLM generating **synthetic queries** based on context from a knowledge base\n",
    "\n",
    "* Those initial queries are then **evolved** to reflect real-life complexity and then together with the context can be used to generate a **target/expected output**\n",
    "\n",
    "![Dataset generation workflow](../../img/deepeval/synthesizer-overview.png \"Synthetic generation\")\n",
    "\n",
    "* There exist two main methods:\n",
    "    \n",
    "    - Self-improvement: Iteratively uses the LLMs output to generate more complex queries\n",
    "\n",
    "    - Distillation: A stronger model is being utilized \n",
    "\n",
    "* Constructing contexts:\n",
    "    - During this phase documents from the knowledge base are split using a *token splitter*\n",
    "\n",
    "    - A random chunk is selected\n",
    "    \n",
    "    - Finally, additional chunks are retrieved based on **semantic similarity**, **knowledge graphs** or other approaches\n",
    "    \n",
    "    - Ensuring that **chunk size**, **chunk overlap** or other similar parameters here and in the **retrieval component** of the **RAG** application are identical will yield better results\n",
    "\n",
    "![Constructing contexts](../../img/deepeval/synthesizer-context.png \"Context construction\")\n",
    "\n",
    "* Constructing synthetic queries:\n",
    "    - In **RAG** when a user submits a query, all the relevant data is retrieved and then a template augments the input with the context. The `synthesizer` reverses the approach.\n",
    "\n",
    "    - Using the contexts the **Synthesizer** can now generate synthetic input\n",
    "\n",
    "    - Doing so we ensure that the input corresponds with the context enhancing the **relevancy** and **accuracy**\n",
    "\n",
    "![Constructing synthetic queries](../../img/deepeval/synthesizer-query.png \"Synthetic queries creation\")\n",
    "\n",
    "* Data Filtering:\n",
    "\n",
    "    Data filtering is important after you have the `synthetic query`, `context` and optionally `reference answer` as to make sure one doesn't try to refine flawed queries and to waste valuable resources. Filtering occurs at 2 critical stages:\n",
    "\n",
    "    1. Context filtering: Removes low-quality chunks that may be unintelligible, due to whitespaces for example\n",
    "\n",
    "    ![Context filtering](../../img/deepeval/synthesizer-context-filtering.png \"Filtering context\")\n",
    "\n",
    "    2. Input filtering: Ensures generated inputs meet quality standards. Sometimes even with good and well-structured context an input might be somewhat ambiguous or unclear based on the context.\n",
    "\n",
    "    ![Input filtering](../../img/deepeval/synthesizer-query-filtering.png \"Filtering queries\")\n",
    "    \n",
    "* Customizing dataset generating:\n",
    "    - Depending on the scenario inputs and outputs can be tailored to specific use cases\n",
    "        \n",
    "        - For example a medical chatbot would have a completely different behaviour than a scientific one. It would need to comfort patients and avoid bias. Also false negatives could turn out to be quite dangerous.\n",
    "    \n",
    "* Data Evolution:\n",
    "    This is crucial for the proper generation of a dataset, since it iteratively refines the dataset.\n",
    "\n",
    "    - **In-Depth Evolving**: Expands simple instructions into more detailed versions\n",
    "\n",
    "    - **In-Breadth Evolving**: Produces diverse instructions to enrich the dataset\n",
    "    \n",
    "    - **Elimination Evolving**: Removes less effective instructions\n",
    "\n",
    "    ![Data evolution types](../../img/deepeval/synthesizer-evolution.png \"Data Evolution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "* To install the dependencies run the `setup` bash script in the root of the `evaluation` folder.\n",
    "\n",
    "* Make sure you select the correct kernel (eval) in your notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After installing the dependencies and selecting the kernel you should be good to go.\n",
    "# Make sure the package is installed before continuing further.\n",
    "! pip3 show deepeval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://huggingface.co/datasets/explodinggradients/ragas-airline-dataset data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM provider\n",
    "\n",
    "**DeepEval** uses **OpenAI** by default as a LLM provider, however **Ollama** is also available. To use it execute the code cell below. This will generate a `.deepeval` file where key-value pairs will be stored about that particular LLM-provider like model name, base url and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Final\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../../env/rag.env\")\n",
    "\n",
    "DATA_GENERATION_MODEL: Final[str] = os.getenv(\"DATA_GENERATION_MODEL\")\n",
    "EMBEDDING_MODEL: Final[str] = os.getenv(\"EMBEDDING_MODEL\")\n",
    "\n",
    "! deepeval set-ollama {DATA_GENERATION_MODEL} --base-url=\"http://localhost:11434/\"\n",
    "! deepeval set-ollama-embeddings {EMBEDDING_MODEL} --base-url=\"http://localhost:11434\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting chunks from knowledge base to be used as context in data generation\n",
    "\n",
    "\n",
    "Since **DeepEval** uses a `TokenTextSplitter` when trying to generate synthetic dataset from [documents](https://www.deepeval.com/docs/synthesizer-generate-from-docs), and `R2R` uses `RecursiveCharacterTextSplitter` we need to perform this step ourselves. Then we can generate a dataset from [contexts](https://www.deepeval.com/docs/synthesizer-generate-from-contexts).\n",
    "\n",
    "**Before executing the next cells:**\n",
    "* Make sure Ollama is up and running.\n",
    "\n",
    "* Download the required models for generation and embedding.\n",
    "\n",
    "* Make sure the RAG application is running, if not `./run.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import ollama\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../../env/rag.env\")\n",
    "\n",
    "# These objects are going to be required when performing semantic search (using the embedding model)\n",
    "\n",
    "ollama_client = ollama.Client(host=\"http://localhost:11434\")\n",
    "\n",
    "ollama_options = ollama.Options(\n",
    "    temperature=float(os.getenv(\"TEMPERATURE\")),\n",
    "    top_p=float(os.getenv(\"TOP_P\")),\n",
    "    top_k=int(os.getenv(\"TOP_K\")),\n",
    "    num_ctx=int(os.getenv(\"LLM_CONTEXT_WINDOW_TOKENS\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple, List\n",
    "\n",
    "# Retrieve this many chunks from a document at random\n",
    "CHUNKS_PER_DOCUMENT: Final[int] = 3\n",
    "\n",
    "# A context is a collection of chunks that share some degree of similarity\n",
    "CHUNKS_PER_CONTEXT: Final[int] = 3\n",
    "\n",
    "# For each chunk have the id as key and compute the embedding to the text\n",
    "CHUNKS_WITH_EMBEDDINGS: Dict[str, Tuple[str, List[float]]] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# First we need to authenticate the admin user and receive a token\n",
    "# The username and password are the default ones provided by `R2R`\n",
    "# Note that you can overwrite those in the `config.toml` file\n",
    "# If this fails it means there're either connectivity issues or the credentials are wrong\n",
    "authetication: requests.Response = requests.post(\n",
    "    url=\"http://localhost:7272/v3/users/login\", # This may vary depending on your setup\n",
    "    headers={\n",
    "        \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
    "    },\n",
    "    data=\"username=admin@example.com&password=change_me_immediately\",\n",
    ")\n",
    "token: str = authetication.json()['results']['access_token']['token'] # Token for further authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the IDs of all currently ingested documents\n",
    "documents: requests.Response = requests.get(\n",
    "    url=\"http://localhost:7272/v3/documents\",\n",
    "    headers={\n",
    "        \"Authorization\": f\"Bearer {token}\"\n",
    "    }\n",
    ")\n",
    "\n",
    "doc_ids: List[str] = [document['id'] for document in documents.json()['results']]\n",
    "print(f\"Found {len(doc_ids)} documents\")\n",
    "\n",
    "# Delete all documents available\n",
    "for doc_id in doc_ids:\n",
    "    del_resp: requests.Response = requests.delete(\n",
    "        url=f\"http://localhost:7272/v3/documents/{doc_id}\",\n",
    "        headers={\n",
    "            \"Authorization\": f\"Bearer {token}\"\n",
    "        }\n",
    "    )\n",
    "    if del_resp.status_code == 200:\n",
    "        print(f\"Deleted document with ID: {doc_id}\")\n",
    "    else:\n",
    "        print(f\"Failed to delete document with ID: {doc_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import mimetypes\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "# Load files\n",
    "loader = DirectoryLoader(\n",
    "    \"./data\", # The folder, where the documents are stored at.\n",
    "    glob=\"**/*.md\",\n",
    "    exclude=\"README.md\"\n",
    ")\n",
    "docs: list[Document] = loader.load()\n",
    "\n",
    "# Clean-up markdown\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    for doc in docs:\n",
    "        doc_filepath: str = doc.metadata['source'].split(\"/\")[-1]\n",
    "        temp_file_path = os.path.join(temp_dir, doc_filepath)\n",
    "        with open(temp_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(doc.page_content)\n",
    "\n",
    "    # Ingest individual files, on every run so that chunk size and chunk overlap match the experiment config\n",
    "    # If working with another dataset modify as required\n",
    "    for i, file in enumerate(os.listdir(temp_dir), 1):\n",
    "        if not file.endswith(\".md\"):\n",
    "            continue\n",
    "\n",
    "        filepath = os.path.join(temp_dir, file)\n",
    "\n",
    "        # Guess the content type (MIME type) based on file extension\n",
    "        mime_type, _ = mimetypes.guess_type(filepath)\n",
    "        if mime_type is None:\n",
    "            mime_type = \"application/octet-stream\"  # fallback if unknown\n",
    "\n",
    "        with open(filepath, \"rb\") as content:\n",
    "            # Ingest file - extract text, chunk it, generate embeddings and finally store in vector store\n",
    "            ingestion_resp: requests.Response = requests.post(\n",
    "                url=\"http://localhost:7272/v3/documents\",\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Bearer {token}\"\n",
    "                },\n",
    "                files={\n",
    "                    \"file\": (file, content, mime_type)\n",
    "                },\n",
    "                data={\n",
    "                    \"metadata\": \"{}\", # Feel free to add your own metadata\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            if ingestion_resp.status_code == 202:\n",
    "                print(f\"[{i}]. Ingested: {file}\")\n",
    "            else:\n",
    "                print(ingestion_resp.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to embed an input text and perform semantic similarity using `cosine distance` as measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_embedding(text: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Uses `ollama` to convert the text to an embedding preserving the semantic meaning.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to compute the embedding for\n",
    "    \n",
    "    Returns:\n",
    "        List of floats representing the vector embedding\n",
    "    \"\"\"\n",
    "    return ollama_client.embeddings(\n",
    "        model=os.getenv(\"EMBEDDING_MODEL\"),\n",
    "        prompt=text,\n",
    "        options=ollama_options\n",
    "    )[\"embedding\"]\n",
    "\n",
    "def compute_similarity(embedding1: List[float], embedding2: List[float]) -> float:\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between two vector embeddings.\n",
    "    \n",
    "    Args:\n",
    "        embedding1: First vector embedding\n",
    "        embedding2: Second vector embedding\n",
    "    \n",
    "    Returns:\n",
    "        float: Cosine similarity score between the two embeddings. \n",
    "        The closer to 1, the more similar the embeddings are.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the embeddings have different lengths\n",
    "    \"\"\"\n",
    "    if len(embedding1) != len(embedding2):\n",
    "        raise ValueError(\"Embeddings must have the same length!\")\n",
    "\n",
    "    vec1 = np.array(embedding1)\n",
    "    vec2 = np.array(embedding2)\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every available chunk in the knowledge base we compute the embedding and store it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks: requests.Response = requests.get(\n",
    "    url=\"http://localhost:7272/v3/chunks\",\n",
    "    headers={\n",
    "        \"Authorization\": f\"Bearer {token}\"\n",
    "    }\n",
    ")\n",
    "\n",
    "if chunks.status_code != 200:\n",
    "    raise Exception(\"Failed to retrieve chunks from vector store\")\n",
    "\n",
    "for chunk in chunks.json()['results']:\n",
    "    CHUNKS_WITH_EMBEDDINGS[chunk['id']] = (\n",
    "        chunk['text'],\n",
    "        compute_embedding(chunk['text'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_n_similar_chunks(current_chunk_id: str, n: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    It receives a `chunk_id` of the current chunk and returns the `n` most similar chunks.\n",
    "    It makes use of `compute_similarity` to compute the cosine similarity between chunks.\n",
    "    The closer to 1, the more similar the chunks are.\n",
    "    The chunks are sorted in descending order by similarity.\n",
    "    Finally, we make sure that only the `n` chunks are kept or if less available we keep all.\n",
    "    \n",
    "    Args:\n",
    "        current_chunk_id: ID of the current chunk\n",
    "        n: Number of similar chunks to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: list of chunk texts\n",
    "    \"\"\"\n",
    "    similarities: Dict[str, float] = {} # chunk_id -> similarity\n",
    "\n",
    "    for chunk_id, (_, embedding) in CHUNKS_WITH_EMBEDDINGS.items():\n",
    "        # We don't want to consider the same chunk relevant for the context\n",
    "        if chunk_id == current_chunk_id:\n",
    "            continue\n",
    "\n",
    "        similarity: float = compute_similarity(\n",
    "            embedding1 = CHUNKS_WITH_EMBEDDINGS[current_chunk_id][1],\n",
    "            embedding2 = embedding\n",
    "        )\n",
    "        similarities[chunk_id] = similarity # chunk_id -> similarity\n",
    "\n",
    "    # Sort by similarity in descending order\n",
    "    most_similar = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top `n` chunk IDs\n",
    "    # If less available we keep all\n",
    "    if len(most_similar) < n:\n",
    "        n = len(most_similar)\n",
    "    \n",
    "    # Retrieve all chunk texts, by not exceeding `n`\n",
    "    top_n_chunks: List[str] = [CHUNKS_WITH_EMBEDDINGS[chunk_id][0] for chunk_id, _ in most_similar[:n]]\n",
    "\n",
    "    return top_n_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The algorithm I follow:\n",
    "    1. I go over all the ingested documents\n",
    "    2. Select 3 or len(chunks) chunks at random for each document\n",
    "    3. For each chunk out ouf every document I select 2 other chunks using semantic similarity (out of all the documents)\n",
    "    4. Finally, all 3 chunks are grouped together and form a context\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "\n",
    "def extract_context_chunks() -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Each context contains chunks, which are similar to each other.\n",
    "    The chunks in a given context might be derived from different documents.\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: list of contexts\n",
    "    \"\"\"\n",
    "    contexts: List[List[str]] = []\n",
    "\n",
    "    # Fetch all documents\n",
    "    documents: requests.Response = requests.get(\n",
    "        url=\"http://localhost:7272/v3/documents\",\n",
    "        headers={\n",
    "            \"Authorization\": f\"Bearer {token}\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Make sure that the request was successful\n",
    "    if documents.status_code != 200:\n",
    "        raise Exception(\"Failed to retrieve documents from vector store\")\n",
    "    \n",
    "    # Iterate over all documents\n",
    "    for i, document in enumerate(documents.json()['results'], 1):\n",
    "\n",
    "        # Fetch all chunks of a document\n",
    "        doc_chunks: requests.Response = requests.get(\n",
    "            url=f\"http://localhost:7272/v3/documents/{document['id']}/chunks\",\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {token}\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Make sure that the request was successful\n",
    "        if doc_chunks.status_code != 200:\n",
    "            raise Exception(\n",
    "                f\"Failed to retrieve chunks from vector store for {document['id']}\"\n",
    "            )\n",
    "\n",
    "        chunk_ids: List[str] = [chunk['id'] for chunk in doc_chunks.json()['results']]\n",
    "\n",
    "        # Select 3 or len(chunks) chunks at random for each document\n",
    "        random_chunk_ids: List[str] = list(chunk_id for chunk_id in random.sample(\n",
    "                chunk_ids,\n",
    "                min(CHUNKS_PER_DOCUMENT, len(chunk_ids)) # At most CHUNKS_PER_DOCUMENT for each document\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Create contexts \n",
    "        for j, chunk_id in enumerate(random_chunk_ids, 1):\n",
    "            initial_text: str = CHUNKS_WITH_EMBEDDINGS[chunk_id][0]\n",
    "            context: List[str] = [initial_text]\n",
    "            similar_chunks: List[str] = retrieve_n_similar_chunks(\n",
    "                chunk_id,\n",
    "                CHUNKS_PER_CONTEXT - 1\n",
    "            )\n",
    "            context.extend(similar_chunks)\n",
    "            contexts.append(context)\n",
    "            print(f\"Extracted context {len(contexts)} from document {i} and chunk {j}\")\n",
    "\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create contexts to be used for synthetic data generation by **DeepEval**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "experiment_id: int = int(input(\"Enter experiment id (Ex. 1): \"))\n",
    "contexts_filename: str = f\"{experiment_id}_contexts\"\n",
    "\n",
    "context_chunks: List[List[str]] = extract_context_chunks()\n",
    "\n",
    "with open(f\"./contexts/{contexts_filename}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        context_chunks,\n",
    "        f,\n",
    "        ensure_ascii=False,\n",
    "        indent=4\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtration config** serves as a way to configure the quality of the generated synthetic input queries. Having higher threshold would ensure that the input queries are of higher quality.\n",
    "\n",
    "If the **quality_score** is still lower than the **synthetic_input_quality_threshold** after **max_quality_retries**, the **golden with the highest quality_score** will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.synthesizer.config import FiltrationConfig\n",
    "\n",
    "# (This step is completely OPTIONAL)\n",
    "# https://www.deepeval.com/docs/synthesizer-introduction\n",
    "filtration_config = FiltrationConfig(\n",
    "    synthetic_input_quality_threshold=0.7,\n",
    "    max_quality_retries=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evolutions** are used to specify the type of approach to use when trying to complicate the synthetic queries. Since this is a **RAG** application I will only use the evolution types which use **context**. The `num_evolutions` parameter can be configured to specify the number of iterations for performing those evolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.synthesizer.config import (\n",
    "    Evolution,\n",
    "    EvolutionConfig,\n",
    ")\n",
    "\n",
    "# (This step is completely OPTIONAL)\n",
    "# https://www.deepeval.com/docs/synthesizer-introduction\n",
    "evolution_config = EvolutionConfig(\n",
    "    num_evolutions=1,\n",
    "    evolutions={\n",
    "        Evolution.MULTICONTEXT: 0.25,\n",
    "        Evolution.CONCRETIZING: 0.25,\n",
    "        Evolution.CONSTRAINED: 0.25,\n",
    "        Evolution.COMPARATIVE: 0.25,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthesizer\n",
    "\n",
    "The synthesizer object as explained at the beginning of the notebook can be used to generate the synthetic dataset. It provides four different methods for this current version of **DeepEval**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.synthesizer import Synthesizer\n",
    "\n",
    "# https://www.deepeval.com/docs/synthesizer-introduction\n",
    "synthesizer = Synthesizer(\n",
    "    filtration_config=filtration_config,\n",
    "    evolution_config=evolution_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the goldens\n",
    "\n",
    "In this notebook I use the `generate_goldens_from_contexts`, which actually skips some steps that are specified in the synthesizer section - the loading and splitting of documents. This provides more freedom, however one has to be careful to properly ingest the documents and to derive high-quality contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.dataset.golden import Golden\n",
    "\n",
    "goldens: list[Golden] = synthesizer.generate_goldens_from_contexts(\n",
    "    contexts=context_chunks,\n",
    "    include_expected_output=True,\n",
    "    max_goldens_per_context=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confident AI (Optional)\n",
    "\n",
    "1. In short **Confident AI** is a cloud-based platform part of the **DeepEval** project, which stores **datasets**, **evaluations**, **traces**, etc. \n",
    "\n",
    "2. If you want to use **Confident AI** platform create an account from here: [Confident AI](https://www.confident-ai.com/)\n",
    "\n",
    "3. After signing-up an **API key** will be generated, which can be used to interact with the platform from inside the notebook.\n",
    "\n",
    "---\n",
    "\n",
    "Example of .env file:\n",
    "```bash\n",
    "DEEPEVAL_RESULTS_FOLDER=<folder> # Results of evaluations can be saved locally (cache)\n",
    "DEEPEVAL_API_KEY=<your api key>  # Relevant if you want to use Confident AI\n",
    "DEEPEVAL_TELEMETRY_OPT_OUT=\"YES\" # Remove telemetry\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from deepeval import login_with_confident_api_key\n",
    "\n",
    "# Loads the environment variables from a `.env` file.\n",
    "# If you want to use Confident AI be sure to create one in this directory.\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "deepeval_api_key: str = os.getenv(\"DEEPEVAL_API_KEY\")\n",
    "\n",
    "# You should get a message letting you know you are logged-in.\n",
    "login_with_confident_api_key(deepeval_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to visit the link provided, upon invoking the `push` method. This will redirect you to the page containing the `goldens`. Then you can clean-up the data and that would almost always be mandatory, since we are using a weak model in the project and the input will not always be **clean**.\n",
    "\n",
    "Do note that if the `push` to the cloud fails you might need to upgrade **DeepEval** to the latest version. To do so run:\n",
    "`pip3 install --upgrade deepeval`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "dataset = EvaluationDataset(goldens=goldens)\n",
    "\n",
    "alias: str = f\"{experiment_id}_dataset\"\n",
    "\n",
    "dataset.push(\n",
    "    alias=alias,\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning up the dataset, you can pull it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "# I did some cleaning on the data since the input was not fully in the expected format on the ConfidentAI platform.\n",
    "final_dataset = EvaluationDataset()\n",
    "final_dataset.pull(alias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling the missing fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure this as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from distutils.util import strtobool\n",
    "from typing import Dict, Union, Final\n",
    "\n",
    "# VANILLA_RAG=False would mean RAG-Fusion\n",
    "use_vanilla_rag: bool = bool(\n",
    "    strtobool(os.getenv(\"VANILLA_RAG\"))\n",
    ")\n",
    "\n",
    "if use_vanilla_rag:\n",
    "    search_strategy: str = \"vanilla\"\n",
    "else:\n",
    "    search_strategy: str = \"query_fusion\" # RAG-Fusion\n",
    "\n",
    "# Used after context had been fetched to generate the final response\n",
    "RAG_GENERATION_CONFIG: Final[Dict[str, Union[str, float, int]]] = {\n",
    "    \"model\": f\"ollama_chat/{os.getenv('CHAT_MODEL')}\",\n",
    "    \"temperature\": float(os.getenv(\"TEMPERATURE\")),\n",
    "    \"top_p\": float(os.getenv(\"TOP_P\")),\n",
    "    \"max_tokens_to_sample\": int(os.getenv(\"MAX_TOKENS\")),\n",
    "}\n",
    "\n",
    "# Relevant during the retrieval phase for fetching relevant context\n",
    "SEARCH_SETTINGS: Final[Dict[str, Union[bool, int, str]]] = {\n",
    "    \"use_semantic_search\": True,\n",
    "    \"limit\": int(os.getenv(\"TOP_K\")),\n",
    "    \"offset\": 0,\n",
    "    \"include_metadatas\": False,\n",
    "    \"include_scores\": True,\n",
    "    \"search_strategy\": search_strategy, # can be vanilla or hyde, (fusion is also supported)\n",
    "    \"chunk_settings\": {\n",
    "        \"index_measure\": \"cosine_distance\",\n",
    "        \"enabled\": True,\n",
    "        \"ef_search\": 80\n",
    "    }\n",
    "}\n",
    "\n",
    "if search_strategy == \"query_fusion\":\n",
    "    # This is only relevant when using `hyde` or `rag-fusion`\n",
    "    # Number of hypothetical documents to generate, by default it's 5 if not specified\n",
    "    # https://r2r-docs.sciphi.ai/api-and-sdks/retrieval/rag-app\n",
    "    SEARCH_SETTINGS['num_sub_queries'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can modify this template as needed\n",
    "TEMPLATE: Final[str] = \"\"\"You are a helpful RAG assistant.\n",
    "Your task is to provide an answer given a question using the context.\n",
    "Please make sure the answer is complete and relevant to the question.\n",
    "\n",
    "**IMPORTANT:\n",
    "1. BASE YOUR ANSWER ONLY ON THE GIVEN CONTEXT.\n",
    "2. IF THE CONTEXT IS NOT ENOUGH TO ANSWER THE QUESTION, SAY THAT YOU CANNOT ANSWER BASED ON THE AVAILABLE INFORMATION.\n",
    "3. DO NOT INCLUDE CITATIONS OR REFERENCES TO SPECIFIC LINES OR PARTS OF THE CONTEXT.\n",
    "4. ALWAYS KEEP YOUR ANSWER RELEVANT AND FOCUSED ON THE QUESTION.\n",
    "5. DO NOT PROVIDE ANY ADDITIONAL INFORMATION EXCEPT THE ANSWER.\n",
    "**\n",
    "\n",
    "### CONTEXT:\n",
    "{context}\n",
    "\n",
    "### QUESTION:\n",
    "{query}\n",
    "\n",
    "### ANSWER:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# First we need to authenticate the admin user and receive a token\n",
    "# The username and password are the default ones provided by `R2R`\n",
    "# Note that you can overwrite those in the `config.toml` file\n",
    "# If this fails it means there're either connectivity issues or the credentials are wrong\n",
    "authetication: requests.Response = requests.post(\n",
    "    url=\"http://localhost:7272/v3/users/login\", # This may vary depending on your setup\n",
    "    headers={\n",
    "        \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
    "    },\n",
    "    data=\"username=admin@example.com&password=change_me_immediately\",\n",
    ")\n",
    "token: str = authetication.json()['results']['access_token']['token'] # Token for further authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevant only when working with `deepseek-r1`, since it produces `<think>` sections. It's part of its reasoning algorithm and then it produces the actual output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_deepseek_response(full_response: str):\n",
    "    \"\"\"\n",
    "    Extract the actual response from deepseek-r1 output by ignoring the <think>...</think> section.\n",
    "    \"\"\"\n",
    "    if \"</think>\" not in full_response:\n",
    "        raise ValueError(\"Response from deepseek-r1 is not full!\")\n",
    "\n",
    "    strings: List[str] = full_response.split(\"</think>\")\n",
    "    answer_without_section: str = strings[-1].lstrip()\n",
    "    return answer_without_section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some debugging information (Optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some debugging info\n",
    "print(f\"\"\"{'='*80}\\nGenerating dataset in ./datasets/{experiment_id}_dataset.jsonl\n",
    "TOP_K={int(os.getenv(\"TOP_K\"))}\n",
    "MAX_TOKENS_TO_SAMPLE={int(os.getenv(\"MAX_TOKENS\"))}\n",
    "CHUNK_SIZE={int(os.getenv(\"CHUNK_SIZE\"))}\n",
    "CHUNK_OVERLAP={int(os.getenv(\"CHUNK_OVERLAP\"))}\n",
    "CHAT_MODEL={os.getenv(\"CHAT_MODEL\")}\n",
    "TEMPERATURE={float(os.getenv(\"TEMPERATURE\"))}\n",
    "VANILLA_RAG={\"True\" if search_strategy == \"vanilla\"else \"False\"}\n",
    "{'='*80}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling out the rest of our dataset, for each individual entry\n",
    "for i, golden in enumerate(final_dataset.goldens):\n",
    "    # [1] Embed the `user_input`\n",
    "    # [2] Perform semantic similarity search fetching the top-k most relevant contexts\n",
    "    # [3] Re-rank based on relevance relative to `user_input`\n",
    "    # [4] Use the template defined above and replace placeholders dynamically\n",
    "    # [5] Submit the augmented prompt to the LLM\n",
    "    # [6] LLM generates the response and returns an object containing it and the context\n",
    "    rag_response: requests.Response = requests.post(\n",
    "        url=\"http://localhost:7272/v3/retrieval/rag\",\n",
    "        headers={\n",
    "            \"Authorization\": f\"Bearer {token}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        },\n",
    "        json={\n",
    "            \"query\": golden.input, # Submit query from synthetically generated goldens\n",
    "            \"rag_generation_config\": RAG_GENERATION_CONFIG,\n",
    "            \"search_mode\": \"custom\",\n",
    "            \"search_settings\": SEARCH_SETTINGS,\n",
    "            \"task_prompt\": TEMPLATE\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if rag_response.status_code != 200:\n",
    "        raise Exception(f\"Request failed {rag_response.json()}\")\n",
    "    \n",
    "    response: Dict = rag_response.json()['results']\n",
    "\n",
    "    # Get the LLM response and context\n",
    "    actual_output: str = response['completion']\n",
    "    retrieved_contexts: List[str] = [\n",
    "        chunk['text']\n",
    "        for chunk in response['search_results']['chunk_search_results']\n",
    "    ]\n",
    "\n",
    "    # If deepseek-r1 is used regardless of parameters count\n",
    "    # remove the content between the <think> tags\n",
    "    if \"deepseek-r1\" in os.getenv(\"CHAT_MODEL\"):\n",
    "        actual_output = extract_deepseek_response(actual_output)\n",
    "\n",
    "    # Fill out the rest of your dataset\n",
    "    golden.actual_output = actual_output\n",
    "    golden.retrieval_context = retrieved_contexts\n",
    "\n",
    "    print(f\"Added data to sample: {i + 1} out of {len(goldens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_out: str = f\"{experiment_id}_dataset.jsonl\"\n",
    "\n",
    "# Persist the complete dataset\n",
    "os.makedirs(\"./datasets\", exist_ok=True)  # Create the directory if it doesn't exist\n",
    "with open(file=f\"./datasets/{dataset_out}\", mode=\"w\", encoding=\"utf-8\") as f:\n",
    "    for golden in goldens:\n",
    "        f.write(json.dumps(golden.model_dump_json(), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"{'='*80}\\nGenerated dataset in ./datasets/{dataset_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having the full dataset, you can once more push it to **Confident AI** and replace the previous one, which was not full."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After having all of the data push the full dataset to ConfidentAI (Optional)\n",
    "\n",
    "final_dataset.push(\n",
    "    alias=alias,\n",
    "    overwrite=True,\n",
    "    auto_convert_test_cases_to_goldens=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
