{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a synthetic dataset using DeepEval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthesizer\n",
    "\n",
    "This object can be used to generate **Golden** instances, which consist out of **input**, **expected output** and **context**. It uses a LLM to come up with random input values based on a context and thereafter tries to enhance those, by making them more complex and realistic through evolutions.\n",
    "\n",
    "For a comprehensive guide on understanding how this object works please refer here: [Synthesizer](https://www.confident-ai.com/blog/the-definitive-guide-to-synthetic-data-generation-using-llms)\n",
    "\n",
    "### Summary\n",
    "\n",
    "I will try to summarize the most important information:\n",
    "\n",
    "* It uses a **LLM to come-up with a comprehensive dataset** much faster than a human can\n",
    "* The process starts with the LLM generating **synthetic queries** based on context from a knowledge base - usually documents\n",
    "* Those initial queries are then **evolved** to reflect real-life complexity and then together with the context can be used to generate a **target/expected output**\n",
    "\n",
    "![Dataset generation workflow](../../img/synthesizer-overview.png \"Synthetic generation\")\n",
    "\n",
    "* There exist two main methods:\n",
    "    - Self-improvement: Iteratively uses the LLMs output to generate more complex queries\n",
    "    - Distillation: A stronger model is being utilized \n",
    "\n",
    "* Constructing contexts:\n",
    "    - During this phase documents from the knowledge base are split using a token splitter\n",
    "    - A random chunk is selected\n",
    "    - Finally, additional chunks are retrieved based on **semantic similarity**, **knowledge graphs** or other approaches\n",
    "    - Ensuring that **chunk size**, **chunk overlap** or other similar parameters here and in the **retrieval component** of the **RAG** application are identical will yield better results\n",
    "\n",
    "![Constructing contexts](../../img/synthesizer-context.png \"Context construction\")\n",
    "\n",
    "* Constructing synthetic queries:\n",
    "    - In **RAG** when a user submits a query, all the relevant data is retrieved and then a template augments the input with the context. The `synthesizer` reverses the approach.\n",
    "    - Using the contexts the **Synthesizer** can now generate synthetic input\n",
    "    - Doing so we ensure that the input corresponds with the context enhancing the **relevancy** and **accuracy**\n",
    "\n",
    "![Constructing synthetic queries](../../img/synthesizer-query.png \"Synthetic queries creation\")\n",
    "\n",
    "* Data Filtering:\n",
    "\n",
    "    Data filtering is important after you have the `synthetic query`, `context` and optionally `reference answer` as to make sure one doesn't try to refine flawed queries and to waste valuable resources. Filtering occurs at 2 critical stages:\n",
    "\n",
    "    1. Context filtering: Removes low-quality chunks that may be unintelligible, due to whitespaces for example\n",
    "\n",
    "    ![Context filtering](../../img/synthesizer-context-filtering.png \"Filtering context\")\n",
    "\n",
    "    2. Input filtering: Ensures generated inputs meet quality standards. Sometimes even with good and well-structured context an input might be somewhat ambiguous or unclear based on the context.\n",
    "\n",
    "    ![Input filtering](../../img/synthesizer-query-filtering.png \"Filtering queries\")\n",
    "    \n",
    "* Customizing dataset generating:\n",
    "    - Depending on the scenario inputs and outputs can be tailored to specific use cases\n",
    "        - For example a medical chatbot would have a completely different behaviour than a scientific one. It would need to comfort patients and avoid bias. Also false negatives could turn out to be quite dangerous.\n",
    "    \n",
    "* Data Evolution:\n",
    "    This is crucial for the proper generation of a dataset, since it iteratively refines the dataset.\n",
    "\n",
    "    - **In-Depth Evolving**: Expands simple instructions into more detailed versions\n",
    "    - **In-Breadth Evolving**: Produces diverse instructions to enrich the dataset\n",
    "    - **Elimination Evolving**: Removes less effective instructions\n",
    "\n",
    "    ![Data evolution types](../../img/synthesizer-evolution.png \"Data Evolution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "* To install the dependencies run the `setup` bash script in the root of the `evaluation` folder.\n",
    "* Make sure you select the correct kernel in your notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: deepeval\n",
      "Version: 2.8.2\n",
      "Summary: The LLM Evaluation Framework\n",
      "Home-page: https://github.com/confident-ai/deepeval\n",
      "Author: Jeffrey Ip\n",
      "Author-email: jeffreyip@confident-ai.com\n",
      "License: Apache-2.0\n",
      "Location: /home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/eval/lib/python3.12/site-packages\n",
      "Requires: aiohttp, anthropic, black, coverage, google-genai, grpcio, nest_asyncio, ollama, openai, opentelemetry-api, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-sdk, portalocker, posthog, pytest, pytest-asyncio, pytest-repeat, pytest-rerunfailures, pytest-xdist, requests, rich, sentry-sdk, setuptools, tabulate, tenacity, tqdm, twine, typer, wheel\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# After installing the dependencies and selecting the kernel you should be good to go.\n",
    "# Make sure the package is installed before continuing further.\n",
    "! pip3 show deepeval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM provider\n",
    "\n",
    "**DeepEval** uses **OpenAI** by default as a LLM provider, however **Ollama** is also available. To use it execute the code cell below. This will generate a `.deepeval` file where key-value pairs will be stored about that particular LLM-provider like model name, base url and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🙌 Congratulations! You're now using a local Ollama model for all evals that \n",
      "require an LLM.\n",
      "🙌 Congratulations! You're now using Ollama embeddings for all evals that \n",
      "require text embeddings.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Final\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../../env/rag.env\")\n",
    "\n",
    "CHAT_MODEL: Final[str] = os.getenv(\"CHAT_MODEL\")\n",
    "EMBEDDING_MODEL: Final[str] = os.getenv(\"EMBEDDING_MODEL\")\n",
    "\n",
    "! deepeval set-ollama {CHAT_MODEL} --base-url=\"http://localhost:11434/\"\n",
    "! deepeval set-ollama-embeddings {EMBEDDING_MODEL} --base-url=\"http://localhost:11434\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting chunks from knowledge base to be used as context in data generation\n",
    "\n",
    "**Before executing the next cell:**\n",
    "* Make sure Ollama is up and running.\n",
    "* Download the required models for generation and embedding.\n",
    "* Make sure docker is up and running.\n",
    "* Activate the compose file in the root of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory already exists. Skipping download.\n",
      "Virtual environment already exists. Skipping creation.\n",
      "Dependencies already installed. Skipping installation.\n",
      "Environment is set and ready to be used\n",
      "================================================================================\n",
      "Generating context in /contexts/chunks_1.json.json.\n",
      "TOP_K=10\n",
      "MAX_TOKENS_TO_SAMPLE=1024\n",
      "CHUNK_SIZE=768\n",
      "CHUNK_OVERLAP=64\n",
      "CHAT_MODEL=llama3.1:8b-instruct-q4_1\n",
      "TEMPERATURE=0.0\n",
      "================================================================================\n",
      "\n",
      "DELETION STEP COMPLETED...\n",
      "data/special_assistance.md: Document created and ingested successfully.\n",
      "data/managing_reservations.md: Document created and ingested successfully.\n",
      "data/flight_delays.md: Document created and ingested successfully.\n",
      "data/baggage_policies.md: Document created and ingested successfully.\n",
      "data/inflight_services.md: Document created and ingested successfully.\n",
      "data/schedule_changes.md: Document created and ingested successfully.\n",
      "data/bookings.md: Document created and ingested successfully.\n",
      "data/flight_cancellations.md: Document created and ingested successfully.\n",
      "INGESTION STEP COMPLETED...\n",
      "Extracted context 1 from document 1 and chunk 1\n",
      "Extracted context 2 from document 1 and chunk 2\n",
      "Extracted context 3 from document 1 and chunk 3\n",
      "Extracted context 4 from document 2 and chunk 1\n",
      "Extracted context 5 from document 2 and chunk 2\n",
      "Extracted context 6 from document 2 and chunk 3\n",
      "Extracted context 7 from document 3 and chunk 1\n",
      "Extracted context 8 from document 3 and chunk 2\n",
      "Extracted context 9 from document 3 and chunk 3\n",
      "Extracted context 10 from document 4 and chunk 1\n",
      "Extracted context 11 from document 4 and chunk 2\n",
      "Extracted context 12 from document 4 and chunk 3\n",
      "Extracted context 13 from document 5 and chunk 1\n",
      "Extracted context 14 from document 5 and chunk 2\n",
      "Extracted context 15 from document 5 and chunk 3\n",
      "Extracted context 16 from document 6 and chunk 1\n",
      "Extracted context 17 from document 6 and chunk 2\n",
      "Extracted context 18 from document 6 and chunk 3\n",
      "Extracted context 19 from document 7 and chunk 1\n",
      "Extracted context 20 from document 7 and chunk 2\n",
      "Extracted context 21 from document 7 and chunk 3\n",
      "Extracted context 22 from document 8 and chunk 1\n",
      "Extracted context 23 from document 8 and chunk 2\n",
      "Extracted context 24 from document 8 and chunk 3\n",
      "EXTRACTION STEP COMPLETED...\n",
      "SAVED TO JSON FILE...\n",
      "Data extracted and saved to /contexts/chunks_1.json.json\n"
     ]
    }
   ],
   "source": [
    "chunks_out: str = input(\"Enter a name for the output file (no extension): \")\n",
    "\n",
    "! chmod u+x ./extract_chunks.sh\n",
    "! ./extract_chunks.sh {chunks_out}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtration config** serves as a way to configure the quality of the generated synthetic input queries. Having higher threshold would ensure that the input queries are of higher quality.\n",
    "\n",
    "If the **quality_score** is still lower than the **synthetic_input_quality_threshold** after **max_quality_retries**, the **golden with the highest quality_score** will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.synthesizer.config import FiltrationConfig\n",
    "\n",
    "# (This step is completely OPTIONAL)\n",
    "# https://www.deepeval.com/docs/synthesizer-introduction\n",
    "filtration_config = FiltrationConfig(\n",
    "    synthetic_input_quality_threshold=0.7,\n",
    "    max_quality_retries=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evolutions** are used to specify the type of approach to use when trying to complicate the synthetic queries. Since this is a **RAG** application I will only use the evolution types which use **context**. The `num_evolutions` parameter can be configured to specify the number of iterations for performing those evolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.synthesizer.config import (\n",
    "    Evolution,\n",
    "    EvolutionConfig,\n",
    ")\n",
    "\n",
    "# (This step is completely OPTIONAL)\n",
    "# https://www.deepeval.com/docs/synthesizer-introduction\n",
    "evolution_config = EvolutionConfig(\n",
    "    num_evolutions=1,\n",
    "    evolutions={\n",
    "        Evolution.MULTICONTEXT: 0.25,\n",
    "        Evolution.CONCRETIZING: 0.25,\n",
    "        Evolution.CONSTRAINED: 0.25,\n",
    "        Evolution.COMPARATIVE: 0.25,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthesizer\n",
    "\n",
    "The synthesizer object as explained at the beginning of the notebook can be used to generate the synthetic dataset. It provides four different methods for this current version of **DeepEval**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.synthesizer import Synthesizer\n",
    "\n",
    "# https://www.deepeval.com/docs/synthesizer-introduction\n",
    "synthesizer = Synthesizer(\n",
    "    filtration_config=filtration_config,\n",
    "    evolution_config=evolution_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the contexts for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load all the contexts that were previously generated\n",
    "with open(file=f\"./contexts/{chunks_out}.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    context_chunks = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the goldens\n",
    "\n",
    "In this notebook I use the `generate_goldens_from_contexts`, which actually skips some steps that are specified in the synthesizer section - the loading and splitting of documents. This provides more freedom, however one has to be careful to properly ingest the documents and to derive high-quality contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "✨ Generating up to 48 goldens using DeepEval (using llama3.1:8b-instruct-q4_1 (Ollama), method=default):  42%|████▏     | 20/48 [40:09<23:10, 49.67s/it]    "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepeval\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgolden\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Golden\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m goldens: \u001b[38;5;28mlist\u001b[39m[Golden] = \u001b[43msynthesizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_goldens_from_contexts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext_chunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_expected_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_goldens_per_context\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/eval/lib/python3.12/site-packages/deepeval/synthesizer/synthesizer.py:279\u001b[39m, in \u001b[36mSynthesizer.generate_goldens_from_contexts\u001b[39m\u001b[34m(self, contexts, include_expected_output, max_goldens_per_context, source_files, _context_scores, _progress_bar, _send_data, _reset_cost)\u001b[39m\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_mode:\n\u001b[32m    277\u001b[39m     loop = get_or_create_event_loop()\n\u001b[32m    278\u001b[39m     goldens.extend(\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m         \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43ma_generate_goldens_from_contexts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m                \u001b[49m\u001b[43minclude_expected_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_expected_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmax_goldens_per_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_goldens_per_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m                \u001b[49m\u001b[43msource_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m     )\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    289\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m synthesizer_progress_context(\n\u001b[32m    290\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    291\u001b[39m         num_evolutions=\u001b[38;5;28mself\u001b[39m.evolution_config.num_evolutions,\n\u001b[32m   (...)\u001b[39m\u001b[32m    297\u001b[39m         async_mode=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    298\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m progress_bar:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/eval/lib/python3.12/site-packages/nest_asyncio.py:92\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     90\u001b[39m     f._log_destroy_pending = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/eval/lib/python3.12/site-packages/nest_asyncio.py:115\u001b[39m, in \u001b[36m_patch_loop.<locals>._run_once\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m     heappop(scheduled)\n\u001b[32m    110\u001b[39m timeout = (\n\u001b[32m    111\u001b[39m     \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[32m    113\u001b[39m         scheduled[\u001b[32m0\u001b[39m]._when - \u001b[38;5;28mself\u001b[39m.time(), \u001b[32m0\u001b[39m), \u001b[32m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._process_events(event_list)\n\u001b[32m    118\u001b[39m end_time = \u001b[38;5;28mself\u001b[39m.time() + \u001b[38;5;28mself\u001b[39m._clock_resolution\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/selectors.py:468\u001b[39m, in \u001b[36mEpollSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    466\u001b[39m ready = []\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    470\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "✨ Generating up to 48 goldens using DeepEval (using llama3.1:8b-instruct-q4_1 (Ollama), method=default):  52%|█████▏    | 25/48 [41:34<38:15, 99.80s/it]\n"
     ]
    }
   ],
   "source": [
    "from deepeval.dataset.golden import Golden\n",
    "\n",
    "goldens: list[Golden] = synthesizer.generate_goldens_from_contexts(\n",
    "    contexts=context_chunks,\n",
    "    include_expected_output=True,\n",
    "    max_goldens_per_context=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confident AI\n",
    "\n",
    "1. In short **Confident AI** is a cloud-based platform part of the **DeepEval** framework, which stores **datasets**, **evaluations** and **monitoring data**. \n",
    "\n",
    "2. If you want to use **Confident AI** platform create an account from here: [Confident AI](https://www.confident-ai.com/)\n",
    "\n",
    "3. After signing-up an **API key** will be generated, which can be used to interact with the platform from inside the notebook.\n",
    "\n",
    "---\n",
    "\n",
    "Example of .env file:\n",
    "```bash\n",
    "DEEPEVAL_RESULTS_FOLDER=<folder> # Results of evaluations can be saved locally (cache)\n",
    "DEEPEVAL_API_KEY=<your api key>  # Relevant if you want to use Confident AI\n",
    "DEEPEVAL_TELEMETRY_OPT_OUT=\"YES\" # Remove telemetry\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🎉🥳 Congratulations! You've successfully logged in! 🙌 \n",
       "</pre>\n"
      ],
      "text/plain": [
       "🎉🥳 Congratulations! You've successfully logged in! 🙌 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from deepeval import login_with_confident_api_key\n",
    "\n",
    "# Loads the environment variables from a `.env` file.\n",
    "# If you want to use Confident AI be sure to create one in this directory.\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "deepeval_api_key: str = os.getenv(\"DEEPEVAL_API_KEY\")\n",
    "\n",
    "# You should get a message letting you know you are logged-in.\n",
    "login_with_confident_api_key(deepeval_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to visit the link provided, upon invoking the `push` method. This will redirect you to the page containing the `goldens`. Then you can clean-up the data and that would almost always be mandatory, since we are using a weak model in the project and the input will not always be **clean**.\n",
    "\n",
    "Do note that if the `push` to the cloud fails you might need to upgrade **DeepEval** to the latest version. To do so run:\n",
    "`pip3 install --upgrade deepeval`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Dataset successfully pushed to Confident AI! View at \n",
       "<a href=\"https://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/datasets/cma2e7zy50rj4vkijvxke9jra\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/datasets/cma2e7zy50rj4vkijvxke9jra</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Dataset successfully pushed to Confident AI! View at \n",
       "\u001b]8;id=648580;https://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/datasets/cma2e7zy50rj4vkijvxke9jra\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/datasets/cma2e7zy50rj4vkijvxke9jra\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gtk-Message: 12:57:34.499: Failed to load module \"canberra-gtk-module\"\n",
      "Gtk-Message: 12:57:34.500: Failed to load module \"canberra-gtk-module\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening in existing browser session.\n"
     ]
    }
   ],
   "source": [
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "dataset = EvaluationDataset(goldens=goldens)\n",
    "\n",
    "dataset.push(\n",
    "    alias=os.getenv(\"DATASET_ALIAS\"),\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "269be0f223ea4a9aa4d8412f80ae01fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "# I did some cleaning on the data since the input was not fully in the expected format on the ConfidentAI platform.\n",
    "final_dataset = EvaluationDataset()\n",
    "final_dataset.pull(alias=os.getenv(\"DATASET_ALIAS\"))\n",
    "\n",
    "# Saving the data locally so I can use it in a script.\n",
    "# Since R2R and DeepEval have conflicting dependencies a virtual environment with both of these \n",
    "# libraries doesn't work. They need to be separated. (`Ollama` is the conflicting package)\n",
    "json_out: list[dict] = []\n",
    "for golden in final_dataset.goldens:\n",
    "    json_out.append(golden.model_dump())\n",
    "\n",
    "# Save json data\n",
    "with open(\"deepeval_dataset.json\", \"w\") as f:\n",
    "    json.dump(json_out, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of `actual response` and `retrieval context`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "goldens_filepath: str = input(\"Enter a name for the goldens (no extension): \")\n",
    "\n",
    "! chmod u+x ./fill_dataset.sh\n",
    "! ./fill_dataset.sh {goldens_filepath}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Dataset successfully pushed to Confident AI! View at \n",
       "<a href=\"https://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/datasets/cma40qrhu025kdfxvjjyc4a0c\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/datasets/cma40qrhu025kdfxvjjyc4a0c</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Dataset successfully pushed to Confident AI! View at \n",
       "\u001b]8;id=809503;https://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/datasets/cma40qrhu025kdfxvjjyc4a0c\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/datasets/cma40qrhu025kdfxvjjyc4a0c\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gtk-Message: 16:15:48.222: Failed to load module \"canberra-gtk-module\"\n",
      "Gtk-Message: 16:15:48.223: Failed to load module \"canberra-gtk-module\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening in existing browser session.\n"
     ]
    }
   ],
   "source": [
    "# After having all of the data push the full dataset to ConfidentAI\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "# Make sure you specify the correct name below\n",
    "with open(\"full_deepeval_dataset.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "full_dataset = EvaluationDataset()\n",
    "full_dataset.add_goldens_from_json_file(\n",
    "    file_path=\"full_deepeval_dataset.json\",\n",
    "    input_key_name=\"input\",\n",
    "    actual_output_key_name=\"actual_output\",\n",
    "    expected_output_key_name=\"expected_output\",\n",
    "    context_key_name=\"context\",\n",
    "    retrieval_context_key_name=\"retrieval_context\"\n",
    ")\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "full_dataset.push(\n",
    "    alias=os.getenv(\"DATASET_ALIAS\"),\n",
    "    overwrite=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
