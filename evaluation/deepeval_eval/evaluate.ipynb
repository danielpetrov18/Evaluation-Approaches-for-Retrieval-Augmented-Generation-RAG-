{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confident AI (Optional)\n",
    "\n",
    "1. In short **Confident AI** is a cloud-based platform fully compatible with the **DeepEval** framework, which can store generated **datasets**, results of **evaluations**, etc. \n",
    "\n",
    "2. If you want to use **Confident AI** platform create an account from here: [Confident AI](https://www.confident-ai.com/)\n",
    "\n",
    "3. After signing-up an **API key** will be generated, which can be used to interact with the platform from inside the notebook.\n",
    "\n",
    "---\n",
    "\n",
    "Example of `.env` file:\n",
    "```bash\n",
    "DEEPEVAL_RESULTS_FOLDER=<folder> # Results of evaluations can be saved locally\n",
    "DEEPEVAL_API_KEY=<your api key>  # Relevant if you want to use Confident AI\n",
    "DEEPEVAL_TELEMETRY_OPT_OUT=\"YES\" # Remove telemetry\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import (\n",
    "    Final, List, Dict, Any, Union\n",
    ")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import (\n",
    "    AnswerRelevancyMetric,\n",
    "    FaithfulnessMetric,\n",
    "    ContextualPrecisionMetric,\n",
    "    ContextualRecallMetric,\n",
    "    ContextualRelevancyMetric    \n",
    ")\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "from deepeval import login_with_confident_api_key\n",
    "from deepeval.evaluate.configs import (\n",
    "    AsyncConfig, DisplayConfig, TestRunResultDisplay\n",
    ")\n",
    "from deepeval.evaluate.evaluate import EvaluationResult\n",
    "\n",
    "# Custom prompts for the DeepEval metrics\n",
    "from prompts.custom_faithfulness_prompt import MyFaithfulnessTemplate\n",
    "from prompts.custom_answer_relevancy_prompt import MyAnswerRelevancyTemplate\n",
    "from prompts.custom_contextual_recall_prompt import MyContextualRecallTemplate\n",
    "from prompts.custom_contextual_precision_prompt import MyContextualPrecisionTemplate\n",
    "from prompts.custom_contextual_relevancy_prompt import MyContextualRelevancyTemplate\n",
    "\n",
    "# Loads the environment variables from a `.env` file.\n",
    "# If you want to use Confident AI be sure to create one parent directory with the variables.\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# Loads all the RAG parameters like chunk size, chunk overlap, temperature, etc.\n",
    "load_dotenv(\"../../env/rag.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional step, if you want to use the cloud platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.getenv(\"DEEPEVAL_API_KEY\"):\n",
    "    deepeval_api_key: str = os.getenv(\"DEEPEVAL_API_KEY\")\n",
    "    \n",
    "    # You should get a message letting you know you are logged-in.\n",
    "    login_with_confident_api_key(deepeval_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMTestCase\n",
    "\n",
    "- Unlike **RAGAs**, where a single interaction between a user and the AI system is represented by a **SingleTurnSample** in **DeepEval** there's the concept of so called **LLMTestCase**.\n",
    "\n",
    "- Similarly to **RAGAs**, **LLMTestCase** objects have the same fields just different names - input, actual_output, expected_output, etc.\n",
    "One can very easily generate a synthetic dataset with one framework, convert it into the proper format valid for the other framework and use it.\n",
    "\n",
    "![Image showcasing what a LLMTestCase is.](../../img/deepeval/LLM-testcase.png \"LLMTestCase\")\n",
    "\n",
    "- Put simply a **LLMTestCase** represents a **single, atomic unit of interaction with your LLM app**.\n",
    "- An **interaction** can mean different things depending on the application being evaluated and the scope.\n",
    "- If one evaluates an **AI Agent** an **interaction** can mean:\n",
    "    - **Agent Level**: The entire process initiated by the agent including all intermediary steps\n",
    "    - **RAG pipeline level**: Just the **RAG** pipeline - **retriever** + **generator**\n",
    "    - Individual components level:\n",
    "        - **Retriever**: Retrieving relevant chunks and ranking them accordingly\n",
    "        - **Generator**: Generating relevant, complete answer free of hallucinations\n",
    "\n",
    "![Image showcasing what an interaction means](../../img/deepeval/llm-interaction.png \"LLMTestCase interaction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "**Evaluation** should be a crucial component of every single application which uses **AI**. **DeepEval** provides more than 30 metrics for evaluation so that one can very easily iterate towards a better LLM application. Each default metric is evaluated by a LLM (`LLM-as-a-judge`). \n",
    "\n",
    "Optionally, one can use the [GEval](https://www.deepeval.com/docs/metrics-llm-evals) to set a custom criteria for evaluation if neither of the other metrics meet the requirements. \n",
    "\n",
    "Alternatively, there's the [DAG](https://www.deepeval.com/docs/metrics-dag), whose purpose is similar to the **GEval**, however it uses a graph and it's fully **deterministic**.\n",
    "\n",
    "When evaluating a test case, multiple metrics can be used and the test would be **positive** **iff** all the **metrics thresholds** have been exceeded and **negative** in any other case. \n",
    "\n",
    "Evaluation workflow:\n",
    "\n",
    "![Image showcasing the evaluation steps](../../img/deepeval/evaluation-workflow.png \"Evaluation workflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Dataset\n",
    "\n",
    "An **evaluation dataset** is just a collection of **LLMTestCase**- or so called **Golden** objects. A **Golden** is structurally the same as a **LLMTestCase**, however it has no `actual_output` and `retrieval_context` fields, which can be generated by your LLM at evaluation time.\n",
    "\n",
    "Datasets can be **pushed**, **stored** and **pulled** from **Confident AIs** platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM provider\n",
    "\n",
    "**DeepEval** uses **OpenAI** by default as a LLM provider, however **Ollama** is also available. To use it execute the code cell below. This will generate a `.deepeval` file where key-value pairs will be stored about that particular LLM-provider like model name, base url and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_MODEL: Final[str] = os.getenv(\"EVALUATION_MODEL\")\n",
    "EMBEDDING_MODEL: Final[str] = os.getenv(\"EMBEDDING_MODEL\")\n",
    "\n",
    "print(f\"Using [{EVALUATION_MODEL}] as evaluation model\")\n",
    "print(f\"Using [{EMBEDDING_MODEL}] as embedding model\")\n",
    "\n",
    "# If you want to use a custom model be sure to configure this properly.\n",
    "# https://www.deepeval.com/integrations/models/ollama\n",
    "! deepeval set-ollama {EVALUATION_MODEL} --base-url=\"http://localhost:11434/\"\n",
    "! deepeval set-ollama-embeddings {EMBEDDING_MODEL} --base-url=\"http://localhost:11434\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset\n",
    "\n",
    "* If you already have a dataset on the platform just use the `pull` method and specify the name/alias.\n",
    "* Alternatively, you can save your synthetic dataset on disk and import it from a `json` or `csv` file.\n",
    "* Note also that if your dataset consists of **goldens** you need to generate the `actual_output` and `retrieval_context` first.\n",
    "\n",
    "For this project I use **RAGAs** for synthetic testdata generation and the full datasets are stored under: `../ragas_eval/datasets` and can be loaded from `jsonl` files. You can also use datasets created by **DeepEval**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The datasets are located under `../ragas_eval/datasets`\n",
    "# If you want, you can use something else.\n",
    "experiment_id: int = int(input(\"Please specify experiment_id (Ex. 1): \"))\n",
    "\n",
    "ragas_samples: List[Dict[str, Any]] = []\n",
    "try:\n",
    "    # Your filepath may vary\n",
    "    # Alternatively, you can pull a dataset from ConfidentAI\n",
    "    with open(\n",
    "        file=f\"../datasets/{experiment_id}_dataset.jsonl\",\n",
    "        mode=\"r\",\n",
    "        encoding=\"utf-8\"\n",
    "    ) as file:\n",
    "        for line in file:\n",
    "            if line.strip():  # Skip empty lines\n",
    "                ragas_samples.append(json.loads(line))\n",
    "\n",
    "    # Convert from JSON to LLMTestCase.\n",
    "    # Have in mind that RAGAs and DeepEval use similar names for the same parameters.\n",
    "    # We are mapping the RAGAs parameters to DeepEval ones.\n",
    "    test_cases: List[LLMTestCase] = []\n",
    "    for ragas_sample in ragas_samples:\n",
    "        test_case = LLMTestCase(\n",
    "            input=ragas_sample[\"user_input\"],\n",
    "            actual_output=ragas_sample[\"response\"],\n",
    "            expected_output=ragas_sample[\"reference\"],\n",
    "            retrieval_context=ragas_sample[\"retrieved_contexts\"],\n",
    "            context=ragas_sample[\"reference_contexts\"],\n",
    "        )\n",
    "        test_cases.append(test_case)\n",
    "    \n",
    "    # The fully loaded and ready for evaluation dataset \n",
    "    evaluation_dataset = EvaluationDataset(test_cases=test_cases)\n",
    "except FileNotFoundError:\n",
    "    raise Exception(\n",
    "        f\"File: `../datasets/{experiment_id}_dataset.jsonl` containing test cases not found!\"\n",
    "    )\n",
    "except json.JSONDecodeError as e:\n",
    "    raise Exception (\n",
    "        f\"Error parsing JSONL file: {str(e)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Triad\n",
    "\n",
    "The RAG triad is composed of three metrics:\n",
    "\n",
    "- **Answer Relevancy**\n",
    "    - This particular metric can assess the `prompt template` submitted to the LLM after the retrieval phase.\n",
    "    - If the score is insufficient it might be worth playing around with the template itself - try to be more detailed by providing more fine-granular instructions, be more explicit or provide few-shot-examples.\n",
    "\n",
    "- **Faithfulness**\n",
    "    - This metric evaluates the `LLM` and its ability to incorporate context into the generation process.\n",
    "    - Low scores would usually signify a bad `LLM model` that **hallucinates** or fails to use the context properly. Alternatively, it might be due to noise in the context retrieved (large `chunk sizes` or high `top-k`)\n",
    "\n",
    "- **Contextual Relevancy**\n",
    "    - This metric verifies if the `retrieved context` is relevant for providing a good/complete answer to the query.\n",
    "    \n",
    "    - It measures the `top-k`, `chunk_size` and `embedding model`.\n",
    "        - The `embedding model` would be useful to capture relevant chunks of context and detect different nuances.\n",
    "        - `top-k` and `chunk-size` will either help or hinder the context retrieval by including or missing important information.\n",
    "\n",
    "If a given RAG application scores high on all 3 metrics, one can be confident that the optimal `hyperparameters` are being used. `Hyperparamaters` are parameters, which can either positively or negatively influence the RAG pipeline. For example a good `embedding model` would be helpful for retrieving relevant data and also be able to pick-up on nuances, whereas a bad one wouldn't. \n",
    "\n",
    "If you have a simple RAG application that doesn't use `tool calls` or doesn't employ an agent then those 3 metrics can be a good starting point.\n",
    "\n",
    "![RAG Triad](../../img/deepeval/RAG-triad.png  \"RAG Triad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Answer Relevancy**\n",
    "\n",
    "It uses the **LLM as a judge** to determine how well the response answers the user input.\n",
    "\n",
    "`Is the answer complete, on-topic and concise?`\n",
    "\n",
    "This metric is especially relevant when it comes to testing the **RAG pipelines generator** by determining the degree of relevance of the generated output with respect to the query. \n",
    "\n",
    "The main **hyperparameter** being tested by this metric is the `prompt template` that the **LLM** receives as instruction to create an output (can it instruct the LLM to output relevant and helpful answers based on the `retrieval_context`). Tweaking the template submitted to the LLM can provide higher scores.\n",
    "\n",
    "---\n",
    "\n",
    "### Approach:\n",
    "\n",
    "The LLM is used to decompose the `actual_output` into claims and then each of those claims get classified with respect to the `input` by the LLM:\n",
    "\n",
    "* `yes` if relevant\n",
    "\n",
    "* `no` if irrelevant\n",
    "\n",
    "* `idk` if non-determined or partially relevant.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula:\n",
    "\n",
    "* Number of relevant statements = Statements marked as `yes` or `idk`\n",
    "\n",
    "* **Answer Relevancy** = $\\frac{\\text{Number of relevant statements}}{\\text{Total number of statements}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_relevancy = AnswerRelevancyMetric(\n",
    "    include_reason=False,\n",
    "    evaluation_template=MyAnswerRelevancyTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Faithfulness**\n",
    "\n",
    "The **Faithfulness** metric measures how factually consistent a response is with the retrieval context. \n",
    "It verifies, whether or not the LLM can use the retrieved context properly and not hallucinate or contradict it.\n",
    "\n",
    "Output that contains hallucinations (some sort of contradiction or made up information) are penalized. The score ranges between 0 to 1, with higher scores indicating better factual consistency.\n",
    "\n",
    "The metric tests the `generator` component in the RAG pipeline and tries to verify if the output contradicts factual information from the **retrieval_context**.\n",
    "\n",
    "A response is considered faithful if its claims can be supported by the retrieved context (ideally we would want a response whose claims/statements are all supported by a node/chunk in the context).\n",
    "\n",
    "---\n",
    "\n",
    "### Approach:\n",
    "\n",
    "1. **Identify Claims**:\n",
    "   - Break down the response into individual statements\n",
    "\n",
    "2. **Identify Truths**:\n",
    "   - Break down the retrieval context into individual statements\n",
    "   - One can configure the number of `truths` to be extracted using the `truths_extraction_limit` parameter\n",
    "\n",
    "3. **Use LLM**\n",
    "    - Using the LLM to determine if all claims in the response are truthful (do not contradict information in the retrieval context and do not introduce new information outside the context)\n",
    "\n",
    "---\n",
    "\n",
    "### Formula:\n",
    "\n",
    "* Number of truthful claims = Statements/claims from the response marked as `yes` or `idk` **(not contradicting a truth or introduce information outside the context)**\n",
    "\n",
    "* **Faithfulness** = $\\frac{\\text{Number of truthful claims}}{\\text{Total number of claims}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness = FaithfulnessMetric(\n",
    "    include_reason=False,\n",
    "    evaluation_template=MyFaithfulnessTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Contextual Precision**\n",
    "\n",
    "The **Contextual Precision** metric measures the the **re-rankers capability to rank relevant** `nodes` higher than irrelevant ones. This ensures that important information is in the beginning of the context, which may lead to better generation.\n",
    "\n",
    "Usually a LLM focuses more on information which is found at the beginning of the context. Information which is ranked higher than it is supposed to, leads to penalties (lower score).\n",
    "\n",
    "---\n",
    "\n",
    "### Approach:\n",
    "\n",
    "The **LLM** determines for each **piece of context** whether or not it's relevant for answering the **input** with respect to the **expected output**. Then the `precision@k` is computed. Finally, it all gets averaged out using the WCP formula.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula:\n",
    "\n",
    "* **Note that sometimes not every node, which was retrieved would be relevant**\n",
    "* **k** is the rank (position of retrieved node context)\n",
    "* **n** is the total number of retrieved nodes\n",
    "* **$r_{k}$** $\\in \\{0, 1\\} $ the relevance indicator (1 if the node is relevant, 0 otherwise)\n",
    "* **Contextual Precision** = $ \\frac{1}{\\text{Number of relevant nodes}} \\times \\sum_{k=1}^{n} (\\frac{\\text{Number of relevant Nodes up to Rank k}}{\\text{k}} \\times r_{k}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_precision = ContextualPrecisionMetric(\n",
    "    include_reason=False,\n",
    "    evaluation_template=MyContextualPrecisionTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Contextual Recall**\n",
    "\n",
    "The **Contextual Recall** metric measures the extent to which the `retrieval_context` aligns with the `expected_output`.\n",
    "\n",
    "This metric penalizes a **retriever** which misses important/relevant nodes during retrieval, since that would lead to an incomplete response. It uses the `expected output` as a reference for the `retrieval context`.\n",
    "\n",
    "The main `hyperparameter` which is evaluated by the metric is the `embedding model`. Since the `embedding model` is used by the retriever when performing semantic similarity search a good model can positively affect the application. The inverse is also true.\n",
    "\n",
    "---\n",
    "\n",
    "### Approach:\n",
    "\n",
    "The **LLM** determines for each statement in the `expected_output` whether or not it can be attributed to a node from the `retrieval context`.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula:\n",
    "\n",
    "**Attributable statement** is one, which contains information present in a node of the context. \n",
    "* **Contextual Recall** = $ \\frac{\\text{Number of attributable statements}}{\\text{Total number of statements}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_recall = ContextualRecallMetric(\n",
    "    include_reason=False,\n",
    "    evaluation_template=MyContextualRecallTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Contextual Relevancy**\n",
    "\n",
    "The **Contextual Relevancy** metric measures the extent to which the `retrieval_context` is useful for generating a proper response relative to the **input**.\n",
    "\n",
    "The hyperparameters which are measured in this case are the `chunk size` and `top-k`.\n",
    "\n",
    "In the case that `chunk size` is large one might fetch various nodes that contain the required information, however also redundant information causing the LLM to hallucinate. The other parameter `top-k` plays an important role in limiting the number of nodes to consider as context for the generation phase.\n",
    "\n",
    "---\n",
    "\n",
    "### Approach:\n",
    "\n",
    "The **LLM** receives the **input** and **retrieval_context** and is prompted to decide for each statement found in the context, whether or not it can be of use for answering the input.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula:\n",
    "\n",
    "* **Relevant statement** is a statement found in the context, which can be utilized to answer the user input.\n",
    "\n",
    "* **Contextual Relevancy** = $ \\frac{\\text{Number of relevant statements}}{\\text{Total number of statements}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_relevancy = ContextualRelevancyMetric(\n",
    "    include_reason=False,\n",
    "    evaluation_template=MyContextualRelevancyTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configs for the fine-granular customization of the evaluation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.deepeval.com/docs/evaluation-flags-and-configs\n",
    "async_conf = AsyncConfig(\n",
    "    run_async=True,\n",
    ")\n",
    "\n",
    "display_conf = DisplayConfig(\n",
    "    show_indicator=True,\n",
    "    print_results=True,\n",
    "    verbose_mode=False,\n",
    "    # Displays only the failed tests\n",
    "    # The ones whose score didn't exceed the threshold\n",
    "    display_option=TestRunResultDisplay.FAILING\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results: EvaluationResult = evaluate(\n",
    "    test_cases=evaluation_dataset.test_cases,\n",
    "    metrics=[\n",
    "        answer_relevancy,\n",
    "        faithfulness,\n",
    "        contextual_precision,\n",
    "        contextual_recall,\n",
    "        contextual_relevancy,\n",
    "    ],\n",
    "    #hyperparameters={},\n",
    "    identifier=f\"{experiment_id}_experiment\",\n",
    "    async_config=async_conf,\n",
    "    display_config=display_conf,\n",
    "    # cache_config=cache_conf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the scores \n",
    "scores: Dict[str, List[Dict[str, Union[str, float]]]] = {} # Metric name -> list of test case name and score\n",
    "\n",
    "for test_result in results.test_results: # Iterate over the test cases\n",
    "    for metric in test_result.metrics_data: # Iterate over the metrics\n",
    "        # Get the test case number\n",
    "        test_case: int = int(test_result.name.split(\"_\")[-1]) # Make sure to convert to int, so we can sort it properly\n",
    "        if metric.name in scores:\n",
    "            scores[metric.name].append({ \"test_case\": test_case, \"score\": metric.score })\n",
    "        else:\n",
    "            scores[metric.name] = [{ \"test_case\": test_case, \"score\": metric.score }]\n",
    "            \n",
    "# Sort properly, since DeepEval doesn't evaluate in the original order\n",
    "for metric_name in scores:\n",
    "    scores[metric_name] = sorted(scores[metric_name], key=lambda x: x[\"test_case\"])\n",
    "    \n",
    "# Store on disk\n",
    "with open(f\"./res/{experiment_id}_eval.jsonl\", \"w\") as f:\n",
    "    for metric_name, metric_scores in scores.items():\n",
    "        record = {\n",
    "            \"metric\": metric_name,\n",
    "            \"scores\": metric_scores  # This is the sorted list of {\"test_case\": ..., \"score\": ...}\n",
    "        }\n",
    "        f.write(json.dumps(record) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
