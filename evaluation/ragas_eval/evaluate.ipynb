{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_ollama import (\n",
    "    ChatOllama, OllamaEmbeddings\n",
    ")\n",
    "\n",
    "from ragas import (\n",
    "    RunConfig, DiskCacheBackend\n",
    ")\n",
    "from ragas.metrics import (\n",
    "    LLMContextPrecisionWithReference,\n",
    "    LLMContextRecall,\n",
    "    ContextEntityRecall,\n",
    "    NoiseSensitivity,\n",
    "    ResponseRelevancy,\n",
    "    Faithfulness,\n",
    "    FactualCorrectness,\n",
    "    SemanticSimilarity # Non-LLM based one\n",
    ")\n",
    "from ragas import EvaluationDataset\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.evaluation import evaluate, EvaluationResult\n",
    "\n",
    "from prompts.metrics.custom_context_recall_prompt import MyContextRecallPrompt\n",
    "from prompts.metrics.custom_context_precision_prompt import MyContextPrecisionPrompt\n",
    "from prompts.metrics.custom_response_relevance_prompt import MyResponseRelevancePrompt\n",
    "from prompts.metrics.custom_context_entities_recall_prompt import MyContextEntitiesRecallPrompt\n",
    "from prompts.metrics.faithfulness.custom_nli_generator_prompt import MyNLIStatementPrompt\n",
    "from prompts.metrics.faithfulness.custom_statement_generator_prompt import MyStatementGeneratorPrompt\n",
    "\n",
    "# Load RAG parameters\n",
    "load_dotenv(\"../../env/rag.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the evaluation dataset\n",
    "\n",
    "- https://docs.ragas.io/en/latest/concepts/components/eval_sample/\n",
    "- https://docs.ragas.io/en/latest/concepts/components/eval_dataset/\n",
    "\n",
    "According to **RAGAs** an **evaluation dataset** is a homogeneous collection of **data samples** with the sole purpose of measuring the capabilities and performance of an AI application.\n",
    "\n",
    "- **Structure**:\n",
    "    - Contains either **SingleTurnSample** or **MultiTurnSample** object instances, representing a unique interaction or a set of interactions respectively between a **Persona** and the AI-system.\n",
    "    - **NOTE**: The dataset can contain **ONLY** a single type of samples. They cannot be mixed together into a single dataset.\n",
    "\n",
    "**Samples** represent a single unit of interaction with the underlying system. As mentioned they can be either **SingleTurnSample** or **MultiTurnSample**.\n",
    "\n",
    "For this project I focus solely on **SingleTurnSample** objects, since I'm evaluating independent queries, not multi-turn interactions.\n",
    "\n",
    "If you have your own custom synthetic data load it as required. Make sure you convert your entries into an `EvaluationDataset`.\n",
    "\n",
    "The schema that you need to respect is:\n",
    "```bash\n",
    "{\n",
    "    \"user_input\": \"...\", # query/question\n",
    "    \"reference\": \"...\",  # expected answer\n",
    "    \"response\": \"...\",   # actual answer (LLM output)\n",
    "    # (optional and not relevant, however generated by RAGAs during synthetic data creation) \n",
    "    \"reference_contexts\": [...], # context used to generate queries\n",
    "    \"retrieved_contexts\": [...]  # context retrieved at runtime to answer a question\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The datasets are located at ../datasets\n",
    "# Specify the experiment id to load the corresponding dataset\n",
    "test_id: str = input(\"Please specify experiment id (Ex. 1): \")\n",
    "\n",
    "try:\n",
    "    evaluation_dataset = EvaluationDataset.from_jsonl(\n",
    "        path=f\"../datasets/{test_id}_dataset.jsonl\"\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    print(f\"Dataset for experiment id {test_id} not found. Please enter correct id.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate required objects for interacting with RAGAs\n",
    "\n",
    "- **RAGAs** would require a **LLM** and an **embedding model** depending on the type of **transformation**s one would like to apply to the **knowledge graph**. For that purpose one must create *wrapper* objects for both of the models. `langchain`, `llama-index`, `haystack`, etc are supported.\n",
    "\n",
    "- Additionally, a **configuration** can be used to modify the default behaviour of the framework. For example timeout values can be modified, maximum retries for failed operations and so on can be configured from the **RunConfig**.\n",
    "\n",
    "- **NOTE**: depending on the LLM model and GPU you may need to modify the `timeout` value, otherwise you will stumble upon `TimeoutException`\n",
    "\n",
    "- Lastly, there's a single implementation in **RAGAs** for caching intermediate steps onto disk. To use it the **DiskCacheBackend** class can come in play. Can be useful if the kernel freezes - operations will not be carried out again, since they are cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = RunConfig(\n",
    "    timeout=86400,    # 24 hours on waiting for a single operation\n",
    "    max_retries=20,   # Max retries before giving up\n",
    "    max_wait=600,     # Max wait between retries\n",
    "    max_workers=4,    # Concurrent requests\n",
    "    log_tenacity=True # Print retry attempts\n",
    ")\n",
    "\n",
    "# This stores data generation and evaluation results locally on disk\n",
    "# When using it for the first time, it will create a .cache folder\n",
    "# When using it again, it will read from that folder and finish almost instantly\n",
    "cacher = DiskCacheBackend(cache_dir=f\".cache-{test_id}\")\n",
    "\n",
    "ollama_llm = ChatOllama(\n",
    "    model=os.getenv(\"DATA_GENERATION_MODEL\"),\n",
    "    base_url=\"http://localhost:11434\", # Can vary for you\n",
    "    temperature=float(os.getenv(\"TEMPERATURE\")),\n",
    "    num_ctx=int(os.getenv(\"LLM_CONTEXT_WINDOW_TOKENS\")),\n",
    "    format=\"json\" # We need to enforce JSON output, since most outputs would be validated by a pydantic model\n",
    ")\n",
    "\n",
    "ollama_embeddings = OllamaEmbeddings(\n",
    "    model=os.getenv(\"EMBEDDING_MODEL\"),\n",
    "    base_url=\"http://localhost:11434\" # Can vary for you\n",
    ")\n",
    "\n",
    "ragas_llm = LangchainLLMWrapper(\n",
    "    langchain_llm=ollama_llm,\n",
    "    run_config=run_config,\n",
    "    cache=cacher\n",
    ")\n",
    "\n",
    "ragas_embeddings = LangchainEmbeddingsWrapper(\n",
    "    embeddings=ollama_embeddings,\n",
    "    run_config=run_config,\n",
    "    cache=cacher\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "A metric is a quantatative measure used to evaluate the performance of a LLM-based application. Metrics help us assess how well an application as a whole or its individual components performs relative to the dataset used. With their help we can iterate towards a more *accurate*, *faithful* and *precise* LLM application.\n",
    "\n",
    "Metrics in **RAGAs** can be classified depending on the mechanism they evaluate the application as:\n",
    "\n",
    "- **LLM-based** - where a LLM is used to perform the evaluation. There might be more than one prompts submitted to the LLM to evaluate the application on a single metric. Furthermore, this type of metrics reflect a human-like evaluation.\n",
    "- **Non-LLM based** - where no LLM is required or used to perform the evaluation. These metrics tend to be much faster and predictable than the previous type. However, in my opinion are not the best for evaluating a RAG system where a single query can be submitted multiple times and each time a new response might be generated (re-phrased).\n",
    "\n",
    "The previously mentioned metric types can be further classified into **SingleTurnMetric** and **MultiTurnMetric** respectively. I would like to note again that in this project the **SingleTurnMetric** would be relevant and used.\n",
    "\n",
    "![Metric types in RAGAs](../../img/ragas/metrics_mindmap.webp \"Metrics RAGAs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Context Precision**\n",
    "\n",
    "> **TL;DR:** What fraction of the retrieved chunks are **actually relevant** and their positioning?\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition**:\n",
    "\n",
    "**Context Precision** is a crucial metric for evaluating the **retrieval capabilities** of a RAG (Retrieval-Augmented Generation) system. Since the **LLM's response** depends heavily on the **context retrieved from the knowledge base**, it is essential to have a reliable retrieval mechanism that fetches **only relevant** information.  \n",
    "\n",
    "By achieving **high context precision**, the system ensures that the retrieved information is **more relevant** and **ranked higher**, leading to **more accurate responses** and **reduced hallucinations**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Approach**:\n",
    "\n",
    "The `retrieved_contexts` are iterated on and for each context, an object consisting of `input`, `reference` and `context` is submitted to the LLM for evaluation. The LLM has to classify the context as either useful or not for arriving at the answer. Then using the verdicts, the final score is computed.\n",
    "\n",
    "---\n",
    "\n",
    "### **Formula:**\n",
    "For each retrieved chunk at **rank (k)**:\n",
    "\n",
    "**Precision@k** = $\\frac{\\text{Number of relevant chunks at rank k}}{\\text{Total number of retrieved chunks at rank k}} = \\frac{\\text{true positives@k}}{\\text{true positives@k + false positives@k}}$\n",
    "\n",
    "We then compute **Context Precision@K** as the **mean of all Precision@k values**, weighted by relevance:\n",
    "\n",
    "**Context Precision@K** = $\\frac{\\sum_{k=1}^{K} (\\text{Precision@k} \\times \\text{Relevance}(k))}{\\text{Total number of relevant chunks in top K results}}$\n",
    "\n",
    "Where:\n",
    "- **Precision@k** is the proportion of relevant chunks at rank (k).\n",
    "- **Relevance(k)**:\n",
    "\\begin{cases}\n",
    "1, & \\text{if the chunk at rank } k \\text{ is relevant} \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "- **K** is the total number of retrieved chunks, which are classified as relevant\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Calculation**\n",
    "Suppose our **retriever fetches 4 chunks**, and **2 of them** are relevant. We calculate **Precision@k** at each rank:\n",
    "\n",
    "| Rank (k) | Retrieved Chunk | Relevant? | Precision@k |\n",
    "|-------------|----------------|------------|-------------|\n",
    "| 1           | ✅ Relevant      | ✅ (1)      | \\( $\\frac{1}{1}$ = 1.00 \\) |\n",
    "| 2           | ❌ Not Relevant  | ❌ (0)      | \\( $\\frac{1}{2}$ = 0.50 \\) |\n",
    "| 3           | ✅ Relevant      | ✅ (1)      | \\( $\\frac{2}{3}$ = 0.67 \\) |\n",
    "| 4           | ❌ Not Relevant  | ❌ (0)      | \\( $\\frac{2}{4}$ = 0.50 \\) |\n",
    "\n",
    "Now, using the **Context Precision formula**:\n",
    "\n",
    "\n",
    "**Context Precision@4** = $\\frac{(1.00 \\times 1) + (0.50 \\times 0) + (0.67 \\times 1) + (0.50 \\times 0)}{2} = $\n",
    "\n",
    "= $\\frac{1.00 + 0 + 0.67 + 0}{2} = \\frac{1.67}{2}$ = 0.835\n",
    "\n",
    "Thus, **Context Precision@4 = 0.835**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why is This Useful?**\n",
    "\n",
    "- **Evaluates Retriever Quality** → Ensures retrieved chunks contain **relevant** information.\n",
    "\n",
    "- **Also evaluates the re-rankers capabilities** -> Ensures relevant chunks are ranked higher.\n",
    "\n",
    "- **Improves RAG Performance** → Helps **reduce hallucinations** and improves **LLM accuracy**, since relevant information would come first in the context.\n",
    "\n",
    "- **Higher values** for this metric would signify a good retriever, ranking **relevant chunks** high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_precision = LLMContextPrecisionWithReference(\n",
    "    name=\"context_precision\",\n",
    "    context_precision_prompt=MyContextPrecisionPrompt(),\n",
    "    max_retries=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Context Recall**\n",
    "\n",
    "### TL;DR:\n",
    "> **How many of the relevant chunks were actually retrieved? Did we miss relevant chunks?**\n",
    "\n",
    "---\n",
    "\n",
    "### **Defition**:\n",
    "\n",
    "**Context Recall** measures how many of the **relevant documents (or pieces of information)** were successfully retrieved from the knowledge base. This metric ensures that the retrieval process does not **miss important information** that could be used to generate a response. A **higher recall** means that fewer relevant documents were left out, making it crucial for applications that prioritize completeness over precision. Since recall focuses on **not missing relevant data**, it always requires a reference (expected output) for comparison.\n",
    "\n",
    "---\n",
    "\n",
    "### **Approach**:\n",
    "\n",
    "This metric evaluates recall by analyzing the **claims** in the reference (expected response) and checking whether they can be attributed to the retrieved context (does the context support the claims). In an ideal scenario, **all claims** in the reference answer should be **supported** by the retrieved context.\n",
    "\n",
    "### **Formula**:\n",
    "\n",
    "**Context Recall** = $\\frac{\\text{Number of claims in the reference supported by the retrieved context}}{\\text{Total number of claims in the reference}}$\n",
    "\n",
    "A recall score **closer to 1** indicates that most relevant data has been successfully retrieved, while a **lower recall** means that key information was missed.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why is This Useful?**\n",
    "- **Evaluates Retriever Performance** → Ensures all information required for answering is fetched.\n",
    "- **Helps Optimize RAG Pipelines** → Higher recall ensures more **comprehensive** context retrieval.\n",
    "\n",
    "A recall score **closer to 1** means that most relevant contexts were retrieved, while a **lower recall** suggests that key information was missed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_recall = LLMContextRecall(\n",
    "    name=\"context_recall\",\n",
    "    context_recall_prompt=MyContextRecallPrompt(),\n",
    "    max_retries=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Context Entities Recall**\n",
    "\n",
    "### TL;DR:\n",
    "> **It is a measure of what fraction of entities are recalled from reference**\n",
    "\n",
    "---\n",
    "\n",
    "### Definition:\n",
    "`ContextEntityRecall` metric measures the recall of the retrieved context, based on the number of entities present in both `reference` and `retrieved_contexts`, relative to the total number of entities in the `reference` alone. In simple terms, it evaluates how well the retrieved contexts capture the entities from the original reference.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula:\n",
    "\n",
    "To compute this metric, we define two sets:\n",
    "\n",
    "- **RE**: The set of entities in the reference.\n",
    "- **RCE**: The set of entities in the retrieved contexts.\n",
    "\n",
    "We determine the number of entities common to both sets (**RCE** $\\cap$  **RE**) and divide it by the total number of entities in the reference (**RE**). The formula is:\n",
    "\n",
    "$ \\text{Context Entity Recall} = \\frac{\\text{Number of common entities between RCE and RE}}{\\text{Total number of entities in RE}} = \\frac{\\text{RCE } \\cap \\text{ RE}}{\\text{RE}} $\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Calculation**\n",
    "\n",
    "#### **Scenario:**\n",
    "\n",
    "We have a **reference** and two retrieved contexts:\n",
    "\n",
    "**Reference:**\n",
    "> \"The Taj Mahal is an ivory-white marble mausoleum on the right bank of the river Yamuna in the Indian city of Agra. It was commissioned in 1631 by the Mughal emperor Shah Jahan to house the tomb of his favorite wife, Mumtaz Mahal.\"\n",
    "\n",
    "**Retrieved Contexts:**\n",
    "\n",
    "- **High Entity Recall Context:**\n",
    "  > \"The Taj Mahal is a symbol of love and architectural marvel located in Agra, India. It was built by the Mughal emperor Shah Jahan in memory of his beloved wife, Mumtaz Mahal. The structure is renowned for its intricate marble work and beautiful gardens surrounding it.\"\n",
    "\n",
    "- **Low Entity Recall Context:**\n",
    "  > \"The Taj Mahal is an iconic monument in India. It is a UNESCO World Heritage Site and attracts millions of visitors annually. The intricate carvings and stunning architecture make it a must-visit destination.\"\n",
    "\n",
    "#### **Step 1: Extract Entities**\n",
    "\n",
    "Entities in the **Reference (RE)**:\n",
    "> \\[\"Taj Mahal\", \"Yamuna\", \"Agra\", \"1631\", \"Shah Jahan\", \"Mumtaz Mahal\"\\]\n",
    "\n",
    "Entities in **High Recall Context (RCE1)**:\n",
    "> \\[\"Taj Mahal\", \"Agra\", \"Shah Jahan\", \"Mumtaz Mahal\", \"India\"\\]\n",
    "\n",
    "Entities in **Low Recall Context (RCE2)**:\n",
    "> \\[\"Taj Mahal\", \"UNESCO\", \"India\"\\]\n",
    "\n",
    "#### **Step 2: Compute Context Entity Recall**\n",
    "\n",
    "For **High Recall Context (RCE1)**:\n",
    "$ \\text{Context Entity Recall} = \\frac{4}{6} = 0.67 $ \n",
    "\n",
    "For **Low Recall Context (RCE2)**:\n",
    "$ \\text{Context Entity Recall} = \\frac{1}{6} = 0.17 $\n",
    "\n",
    "Since the first context retains more entities from the reference, it has a **higher entity recall**, indicating it is **more comprehensive** in capturing the essential information.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Insights:**\n",
    "\n",
    "* **Higher Entity Recall Improves Answer Completeness:** If entities are important for context, a higher recall ensures critical information is included in the retrieved context.\n",
    "\n",
    "* **Useful for Fact-Heavy Applications:** Applications in **legal, medical, and historical domains** benefit significantly from high entity recall.\n",
    "\n",
    "* **Balances with Context Precision:** While high recall is beneficial, retrieving too many irrelevant entities can introduce noise. **Optimizing both recall and precision** is crucial for effective retrieval in RAG systems.\n",
    "\n",
    "* **Comparison of Retrieval Mechanisms:** If two retrieval mechanisms fetch different contexts, **Context Entity Recall** helps determine which one is better at preserving key entities from the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_entity_recall = ContextEntityRecall(\n",
    "    context_entity_recall_prompt=MyContextEntitiesRecallPrompt(),\n",
    "    max_retries=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Noise Sensitivity**\n",
    "\n",
    "### TL;DR:\n",
    "> **How prone is the system to generating incorrect claims from retrieved contexts?**\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition**:\n",
    "**Noise Sensitivity** measures how often a system makes errors by providing incorrect responses when utilizing either relevant or irrelevant retrieved documents. This metric evaluates the robustness of a Retrieval-Augmented Generation (RAG) system against potentially misleading or noisy information.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conceptual Insight**:\n",
    "The metric fundamentally tests how easily an LLM can be \"tricked\" into generating factually incorrect responses. It distinguishes between two critical scenarios:\n",
    "\n",
    "1. **Relevant Context Noise**: \n",
    "   - More subtle and dangerous\n",
    "   - Noise is camouflaged within seemingly pertinent information\n",
    "   - High risk of inadvertently incorporating incorrect claims\n",
    "\n",
    "2. **Irrelevant Context Noise**:\n",
    "   - A robust LLM should completely resist this\n",
    "   - Contexts unrelated to the user's query\n",
    "   - Zero tolerance for incorporating unrelated information\n",
    "\n",
    "---\n",
    "\n",
    "### LLM-Based Noise Sensitivity\n",
    "\n",
    "The metric assesses noise sensitivity by decomposing the response and reference into individual claims, then analyzing:\n",
    "- Whether claims are correct according to the ground truth\n",
    "- Whether incorrect claims can be attributed to retrieved contexts (either relevant or irrelevant)\n",
    "\n",
    "---\n",
    "\n",
    "### **Calculation Approach**:\n",
    "\n",
    "1. **Decompose Statements**: Break down the reference and response into individual claims using an LLM evaluator\n",
    "2. **Context Verification**: Check if claims can be supported by retrieved contexts using NLI techniques\n",
    "3. **Identify Relevant Contexts**: Determine which contexts support the ground truth\n",
    "4. **Map Response Claims**: Map each claim in the response to contexts that support it\n",
    "5. **Identify Incorrect Claims**: Determine which response claims are unsupported by the ground truth\n",
    "6. **Calculate Sensitivity**: Compute the proportion of incorrect claims derived from contexts\n",
    "\n",
    "---\n",
    "\n",
    "### **Formula**:\n",
    "\n",
    "Let's define the following:\n",
    "- $S_r$ = Set of response statements\n",
    "- $S_i$ = Set of incorrect statements in the response (not supported by ground truth)\n",
    "- $C_r$ = Set of relevant contexts (contexts that support ground truth)\n",
    "- $C_i$ = Set of irrelevant contexts (contexts that don't support ground truth)\n",
    "- $f(s,c)$ = Function that returns 1 if statement $s$ can be inferred from context $c$, 0 otherwise\n",
    "\n",
    "For each statement $s$ in the response, we can define:\n",
    "- $\\text{relevant\\_faithful}(s) = \\max_{c \\in C_r} f(s,c)$ (1 if statement can be inferred from any relevant context)\n",
    "- $\\text{irrelevant\\_faithful}(s) = \\max_{c \\in C_i} f(s,c) \\cdot (1 - \\text{relevant\\_faithful}(s))$ (1 if statement can be inferred from irrelevant context and not from any relevant context)\n",
    "\n",
    "Then:\n",
    "\n",
    "**Relevant Mode Noise Sensitivity**:\n",
    "$$\\text{NS}_{\\text{relevant}} = \\frac{\\sum_{s \\in S_i} \\text{relevant\\_faithful}(s)}{|S_r|}$$\n",
    "\n",
    "**Irrelevant Mode Noise Sensitivity**:\n",
    "$$\\text{NS}_{\\text{irrelevant}} = \\frac{\\sum_{s \\in S_i} \\text{irrelevant\\_faithful}(s)}{|S_r|}$$\n",
    "\n",
    "Where $|S_r|$ is the total number of statements in the response.\n",
    "\n",
    "In plain English:\n",
    "- Relevant mode: Proportion of response statements that are both incorrect and supported by relevant contexts\n",
    "   - incorrect statement means that a there's a contradiction between a statement in the `response` and `reference`\n",
    "   - relevant context is a context, which supports information from the `reference`\n",
    "- Irrelevant mode: Proportion of response statements that are both incorrect and supported only by irrelevant contexts\n",
    "   - irrelevant context is a context, which doesn't support any statement in the `reference`\n",
    "\n",
    "A score **closer to 0** indicates better performance, suggesting:\n",
    "- Fewer incorrect claims\n",
    "- Less influence from noisy or irrelevant contexts\n",
    "- More robust response generation\n",
    "\n",
    "---\n",
    "\n",
    "### **Modes**:\n",
    "- **Relevant Mode** (default): Focuses on noise in relevant contexts\n",
    "- **Irrelevant Mode**: Analyzes noise from irrelevant retrieved documents\n",
    "\n",
    "---\n",
    "\n",
    "### **Evaluation Expectations**:\n",
    "- **Relevant Context**: Some noise tolerance, but should be minimal\n",
    "- **Irrelevant Context**: Virtually zero noise should be incorporated\n",
    "  - A high-quality LLM should completely disregard irrelevant information\n",
    "  - Any noise from irrelevant contexts indicates a significant vulnerability\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Scenario**:\n",
    "Consider a query about the Life Insurance Corporation of India (LIC):\n",
    "- **Question**: \"What is the Life Insurance Corporation of India (LIC) known for?\"\n",
    "- **Reference**: \"The Life Insurance Corporation of India (LIC) is the largest insurance company in India, established in 1956 through the nationalization of the insurance industry. It is known for managing a large portfolio of investments.\"\n",
    "- **Response**: \"The Life Insurance Corporation of India (LIC) is the largest insurance company in India, known for its vast portfolio of investments. LIC contributes to the financial stability of the country.\"\n",
    "- **Retrieved Contexts**: Mix of relevant information about LIC and irrelevant information about the Indian economy\n",
    "- **Noise Sensitivity Score**: 0.33 (one incorrect claim out of three total claims)\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Insights**:\n",
    "- Helps identify system vulnerabilities to hallucination\n",
    "- Provides a quantitative measure of response reliability\n",
    "- Distinguishes between subtle (relevant) and gross (irrelevant) information distortions\n",
    "- Serves as a critical component in comprehensive RAG system evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this project we will focus on the `relevant` mode\n",
    "# This means how sensitive is the LLM to noise from relevant context\n",
    "noise_sensitivity = NoiseSensitivity(\n",
    "    nli_statements_prompt=MyNLIStatementPrompt(),\n",
    "    statement_generator_prompt=MyStatementGeneratorPrompt(),\n",
    "    max_retries=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Response Relevancy**\n",
    "\n",
    "### TL;DR:\n",
    "> **How well does the answer address the original user query?**\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition**:\n",
    "**Response Relevancy** measures how closely an AI-generated response aligns with the original user input. It evaluates the answer's ability to directly and appropriately address the user's question, penalizing responses that are incomplete, off-topic, or include unnecessary details. It doesn't judge the **factual accuracy**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conceptual Insight**:\n",
    "The metric focuses on the semantic alignment between:\n",
    "- The original user query\n",
    "- The generated response\n",
    "\n",
    "Key evaluation criteria:\n",
    "1. **Direct Address**: Does the answer directly tackle the user's question?\n",
    "2. **Information Completeness**: Does the response provide sufficient information?\n",
    "3. **Topic Coherence**: Does the answer stay focused on the query's intent?\n",
    "\n",
    "---\n",
    "\n",
    "### **Calculation Approach**:\n",
    "\n",
    "1. **Question Generation**: \n",
    "   - Use the LLM to generate artificial questions based on the response\n",
    "   - Default is to create 3 variant questions\n",
    "   - These questions should capture the essence of the response\n",
    "\n",
    "2. **Semantic Similarity**:\n",
    "   - Compute cosine similarity between:\n",
    "     - Original user input embedding\n",
    "     - Embeddings of generated questions\n",
    "   - Measures how closely the questions match the original query\n",
    "\n",
    "3. **Scoring**:\n",
    "   - Average the cosine similarity scores\n",
    "   - Higher scores indicate better relevance\n",
    "   - Scores typically range between 0 and 1\n",
    "\n",
    "---\n",
    "\n",
    "### **Formula**:\n",
    "\n",
    "**Response Relevancy** = $\\frac{\\sum_{i=1}^{N} \\text{cosine\\_similarity}(E_{g_{i}}, E_{o})}{N}$ \n",
    "\n",
    "Where:\n",
    "- $E_{g_{i}}$: Embedding of the i-th generated question\n",
    "- $E_{o}$: Embedding of the original user input\n",
    "- N: Number of generated questions (default: 3)\n",
    "\n",
    "---\n",
    "\n",
    "### **Evaluation Expectations**:\n",
    "- **High Score (Close to 1)**: \n",
    "  - Answer directly addresses the query\n",
    "  - Comprehensive and focused response\n",
    "  - Minimal irrelevant information\n",
    "\n",
    "- **Low Score (Close to 0)**: \n",
    "  - Response is off-topic\n",
    "  - Incomplete or evasive answer\n",
    "  - Includes excessive unrelated details\n",
    "\n",
    "---\n",
    "\n",
    "### **Important Limitations**:\n",
    "- **No Factuality Check**: \n",
    "  - Measures relevance, not accuracy\n",
    "  - Does not verify the truthfulness of the response\n",
    "- **Embedding-Based**: \n",
    "  - Relies on semantic similarity\n",
    "  - May not catch nuanced relevance\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Scenario**:\n",
    "- **User Query**: \"When was the first Super Bowl?\"\n",
    "- **Good Response**: \"The first Super Bowl was held on Jan 15, 1967, between the Green Bay Packers and Kansas City Chiefs.\"\n",
    "- **Poor Response**: \"Football is a popular sport in the United States with many interesting historical moments.\"\n",
    "\n",
    "### **Key Insights**:\n",
    "- Helps evaluate response quality beyond simple keyword matching\n",
    "- Provides a quantitative measure of semantic alignment\n",
    "- Supports improving AI system's query understanding\n",
    "- Identifies potential issues with off-topic or unfocused responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_relevancy = ResponseRelevancy(\n",
    "    question_generation=MyResponseRelevancePrompt()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Faithfulness**\n",
    "\n",
    "### TL;DR:\n",
    "> **How factually consistent is the response with the retrieved context?**\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition**:\n",
    "The **Faithfulness** metric measures how factually consistent a response is with the retrieved context. It ranges from 0 to 1, with higher scores indicating better consistency.\n",
    "\n",
    "A response is considered faithful if all of its claims can be supported by the retrieved context.\n",
    "\n",
    "---\n",
    "\n",
    "### **Calculation Approach**:\n",
    "\n",
    "1. **Identify Claims**: \n",
    "   - Break down the response into individual statements\n",
    "   - Examine each claim systematically\n",
    "\n",
    "2. **Context Verification**:\n",
    "   - Check each claim to see if it can be inferred from the retrieved context\n",
    "   - Determine the support level of each statement\n",
    "\n",
    "3. **Scoring**:\n",
    "   - Compute the faithfulness score using the formula:\n",
    "     \n",
    "     $\\text{Faithfulness Score} = \\frac{\\text{Number of claims supported by the retrieved context}}{\\text{Total number of claims in the response}}$\n",
    "\n",
    "---\n",
    "\n",
    "### **Example**:\n",
    "\n",
    "**Question**: Where and when was Einstein born?\n",
    "\n",
    "**Context**: Albert Einstein (born 14 March 1879) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time.\n",
    "\n",
    "**High Faithfulness Answer**: Einstein was born in Germany on 14th March 1879.\n",
    "\n",
    "**Low Faithfulness Answer**: Einstein was born in Germany on 20th March 1879.\n",
    "\n",
    "#### Calculation Steps:\n",
    "- **Step 1**: Break the generated answer into individual statements\n",
    "- **Step 2**: Verify if each statement can be inferred from the given context\n",
    "- **Step 3**: Apply the faithfulness formula\n",
    "\n",
    "---\n",
    "\n",
    "### **Evaluation Expectations**:\n",
    "- **Perfect Score (1.0)**: \n",
    "  - All claims are directly supported by the context\n",
    "  - No extraneous or unsupported information\n",
    "\n",
    "- **Partial Score**: \n",
    "  - Some claims are supported\n",
    "  - Partial consistency with the retrieved context\n",
    "\n",
    "- **Low Score (Close to 0)**: \n",
    "  - Most claims cannot be verified\n",
    "  - Significant deviation from the original context\n",
    "\n",
    "---\n",
    "\n",
    "### **Advanced Verification**:\n",
    "**HHEM-2.1-Open** can be used as a classifier model to:\n",
    "- Detect hallucinations in LLM-generated text\n",
    "- Cross-check claims with the given context\n",
    "- Efficiently determine claim inferability\n",
    "- Avoid **biases** by not using `LLM-As-A-Judge`\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Insights**:\n",
    "- Measures the factual consistency of AI-generated responses\n",
    "- Helps identify potential hallucinations or fabrications\n",
    "- Provides a quantitative assessment of contextual alignment\n",
    "- Supports improving the reliability of AI-generated content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness = Faithfulness(\n",
    "    nli_statements_prompt=MyNLIStatementPrompt(),\n",
    "    statement_generator_prompt=MyStatementGeneratorPrompt(),\n",
    "    max_retries=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Factual Correctness**\n",
    "\n",
    "### TL;DR:\n",
    "> **How factually consistent is a model's response with the ground truth?**\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition**:\n",
    "**Factual Correctness** measures the degree to which a model's `response` aligns with the `reference` by assessing the factual consistency between the two. It does this by breaking down both the response and reference into distinct claims and then determining whether these claims match using natural language inference (NLI). This metric helps evaluate how accurately the generated response retains factual integrity.\n",
    "\n",
    "The factual correctness score ranges from **0 to 1**, where higher values indicate better factual alignment.\n",
    "\n",
    "---\n",
    "\n",
    "### **Claim Breakdown and Classification**:\n",
    "This metric identifies three types of claims:\n",
    "- **True Positives (TP):** Claims that are present in both the response and the reference.\n",
    "- **False Positives (FP):** Claims that are in the response but not supported by the reference.\n",
    "- **False Negatives (FN):** Claims that are in the reference but missing from the response.\n",
    "\n",
    "---\n",
    "\n",
    "### **Calculation Modes**:\n",
    "The metric can be computed in three different modes:\n",
    "\n",
    "#### **Precision Mode:**\n",
    "> Measures how much of the information in the response is factually correct.\n",
    "\n",
    "$ \\text{Precision} = \\frac{TP}{TP + FP} $\n",
    "\n",
    "- High precision means the response avoids including unsupported claims.\n",
    "- Penalizes responses that introduce hallucinated information.\n",
    "\n",
    "#### **Recall Mode:**\n",
    "> Measures how much of the reference information is retained in the response.\n",
    "\n",
    "$ \\text{Recall} = \\frac{TP}{TP + FN} $\n",
    "\n",
    "- High recall means the response includes all important reference claims.\n",
    "- Penalizes responses that omit key factual details.\n",
    "\n",
    "#### **F1 Mode (Default):**\n",
    "> Balances precision and recall for a comprehensive factual correctness score.\n",
    "\n",
    "$ \\text{F1} = \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} $\n",
    "\n",
    "---\n",
    "\n",
    "### **Controlling the Number of Claims**\n",
    "Each sentence in the response and reference can be decomposed into multiple claims. The granularity of this decomposition is determined by **atomicity** and **coverage**.\n",
    "\n",
    "#### **Atomicity:**\n",
    "Controls how much a sentence is broken down:\n",
    "- **High Atomicity:** Breaks a sentence into fine-grained claims.\n",
    "- **Low Atomicity:** Keeps a sentence more intact with minimal decomposition.\n",
    "\n",
    "#### **Coverage:**\n",
    "Determines how much information is extracted:\n",
    "- **High Coverage:** Extracts all details from the original sentence.\n",
    "- **Low Coverage:** Focuses on key information, omitting minor details.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example of Atomicity and Coverage Adjustments**\n",
    "#### **High Atomicity & High Coverage:**\n",
    "```python\n",
    "scorer = FactualCorrectness(mode=\"precision\", atomicity=\"high\", coverage=\"high\")\n",
    "```\n",
    "**Original Sentence:**  \n",
    "> \"Marie Curie was a Polish and naturalized-French physicist and chemist who conducted pioneering research on radioactivity.\"\n",
    "\n",
    "**Decomposed Claims:**\n",
    "- \"Marie Curie was a Polish physicist.\"\n",
    "- \"Marie Curie was a naturalized-French physicist.\"\n",
    "- \"Marie Curie was a chemist.\"\n",
    "- \"Marie Curie conducted pioneering research on radioactivity.\"\n",
    "\n",
    "---\n",
    "\n",
    "### **Practical Application**\n",
    "- **High Atomicity & High Coverage** → Best for detailed fact-checking and claim extraction.\n",
    "- **Low Atomicity & Low Coverage** → Suitable for summarization or when only key facts are needed.\n",
    "\n",
    "This flexibility ensures the metric can be tailored to different levels of granularity based on the application.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways:**\n",
    "* Factual correctness evaluates the factual overlap between a response and a reference.  \n",
    "* It provides **precision**, **recall**, and **F1-score** options for measurement.  \n",
    "* Atomicity and coverage control the **granularity** of claim decomposition.  \n",
    "* Helps identify hallucinations (FP) and missing information (FN) in responses.  \n",
    "\n",
    "This metric is crucial for **evaluating Retrieval-Augmented Generation (RAG) systems** where factual consistency is a priority!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factual_correctness = FactualCorrectness(\n",
    "    nli_prompt=MyNLIStatementPrompt()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Semantic Similarity**\n",
    "\n",
    "### TL;DR:\n",
    "> **How semantically similar are two texts?**\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition**:\n",
    "**Semantic Similarity** measures the degree of similarity between two texts. It uses an `embedding model` to convert the text into a high dimensional vector or so-called `embedding` and it computes the similarity using `cosine distance` for example between 2 such vectors. Two texts might contain very similar information semantically, however can be phrased in various ways. This metric determines if they are similar or close in meaning to each other regardless of the wording.\n",
    "\n",
    "---\n",
    "\n",
    "### **Approach**:\n",
    "Compute the emebedding of both the `response` and `reference`, then normalize the vectors and finally compute the score itself. The score will range between **0 to 1** with higher values indication higher similarity. This metric is great for assessing the `generator component` without requiring a LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_similarity = SemanticSimilarity(\n",
    "    threshold=0.7, # Default is 0.5 = 50%\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Finally, after all relevant metrics are initialized and customized as needed we can evaluate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results: EvaluationResult = evaluate(\n",
    "    dataset=evaluation_dataset,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        context_entity_recall,              \n",
    "        noise_sensitivity,    \n",
    "        response_relevancy,                 \n",
    "        faithfulness,                       \n",
    "        factual_correctness,                \n",
    "        semantic_similarity                 \n",
    "    ],\n",
    "    llm=ragas_llm,\n",
    "    embeddings=ragas_embeddings,\n",
    "    experiment_name=f\"{test_id}_evaluation\",\n",
    "    run_config=run_config,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the directory exists\n",
    "os.makedirs(\"./res\", exist_ok=True)\n",
    "\n",
    "# Save results locally (optional)\n",
    "result_df: pd.DataFrame = results.to_pandas()\n",
    "result_df.to_csv(f'./res/{test_id}_eval_results.csv', index=False)\n",
    "\n",
    "# Display metric scores\n",
    "results.scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to the platform (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.getenv(\"RAGAS_APP_TOKEN\"):\n",
    "    results.upload()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
