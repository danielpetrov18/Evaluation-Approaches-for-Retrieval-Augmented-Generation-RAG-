{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance of High-Quality Test Dataset for RAG Systems\n",
    "\n",
    "### Purpose\n",
    "Test sets are critical for:\n",
    "- Accurately measuring RAG system performance\n",
    "- Identifying system strengths and weaknesses\n",
    "- Guiding continuous improvement\n",
    "\n",
    "### Key Evaluation Dimensions\n",
    "1. **Retrieval Effectiveness**: Assess how well context is retrieved\n",
    "    - completeness (recall)\n",
    "    - relevancy (relative to user input)\n",
    "    - precision (proper ranking)\n",
    "2. **Generation Quality**: Evaluate the generated responses\n",
    "    - faithfulness/groundedness in context (lack of hallucinations)\n",
    "    - relevance (relative to the user input)\n",
    "    - coherence\n",
    "    - correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "* Execute the script in the root of the `evaluation` folder.\n",
    "* If executing the script fails run: `chmod u+x setup.sh`.\n",
    "* It will install all required dependencies and open a jupyterlab instance.\n",
    "\n",
    "---\n",
    "\n",
    "#### (OPTIONAL STEP)\n",
    "\n",
    "**RAGAs** provides a cloud platform where a dataset and evaluation results can be stored and viewed. To use it follow this link: [RAGAs.io](https://app.ragas.io/).\n",
    "\n",
    "* Sign-up\n",
    "* Retrieve the **token**\n",
    "* Create a `.env` file in the root of the evaluation folder with the following content:\n",
    "\n",
    "```bash\n",
    "RAGAS_APP_TOKEN=apt.......-9f6ed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Configuration for generating the goldens\n",
    "\n",
    "Goldens (synthetically generated) are the samples containing the:\n",
    "- `reference` (expected output) \n",
    "- `reference_contexts` (context used for generating the query)\n",
    "- `user_input` (query). \n",
    "\n",
    "For filling out the rest of the fields I use various of different configurations, which are to be found at the root of the project under *experiments.csv*.\n",
    "- `response` (actual output / LLM-generated using the context)\n",
    "- `retrieved_contexts` (data retrieved during retrieval) \n",
    "\n",
    "For the generation of the *goldens* for all experiments I will make use of the following parameters:\n",
    "- generative model - `llama3.1:8b-instruct-q4_1`\n",
    "- temperature - `0.0`\n",
    "- the rest of the parameters will vary depending on the experiments configuration - `chunk size`, `overlap`, etc.\n",
    "\n",
    "The data generation pipeline consists of 2 stages:\n",
    "- we first create the so called `goldens` using RAGAs (up to step 11)\n",
    "- then we use the RAG application to fill out the rest (you can use your own custom RAG pipeline - step 12)\n",
    "\n",
    "**NOTE**: Before running this notebook make sure:\n",
    "- you set the correct variables under `env/rag.env` and start the application by running `./run.sh` in the root of the project\n",
    "- Alternatively, you could make use of your own RAG application.\n",
    "    - make sure you set the correct environment variables to reflect the experiment you want to generate data for under `env/rag.env`\n",
    "    - export all the environment variables under `env`\n",
    "    - run ollama: `ollama serve` (check out the `run.sh` for additional environment variables)\n",
    "    - run all the docker services - `docker compose up -d --build`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Retrieve data (Optional):\n",
    "\n",
    "The data will be used as a corpus to create synthetic queries (based on the context) and generate references (expected outputs).\n",
    "\n",
    "* The data can be a dataset from `huggingface` or any other platform.\n",
    "* Alternatively, files available on disk - pdf, md, etc.\n",
    "* One can also use `AsyncHtmlLoader` from `langchain` to scrape from the internet.\n",
    "    - **Careful when performing web scraping to not violate any terms and conditions!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PATH: str = \"data\"\n",
    "\n",
    "# For this notebook I will use a dataset provided by RAGAs.\n",
    "# The dataset contains markdown files about a fictional airline company.\n",
    "! git clone https://huggingface.co/datasets/explodinggradients/ragas-airline-dataset {DIR_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import all dependencies\n",
    "\n",
    "This notebook relies on various of libraries, so it's best to import everything in a single cell just to keep things neat.\n",
    "\n",
    "Additionally, environment variables will be exported here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import mimetypes\n",
    "from distutils.util import strtobool\n",
    "from typing import Final, List, Dict, Union\n",
    "\n",
    "import requests\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_ollama import (\n",
    "    ChatOllama, OllamaEmbeddings\n",
    ")\n",
    "\n",
    "from ragas import (\n",
    "    RunConfig, DiskCacheBackend\n",
    ")\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "from ragas.testset.graph import (\n",
    "    Node, NodeType, KnowledgeGraph\n",
    ")\n",
    "from ragas.testset.transforms import (\n",
    "    Parallel,\n",
    "    OverlapScoreBuilder,\n",
    "    KeyphrasesExtractor,\n",
    "    apply_transforms\n",
    ")\n",
    "from ragas.testset.persona import Persona\n",
    "from ragas.testset import TestsetGenerator, Testset\n",
    "from ragas.testset.transforms.extractors import NERExtractor\n",
    "from ragas.testset.synthesizers.multi_hop.specific import MultiHopSpecificQuerySynthesizer\n",
    "from ragas.testset.synthesizers.single_hop.specific import SingleHopSpecificQuerySynthesizer\n",
    "\n",
    "from prompts.extractors.custom_ner_prompt import MyNERPrompt\n",
    "from prompts.extractors.custom_keyphrases_prompt import MyKeyphrasesExtractorPrompt\n",
    "from prompts.synthesizers.custom_themes_matching import MyThemesPersonasMatchingPrompt\n",
    "from prompts.synthesizers.custom_multi_hop_qa_generation import MyMultiHopQAGenerationPrompt\n",
    "from prompts.synthesizers.custom_single_hop_qa_generation import MySingleHopQAGenerationPrompt\n",
    "\n",
    "# Load the RAG parameters\n",
    "load_dotenv(\"../../env/rag.env\")\n",
    "\n",
    "# Load the RAGAs token (if available - optional)\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load documents data and prepare them for knowledge graph creation (Pre-processing phase)\n",
    "\n",
    "The information from the documents will be used as context by RAGAs to generate synthetic queries.\n",
    "\n",
    "For extracting data from documents and splitting them into chunks we will make use of the [unstructured library](https://docs.unstructured.io/welcome). In my project I use the `basic` chunking strategy, however `by_title` could also be useful.\n",
    "\n",
    "If you want to learn more about that follow these links:\n",
    "- https://docs.unstructured.io/api-reference/partition/partitioning\n",
    "- https://docs.unstructured.io/api-reference/partition/chunking\n",
    "\n",
    "Do also check out the file under `project/backend/config.toml` and the `ingestion` section:\n",
    "```bash\n",
    "# https://unstructured.io/blog/chunking-for-rag-best-practices\n",
    "[ingestion]\n",
    "provider = \"unstructured_local\"  # Use the local instance that is running in a container\n",
    "strategy = \"auto\"                # https://docs.unstructured.io/open-source/concepts/partitioning-strategies\n",
    "chunking_strategy = \"basic\"      # https://docs.unstructured.io/api-reference/partition/chunking\n",
    "new_after_n_chars = 1024         # Soft limit for chunk size (always max_characters)\n",
    "max_characters = 1024            # Hard limit for chunk size (it can never exceed this value)\n",
    "combine_text_under_n_chars = 512 # If chunks are smaller than this value, they will be combined (always max_characters / 2)\n",
    "overlap = 128                    # chunk overlap \n",
    "chunk_size = 1024                # This is used for the RecursiveCharacterTextSplitter in case unstructured doesn't work (fallback)\n",
    "chunk_overlap = 128              # This is used for the RecursiveCharacterTextSplitter in case unstructured doesn't work (fallback) \n",
    "chunks_for_document_summary = 16\n",
    "document_summary_model = \"ollama_chat/llama3.1:8b\"\n",
    "automatic_extraction = false\n",
    "```\n",
    "\n",
    "#### Pseudo-algorithm\n",
    "1. Retrieve the configuration for ingestion\n",
    "2. Format it properly and prepare the ingestion request\n",
    "3. Submit the request to the `unstructured` service\n",
    "4. Extract the data from the response for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.\n",
    "# We request the current configuration settings from r2r.\n",
    "# They are hardcoded at project/backend/config.toml.\n",
    "# However we can overwrite them at runtime using environment variables.\n",
    "settings_resp: requests.Response = requests.get(\n",
    "    url=\"http://localhost:7272/v3/system/settings\"\n",
    ")\n",
    "\n",
    "# Make sure the environment variables are correct under: env/rag.env\n",
    "# Since each experiment can have varying parameters and restarting the docker containers\n",
    "# is not very efficient one can modify the environment variables directly in the env/rag.env file.\n",
    "# Then we can just overwrite specific ones so that they match the experiment requirements.\n",
    "# Before re-running the notebook make sure you restart the kernel so changes take place.\n",
    "\n",
    "# Get the current ingestion configuration\n",
    "ingestion_config: Dict = settings_resp.json()['results']['config']['ingestion']\n",
    "\n",
    "# Extra fields is the relevant part of the ingestion config for submitting a request to unstructured.\n",
    "ingestion_config_request: Dict = ingestion_config['extra_fields']\n",
    "ingestion_config_request['chunking_strategy'] = ingestion_config['chunking_strategy']\n",
    "\n",
    "# Overwrite the experiment specific parameters for ingestion.\n",
    "ingestion_config_request['max_characters'] = int(os.getenv('CHUNK_SIZE'))\n",
    "ingestion_config_request['new_after_n_chars'] = int(ingestion_config_request['max_characters'])\n",
    "ingestion_config_request['overlap'] = int(os.getenv('CHUNK_OVERLAP'))\n",
    "ingestion_config_request['combine_text_under_n_chars'] = (\n",
    "    int(ingestion_config_request['max_characters']) // 2\n",
    ")\n",
    "\n",
    "# The max_characters should match the CHUNK_SIZE environment variable in the rag.env file\n",
    "# The new_after_n_chars is equal to the max_characters\n",
    "# The overlap should match the CHUNK_OVERLAP environment variable in the rag.env file\n",
    "# combine_text_under_n_chars is always max_characters / 2\n",
    "print(f\"Current ingestion config sent to unstructured:\\n{ingestion_config_request}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markdown_to_plaintext(md_content: str) -> str:\n",
    "    html: str = markdown.markdown(md_content)\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "    return soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will hold all the chunks for each particular file\n",
    "# Each key-value pair will be a mapping from the filename to its chunks\n",
    "chunks: Dict[str, List[str]] = {}\n",
    "\n",
    "# Iterate over the files and prepare the file content for chunking by unstructured\n",
    "for file in os.listdir(DIR_PATH):\n",
    "    if file.endswith(\".md\") and file != \"README.md\": # This may vary depending on the documents you are working with\n",
    "        # Open and read the contents\n",
    "        with open(file=f\"data/{file}\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            markdown_text: str = f.read()\n",
    "\n",
    "        plain_text: str = markdown_to_plaintext(markdown_text)\n",
    "        file_bytes: bytes = plain_text.encode(\"utf-8\")  # convert back to bytes\n",
    "        encoded_file: bytes = base64.b64encode(file_bytes).decode(\"utf-8\")\n",
    "        \n",
    "        # Step 2.\n",
    "        # Prepare the payload for the ingestion request\n",
    "        payload = {\n",
    "            \"file_content\": encoded_file,\n",
    "            \"filename\": file,\n",
    "            \"ingestion_config\": ingestion_config_request\n",
    "        }\n",
    "\n",
    "        # Step 3.\n",
    "        # Send request\n",
    "        response: requests.Response = requests.post(\n",
    "            \"http://localhost:7275/partition\", # See the docker compose file\n",
    "            json=payload,\n",
    "            timeout=60\n",
    "        )\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            raise Exception(\n",
    "                f\"Failed to process file {file}: {response.status_code} - {response.text}\"\n",
    "            )\n",
    "\n",
    "        # Step 4.\n",
    "        # Collect the chunks for each file\n",
    "        chunks[file] = [el['text'] for el in response.json()['elements'] if el['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Construct knowledge graph\n",
    "\n",
    "- https://docs.ragas.io/en/latest/getstarted/rag_testset_generation/\n",
    "- https://docs.ragas.io/en/latest/concepts/test_data_generation/rag/\n",
    "\n",
    "- A **knowledge graph** is a fundamental concept when it comes to **RAGAs** and using its capabilities for **automatic synthetic data generation**.\n",
    "\n",
    "- A **knowledge graph** consists of **node**s at first, which represent **documents/chunks** - their content and additionally metadata (optional).\n",
    "\n",
    "- Thereafter, one can enrich the graph by using various **extractors** and applying different **transformations**. Doing so additional attributes get added to the relevant nodes and **relationships can get built**, which express some kind of connection between nodes. The transformations can be applied only through the use of **Extractor**s, **Splitter**s and or **RelationshipBuilder**s. They serve as a way to gather relevant data from the documents depending on the type of extractor and this way to logically connect 2 or more nodes together.\n",
    "\n",
    "- Finally, the graph is used to generate so called **scenario**s.\n",
    "\n",
    "- Optionally, one could get **persona**s generated from it (optional).\n",
    "\n",
    "![Knowledge graph creation workflow RAGAs](../../img/ragas/kg_rag.webp \"Knowledge graph RAGAs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg = KnowledgeGraph()\n",
    "\n",
    "# After having chunked the documents we can create the knowledge graph\n",
    "# Then we can use transformations to enrich it\n",
    "# Finally, RAGAs will use the nodes and their relationships to build contexts and finally synthesize queries\n",
    "for filename, text_chunks in chunks.items():\n",
    "    for chunk in text_chunks:\n",
    "        kg.add(\n",
    "            Node(\n",
    "                # Since we already split the documents, we can use the chunk type.\n",
    "                # Alternatively, with full documents you can use DOCUMENT.\n",
    "                type=NodeType.CHUNK,\n",
    "                # Can add other properties if you want\n",
    "                properties={\n",
    "                    \"page_content\": chunk,\n",
    "                    \"document_metadata\": {\n",
    "                        \"source\": filename,\n",
    "                        \"file_type\": mimetypes.guess_type(filename)[0] or \"text/plain\"\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Instantiate required objects for interacting with RAGAs\n",
    "\n",
    "- https://docs.ragas.io/en/latest/references/llms/\n",
    "- https://docs.ragas.io/en/latest/references/embeddings/\n",
    "- https://docs.ragas.io/en/latest/references/run_config/\n",
    "- https://docs.ragas.io/en/latest/references/cache/\n",
    "\n",
    "- **RAGAs** would require a **LLM** and an **embedding model** depending on the type of **transformation**s one would like to apply to the **knowledge graph**. For that purpose one must create *wrapper* objects for both of the models. `langchain`, `llama-index`, `haystack`, etc are supported.\n",
    "\n",
    "- Additionally, a **configuration** can be used to modify the default behaviour of the framework. For example timeout values can be modified, maximum retries for failed operations and so on can be configured from the **RunConfig**.\n",
    "\n",
    "- **NOTE**: depending on the LLM model and GPU you may need to modify the `timeout` value, otherwise you will stumble upon `TimeoutException`\n",
    "\n",
    "- Lastly, there's a single implementation in **RAGAs** for caching intermediate steps onto disk. To use it the **DiskCacheBackend** class can come in play. Can be useful if the kernel freezes - operations will not be carried out again, since they are cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id: str = input(\"Enter the test id (Ex. 1): \")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    timeout=86400,    # 24 hours on waiting for a single operation\n",
    "    max_retries=20,   # Max retries before giving up\n",
    "    max_wait=600,     # Max wait between retries\n",
    "    max_workers=4,    # Concurrent requests\n",
    "    log_tenacity=True # Print retry attempts\n",
    ")\n",
    "\n",
    "# This stores data generation and evaluation results locally on disk\n",
    "# When using it for the first time, it will create a .cache folder\n",
    "# When using it again, it will read from that folder and finish almost instantly\n",
    "cacher = DiskCacheBackend(cache_dir=f\".cache-{test_id}\")\n",
    "\n",
    "ollama_llm = ChatOllama(\n",
    "    model=os.getenv(\"DATA_GENERATION_MODEL\"),\n",
    "    base_url=\"http://localhost:11434\", # Can vary for you\n",
    "    temperature=float(os.getenv(\"TEMPERATURE\")),\n",
    "    num_ctx=int(os.getenv(\"LLM_CONTEXT_WINDOW_TOKENS\")),\n",
    "    format=\"json\" # We need to enforce JSON output, since most outputs would be validated by a pydantic model\n",
    ")\n",
    "\n",
    "ollama_embeddings = OllamaEmbeddings(\n",
    "    model=os.getenv(\"EMBEDDING_MODEL\"),\n",
    "    base_url=\"http://localhost:11434\" # Can vary for you\n",
    ")\n",
    "\n",
    "ragas_llm = LangchainLLMWrapper(\n",
    "    langchain_llm=ollama_llm,\n",
    "    run_config=run_config,\n",
    "    cache=cacher\n",
    ")\n",
    "\n",
    "ragas_embeddings = LangchainEmbeddingsWrapper(\n",
    "    embeddings=ollama_embeddings,\n",
    "    run_config=run_config,\n",
    "    cache=cacher\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Create the transformation pipeline\n",
    "\n",
    "https://docs.ragas.io/en/latest/references/transforms/\n",
    "\n",
    "The sequence of transformations:\n",
    "\n",
    "1. Named Entity Recognition (NER) and Keyphrases extraction:\n",
    "    - NERExtractor identifies and extracts named entities (e.g., people, organizations, locations)\n",
    "    - KeyphrasesExtractor extracts the main keyphrases to be found in the text\n",
    "\n",
    "2. NEROverlapBuilder and KeyphraseOverlapBuilder:\n",
    "    - Used to establish a relationship between nodes containing similar:\n",
    "        - entities\n",
    "        - keyphrases\n",
    "\n",
    "3. Parallel Processing for Efficiency:\n",
    "    - Certain transformations can run in parallel to improve performance.\n",
    "\n",
    "- Final Outcome:\n",
    "    - A structured set of document transformations that extract valuable information\n",
    "    - Used to enrich the knowledge graph for further generation of scenarios and finally samples\n",
    "\n",
    "**NOTE:**\n",
    "- Some of the extractors *(LLM-based ones)* do receive an optional `prompt`, which one can use to modify the workflow. For instance the `NERExtractor` can receive a custom prompt, which could contain instructions that differ from the original one and extracts entities in a different way.\n",
    "- Refer to the `prompts/extractors` folder.\n",
    "- Also, there're many other transformations that can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract entities\n",
    "ner_extractor = NERExtractor(\n",
    "    llm=ragas_llm,\n",
    "    prompt=MyNERPrompt(\n",
    "        name=\"custom_ner_extractor_prompt\"\n",
    "    ),\n",
    "    max_num_entities=15\n",
    ")\n",
    "\n",
    "# Extract keyphrases\n",
    "keyphrases_extractor = KeyphrasesExtractor(\n",
    "    llm=ragas_llm,\n",
    "    prompt=MyKeyphrasesExtractorPrompt(\n",
    "        name=\"custom_keyphrases_extractor_prompt\"\n",
    "    ),\n",
    "    max_num=15\n",
    ")\n",
    "\n",
    "# Create relationships between chunks based on entities\n",
    "ner_overlap_sim = OverlapScoreBuilder()\n",
    "\n",
    "# Create relationships between chunks based on keyphrases\n",
    "keyphrases_overlap_sim = OverlapScoreBuilder(\n",
    "    property_name=\"keyphrases\",\n",
    ")\n",
    "\n",
    "# Collect all the transformations\n",
    "transforms = [\n",
    "    Parallel(\n",
    "        ner_extractor,\n",
    "        keyphrases_extractor\n",
    "    ),\n",
    "    Parallel(\n",
    "        ner_overlap_sim,\n",
    "        keyphrases_overlap_sim\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Apply the transformations to the knowledge graph\n",
    "\n",
    "In the cell below the `apply_transforms` is going to apply all the previously defined transformations enriching the `knowledge graph` in the process.\n",
    "\n",
    "Make sure you have your LLM-provider available, otherwise this will not work.\n",
    "You will also need to have both the LLM and embedding models installed, if using a provider locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_transforms(\n",
    "    kg,\n",
    "    transforms,\n",
    "    run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The knowledge graph should now have relationships\n",
    "kg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Generating personas\n",
    "\n",
    "https://docs.ragas.io/en/latest/howtos/customizations/testgenerator/_persona_generator/\n",
    "\n",
    "- A **persona** is an entity/role which interacts with the system. **Personas** provide context and perspective, ensuring that **generated queries are natural, user-specific, and diverse**.\n",
    "\n",
    "- Example: a Senior DevOps engineer, a Junior Data Scientist, a Marketing Manager in the context of an IT company\n",
    "\n",
    "- **Persona** object consists of a **name** and a **description**.\n",
    "\n",
    "- The name is used to identify the persona and the description is used to describe the role of the persona.\n",
    "\n",
    "- Do note that personas can also be generated by a **knowledge graph** if you have one available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example is taken from `RAGAs`:\n",
    "# https://docs.ragas.io/en/latest/howtos/applications/singlehop_testset_gen/#configuring-personas-for-query-generation\n",
    "\n",
    "persona_first_time_flier = Persona(\n",
    "    name=\"First Time Flier\",\n",
    "    role_description=\"Is flying for the first time and may feel anxious. Needs clear guidance on flight procedures, safety protocols, and what to expect throughout the journey.\",\n",
    ")\n",
    "\n",
    "persona_frequent_flier = Persona(\n",
    "    name=\"Frequent Flier\",\n",
    "    role_description=\"Travels regularly and values efficiency and comfort. Interested in loyalty programs, express services, and a seamless travel experience.\",\n",
    ")\n",
    "\n",
    "persona_angry_business_flier = Persona(\n",
    "    name=\"Angry Business Class Flier\",\n",
    "    role_description=\"Demands top-tier service and is easily irritated by any delays or issues. Expects immediate resolutions and is quick to express frustration if standards are not met.\",\n",
    ")\n",
    "\n",
    "personas: List[Persona] = [\n",
    "    persona_first_time_flier,\n",
    "    persona_frequent_flier,\n",
    "    persona_angry_business_flier\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Generate query types \n",
    "\n",
    "- https://docs.ragas.io/en/latest/concepts/test_data_generation/rag/\n",
    "- https://docs.ragas.io/en/latest/references/synthesizers/\n",
    "\n",
    "- There are two main types of queries in **RAGAs**:\n",
    "    \n",
    "    - **SingleHopQuery** where the **context** relevant for answering a question lies in a **single document/chunk**\n",
    "    - **MultiHopQuery** where the **context** relevant for answering a question lies in **multiple documents/chunks**\n",
    "\n",
    "- Additionally, for each of those queries there's a **Specific** or **Abstract** query variant:\n",
    "    \n",
    "    - **Specific** one which pertains to a **fact**.\n",
    " \n",
    "        - Example: When did WW1 break out? (Can be precisely answered, there's no room for guessing/interpretation)\n",
    "    \n",
    "    - **Abstract** one which is more about testing the **reasoning** capabilities of the LLM. \n",
    "\n",
    "        - Example: Why did WW1 break out? (There's room for interpretation in this case)\n",
    "\n",
    "- **Specific** vs. **Abstract Queries** in a RAG\n",
    "    - Specific Query: Focuses on clear, fact-based retrieval. The goal in RAG is to retrieve highly relevant information from one or more documents that directly address the specific question.\n",
    "    - Abstract Query: Requires a broader, more interpretive response. In RAG, abstract queries challenge the retrieval system to pull from documents that contain higher-level reasoning, explanations, or opinions, rather than simple facts.\n",
    "\n",
    "![Query tpes in RAGAs](../../img/ragas/ragas_query_types.png  \"Queries\")\n",
    "\n",
    "**Synthesizers** are responsible for **converting enriched nodes and personas into queries**. They achieve this by **selecting a node property (e.g., \"entities\" or \"keyphrases\"), pairing it with a persona, style, and query length**, and then using a LLM to generate a query-answer pair based on the content of the node.\n",
    "\n",
    "* Query lengths may vary:\n",
    "    - short\n",
    "    - medium\n",
    "    - long\n",
    "\n",
    "* Query style:\n",
    "    - misspelled\n",
    "    - websearch-like\n",
    "    - perfect-grammar\n",
    "    - poor-grammar\n",
    "\n",
    "Note that **synthesizers** can additionally be extended/modified by specifying custom **prompts**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_hop_specific_entities = SingleHopSpecificQuerySynthesizer(\n",
    "    llm=ragas_llm,\n",
    "    generate_query_reference_prompt=MySingleHopQAGenerationPrompt(),\n",
    "    theme_persona_matching_prompt=MyThemesPersonasMatchingPrompt(),\n",
    "    property_name=\"entities\"\n",
    ")\n",
    "\n",
    "single_hop_specific_keyphrases = SingleHopSpecificQuerySynthesizer(\n",
    "    llm=ragas_llm,\n",
    "    generate_query_reference_prompt=MySingleHopQAGenerationPrompt(),\n",
    "    theme_persona_matching_prompt=MyThemesPersonasMatchingPrompt(),\n",
    "    property_name=\"keyphrases\"\n",
    ")\n",
    "\n",
    "multi_hop_specific_entities = MultiHopSpecificQuerySynthesizer(\n",
    "    llm=ragas_llm,\n",
    "    generate_query_reference_prompt=MyMultiHopQAGenerationPrompt(),\n",
    "    theme_persona_matching_prompt=MyThemesPersonasMatchingPrompt()\n",
    ")\n",
    "\n",
    "multi_hop_specific_keyphrases = MultiHopSpecificQuerySynthesizer(\n",
    "    llm=ragas_llm,\n",
    "    generate_query_reference_prompt=MyMultiHopQAGenerationPrompt(),\n",
    "    relation_type=\"keyphrases_overlap\",\n",
    "    property_name=\"keyphrases\",\n",
    "    theme_persona_matching_prompt=MyThemesPersonasMatchingPrompt()\n",
    ")\n",
    "\n",
    "query_distribution = [\n",
    "    (single_hop_specific_entities, 0.25),\n",
    "    (single_hop_specific_keyphrases, 0.25),\n",
    "    (multi_hop_specific_entities, 0.25),\n",
    "    (multi_hop_specific_keyphrases, 0.25)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Generate the samples\n",
    "\n",
    "- https://docs.ragas.io/en/latest/concepts/components/eval_sample/\n",
    "- https://docs.ragas.io/en/latest/concepts/components/eval_dataset/\n",
    "- https://docs.ragas.io/en/latest/references/generate/\n",
    "\n",
    "#### Definition of evaluation sample\n",
    "\n",
    "An evaluation sample is a single structured data instance that is used to assess and measure the performance of your LLM application in specific scenarios. It represents a single unit of interaction or a specific use case that the AI application is expected to handle. In Ragas, evaluation samples are represented using the `SingleTurnSample` and `MultiTurnSample` classes.\n",
    "\n",
    "#### SingleTurnSample\n",
    "\n",
    "`SingleTurnSample` represents a single-turn interaction between a user, LLM, and expected results for evaluation. It is suitable for evaluations that involve a single question and answer pair, possibly with additional context or reference information.\n",
    "\n",
    "This type of sample is ideal for straightforward question-answering scenarios where a user asks a single question and expects a direct response.\n",
    "\n",
    "![Scenario generation workflow RAGAs](../../img/ragas/scenario_rag.webp \"Scenarios RAGAs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = TestsetGenerator(\n",
    "    ragas_llm,\n",
    "    ragas_embeddings,\n",
    "    kg,\n",
    "    personas\n",
    ")\n",
    "\n",
    "dataset: Testset = generator.generate(\n",
    "    testset_size=50,\n",
    "    query_distribution=query_distribution,\n",
    "    num_personas=len(personas),\n",
    "    run_config=run_config,\n",
    "    with_debugging_logs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Upload to the cloud (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only upload if you have a token, otherwise you will not be authorized\n",
    "if os.getenv(\"RAGAS_APP_TOKEN\"):\n",
    "    dataset.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Fill out the missing fields in the dataset to complete it.\n",
    "\n",
    "- If you are not using [R2R](https://r2r-docs.sciphi.ai/introduction) make sure you configure the parameters and your pipeline as needed.\n",
    "- This notebook assumes you follow along all the steps sequentially and make use of my application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VANILLA_RAG=False would mean RAG-Fusion\n",
    "use_vanilla_rag: bool = bool(\n",
    "    strtobool(os.getenv(\"VANILLA_RAG\"))\n",
    ")\n",
    "\n",
    "if use_vanilla_rag:\n",
    "    search_strategy: str = \"vanilla\"\n",
    "else:\n",
    "    search_strategy: str = \"query_fusion\" # RAG-Fusion\n",
    "\n",
    "# Used after context had been fetched to generate the final response\n",
    "RAG_GENERATION_CONFIG: Final[Dict[str, Union[str, float, int]]] = {\n",
    "    \"model\": f\"ollama_chat/{os.getenv('CHAT_MODEL')}\",\n",
    "    \"temperature\": float(os.getenv(\"TEMPERATURE\")),\n",
    "    \"top_p\": float(os.getenv(\"TOP_P\")),\n",
    "    \"max_tokens_to_sample\": int(os.getenv(\"MAX_TOKENS\"))\n",
    "}\n",
    "\n",
    "# Relevant during the retrieval phase for fetching relevant context\n",
    "# Only cosine similarity is used by this project, however keyword-based search and Graph-RAG are also supported\n",
    "SEARCH_SETTINGS: Final[Dict[str, Union[bool, int, str]]] = {\n",
    "    \"use_semantic_search\": True,\n",
    "    \"limit\": int(os.getenv(\"TOP_K\")),\n",
    "    \"offset\": 0,\n",
    "    \"include_metadatas\": False,\n",
    "    \"include_scores\": True,\n",
    "    \"search_strategy\": search_strategy, # can be vanilla or RAG-fusion, (HyDE is also supported)\n",
    "    \"chunk_settings\": {\n",
    "        \"index_measure\": \"cosine_distance\",\n",
    "        \"enabled\": True,\n",
    "        \"ef_search\": 80\n",
    "    }\n",
    "}\n",
    "\n",
    "if search_strategy == \"query_fusion\":\n",
    "    # This is only relevant when using `hyde` or `rag-fusion`\n",
    "    # Number of hypothetical documents to generate, by default it's 5 if not specified\n",
    "    # https://r2r-docs.sciphi.ai/api-and-sdks/retrieval/rag-app\n",
    "    SEARCH_SETTINGS['num_sub_queries'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can modify this template as needed\n",
    "# This gets submitted to the LLM after context had been fetched and re-ranked\n",
    "TEMPLATE: Final[str] = \"\"\"You are a helpful assistant in a Retrieval-Augmented Generation (RAG) system.\n",
    "Your task is to answer the user's question using *only* the provided context below.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Answer concisely and clearly using only the information from the context.\n",
    "- If the context does not contain sufficient information to answer the question, state that clearly.\n",
    "- DO NOT include citations, references, or mention the context itself.\n",
    "- Do not speculate or make up information beyond what is in the context.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{query}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to authenticate the admin user and receive a token\n",
    "# The username and password are the default ones provided by `R2R`\n",
    "# Note that you can overwrite those in the `config.toml` file\n",
    "# If this fails it means there're either connectivity issues or the credentials are wrong\n",
    "authetication: requests.Response = requests.post(\n",
    "    url=\"http://localhost:7272/v3/users/login\", # This may vary depending on your setup\n",
    "    headers={\n",
    "        \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
    "    },\n",
    "    data=\"username=admin@example.com&password=change_me_immediately\",\n",
    ")\n",
    "token: str = authetication.json()['results']['access_token']['token'] # Token for further authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the IDs of all currently ingested documents\n",
    "documents: requests.Response = requests.get(\n",
    "    url=\"http://localhost:7272/v3/documents\",\n",
    "    headers={\n",
    "        \"Authorization\": f\"Bearer {token}\"\n",
    "    }\n",
    ")\n",
    "\n",
    "doc_ids: List[str] = [document['id'] for document in documents.json()['results']]\n",
    "print(f\"Found {len(doc_ids)} documents\")\n",
    "\n",
    "# Delete all documents available\n",
    "for doc_id in doc_ids:\n",
    "    del_resp: requests.Response = requests.delete(\n",
    "        url=f\"http://localhost:7272/v3/documents/{doc_id}\",\n",
    "        headers={\n",
    "            \"Authorization\": f\"Bearer {token}\"\n",
    "        }\n",
    "    )\n",
    "    if del_resp.status_code == 200:\n",
    "        print(f\"Deleted document with ID: {doc_id}\")\n",
    "    else:\n",
    "        print(f\"Failed to delete document with ID: {doc_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our experiments test different parameters like `chunk size` and `chunk overlap` we need to make sure that documents are re-ingested, so that we properly conduct each and every experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payload for ingestion by unstructured\n",
    "settings_resp: requests.Response = requests.get(\n",
    "    url=\"http://localhost:7272/v3/system/settings\"\n",
    ")\n",
    "ingestion_config: Dict = settings_resp.json()['results']['config']['ingestion']\n",
    "ingestion_config['extra_fields'] = ingestion_config_request\n",
    "\n",
    "print(ingestion_config['extra_fields'])\n",
    "\n",
    "# Files getting ingested and saved into r2r\n",
    "for i, filename in enumerate(os.listdir(\"data\"), 1):\n",
    "    # Skip files that are not markdown or the README file\n",
    "    if not filename.endswith(\".md\") or filename == \"README.md\":\n",
    "        print(f\"[{i}]. Skipping: {filename}\")\n",
    "        continue\n",
    "\n",
    "    file_path: str = os.path.join(\"data\", filename)\n",
    "\n",
    "    # Read the files content\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        markdown_text = f.read()\n",
    "\n",
    "    plaintext: str = markdown_to_plaintext(markdown_text)\n",
    "\n",
    "    # Guess the content type (MIME type) based on file extension\n",
    "    mime_type, _ = mimetypes.guess_type(file_path)\n",
    "    if mime_type is None:\n",
    "        mime_type = \"application/octet-stream\"  # fallback if unknown\n",
    "\n",
    "    # Ingest file - extract text, chunk it, generate embeddings and finally store in vector store\n",
    "    ingestion_resp: requests.Response = requests.post(\n",
    "        url=\"http://localhost:7272/v3/documents\",\n",
    "        headers={\n",
    "            \"Authorization\": f\"Bearer {token}\"\n",
    "        },\n",
    "        files={\n",
    "            \"file\": (filename.replace(\".md\", \".txt\"), plaintext, mime_type)\n",
    "        },\n",
    "        json={\n",
    "            \"metadata\": \"{}\", # Feel free to add your own metadata\n",
    "            \"ingestion_mode\": \"custom\",\n",
    "            \"ingestion_config\": ingestion_config\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if ingestion_resp.status_code == 202:\n",
    "        print(f\"[{i}]. Ingested: {filename}\")\n",
    "    else:\n",
    "        print(f\"[{i}]. Failed to ingest {filename} — {response.status_code}\")\n",
    "        print(ingestion_resp.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_deepseek_response(full_response: str):\n",
    "    \"\"\"\n",
    "    Extract the actual response from deepseek-r1 output by ignoring the <think>...</think> section.\n",
    "    \"\"\"\n",
    "    if \"</think>\" not in full_response:\n",
    "        raise ValueError(\"Response from deepseek-r1 is not full!\")\n",
    "\n",
    "    strings: List[str] = full_response.split(\"</think>\")\n",
    "    answer_without_section: str = strings[-1].lstrip()\n",
    "    return answer_without_section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below actually fills out the rest of the fields - `response` and `retrieved_contexts`.\n",
    "This is where you could use your own pipeline and create synthetic datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some debugging info\n",
    "print(f\"\"\"{'='*80}\\nGenerating dataset in ../datasets/{test_id}_dataset.jsonl\n",
    "TOP_K={int(os.getenv(\"TOP_K\"))}\n",
    "CHUNK_SIZE={int(os.getenv(\"CHUNK_SIZE\"))}\n",
    "CHUNK_OVERLAP={int(os.getenv(\"CHUNK_OVERLAP\"))}\n",
    "CHAT_MODEL={os.getenv(\"CHAT_MODEL\")}\n",
    "VANILLA_RAG={\"True\" if search_strategy == \"vanilla\"else \"False\"}\n",
    "{'='*80}\n",
    "\"\"\")\n",
    "\n",
    "# Filling out the rest of our dataset, for each individual entry\n",
    "# This is where you can employ your own RAG pipeline\n",
    "for i, sample in enumerate(dataset.samples):\n",
    "    # [1] Embed the `user_input`\n",
    "    # [2] Perform semantic similarity search fetching the top-k most relevant contexts\n",
    "    # [3] Re-rank based on relevance relative to `user_input`\n",
    "    context: requests.Response = requests.post(\n",
    "        url=\"http://localhost:7272/v3/retrieval/search\",\n",
    "        headers={\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {token}\"\n",
    "        },\n",
    "        json={\n",
    "            \"query\": sample.eval_sample.user_input,\n",
    "            \"search_settings\": SEARCH_SETTINGS,\n",
    "            \"search_mode\": \"custom\"\n",
    "        }\n",
    "    )\n",
    "    # Extract the relevant context (if any)\n",
    "    retrieved_chunks = [el['text'] for el in context.json()['results']['chunk_search_results'] if el['text']]\n",
    "    \n",
    "    # [4] Use the template defined above and replace placeholders dynamically\n",
    "    user_message: str = TEMPLATE.format(\n",
    "        context=\"\\n\".join(retrieved_chunks),\n",
    "        query=sample.eval_sample.user_input\n",
    "    )\n",
    "    \n",
    "    # [5] Submit the augmented prompt to the LLM\n",
    "    rag_response: requests.Response = requests.post(\n",
    "        url=\"http://localhost:7272/v3/retrieval/completion\",\n",
    "        headers={\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {token}\"\n",
    "        },\n",
    "        json={\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful assistant in a Retrieval-Augmented Generation (RAG) system. Your task is to answer the user's questions using only the context provided.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_message\n",
    "                }\n",
    "            ],\n",
    "            \"generation_config\": RAG_GENERATION_CONFIG\n",
    "        }\n",
    "    )\n",
    "    if rag_response.status_code != 200:\n",
    "        raise Exception(f\"Request failed {rag_response.json()}\")\n",
    "    \n",
    "    # [6] LLM generates the response\n",
    "    response: str = rag_response.json()['results']['choices'][0]['message']['content']\n",
    "    \n",
    "    # If deepseek-r1 is used remove the content between the <think> tags\n",
    "    if \"deepseek-r1\" in os.getenv(\"CHAT_MODEL\"):\n",
    "        response: str = extract_deepseek_response(response)\n",
    "\n",
    "    # [7] Augment dataset\n",
    "    sample.eval_sample.response = response\n",
    "    sample.eval_sample.retrieved_contexts = retrieved_chunks\n",
    "\n",
    "    print(f\"Added data to sample: {i + 1} out of {len(dataset.samples)}\")\n",
    "\n",
    "# Persist the complete dataset\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(\"../datasets\", exist_ok=True)\n",
    "dataset.to_jsonl(path=f\"../datasets/{test_id}_dataset.jsonl\")\n",
    "\n",
    "print(f\"{'='*80}\\nGenerated dataset in ./datasets/{test_id}_dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Synthetic Data Generation completed\n",
    "\n",
    "Congratulations!\n",
    "\n",
    "With this you have created your own synthetic goldens, used various configurations to fill out the rest of the missing fields and saved all the data locally. Now you can move on to the next notebook, check out all the relevant (in my opinion) metrics and evaluate your application using the different experiments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
