{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance of High-Quality Test Dataset for RAG Systems\n",
    "\n",
    "### Purpose\n",
    "Test sets are critical for:\n",
    "- Accurately measuring RAG system performance\n",
    "- Identifying system strengths and weaknesses\n",
    "- Guiding continuous improvement\n",
    "\n",
    "### Key Evaluation Dimensions\n",
    "1. **Retrieval Effectiveness**: Assess how well relevant context is retrieved\n",
    "2. **Generation Quality**: Evaluate the accuracy and coherence of generated responses\n",
    "3. **Contextual Relevance**: Measure how well the system understands and integrates retrieved information\n",
    "\n",
    "### Evaluation Goals\n",
    "- Benchmark system performance\n",
    "- Detect hallucinations\n",
    "- Validate generalization capabilities\n",
    "- Simulate real-world query complexity\n",
    "\n",
    "### Best Practices\n",
    "- Use diverse query types\n",
    "- Cover multiple domains\n",
    "- Include edge cases\n",
    "- Create reproducible test scenarios\n",
    "- Contains enough number of samples to derive statistically significant conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "* Execute the script in the root of the `evaluation` folder.\n",
    "\n",
    "* If executing the script fails run: `chmod u+x setup.sh`.\n",
    "\n",
    "* It will install all required dependencies.\n",
    "\n",
    "* Finally, make sure you select it in the notebook by specifying `eval` as kernel.\n",
    "\n",
    "---\n",
    "\n",
    "*(OPTIONAL STEP)*\n",
    "\n",
    "**RAGAs** provides a cloud platform where a dataset and evaluation results can be stored and viewed. To use it follow this link: [RAGAs.io](https://app.ragas.io/).\n",
    "\n",
    "* Sign-up\n",
    "\n",
    "* Retrieve the **token**\n",
    "\n",
    "* Create a `.env` file with the following content:\n",
    "\n",
    "```bash\n",
    "RAGAS_APP_TOKEN=apt.......-9f6ed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Configuration for generating the goldens\n",
    "\n",
    "Goldens are the samples containing the `reference`, `reference_contexts` and `user_input`. For filling out the rest of the fields: `response` and `retrieved_contexts`, I use various different configurations, which are to be found at the root of the project under *experiments.csv*.\n",
    "\n",
    "The goldens are going to be saved under `./goldens`.\n",
    "\n",
    "Configuration for the generation of *goldens*:\n",
    "\n",
    "* For the generation of the goldens for all experiments I will make use of the following parameters:\n",
    "    \n",
    "    * Generation model - `llama3.1:8b-instruct-q4_1`\n",
    "\n",
    "    * Temperature - `0.0`\n",
    "\n",
    "    * The rest of the parameters will vary depending on the experiments configuration\n",
    "\n",
    "* For filling out the rest of the fields:\n",
    "\n",
    "    * Generation model - depends on the experiment configuration\n",
    "\n",
    "    * Temperature - depends on the experiment configuration\n",
    "\n",
    "    * The rest of the parameters will vary depending on the experiment configuration\n",
    "\n",
    "* The goal is that I make use of the stronger instruction following capabilities of `llama3.1:8b-instruct-q4_1` to try to generate a synthetic dataset, which is as *clean* and *error-free/logic-free* as possible and to then try the **RAG** application with various settings and compare the results.\n",
    "\n",
    "* I think that using the same `model` and `temperature` for the generation of *goldens* would make the experiments fair.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Retrieve data:\n",
    "* The data can be a dataset from `huggingface` or any other platform.\n",
    "\n",
    "* Alternatively, files available on disk - pdf, md, etc.\n",
    "\n",
    "* One can also use `AsyncHtmlLoader` from `langchain` to scrape from the internet.\n",
    "    - **Careful when performing web scraping to not violate any terms and conditions!**\n",
    "\n",
    "**NOTE**:\n",
    "\n",
    "* Make sure you install the requirements first if you want to test the notebook.\n",
    "\n",
    "    * To do so run the `setup.sh` in the parent folder.\n",
    "\n",
    "* Make sure you select the proper environment as your kernel: `eval`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'data' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "# For this notebook I will use a dataset provided by RAGAs.\n",
    "# The dataset contains markdown files about a fictional airline company.\n",
    "! git clone https://huggingface.co/datasets/explodinggradients/ragas-airline-dataset data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load data into document objects\n",
    "\n",
    "For extracting data from documents and splitting them into chunks one can make use of various frameworks:\n",
    "\n",
    "* `langchain` <- The one I use\n",
    "\n",
    "* `llamaindex` <- Another popular solution\n",
    "\n",
    "Both frameworks provide various abstractions to load documents, extract data and split into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Final\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load the RAG parameters\n",
    "load_dotenv(\"../../env/rag.env\")\n",
    "\n",
    "# The path is the folder, where the documents are stored at.\n",
    "# Make sure you select the proper one, if other documents.\n",
    "DIR_PATH: Final[str] = \"data/\"\n",
    "loader = DirectoryLoader(\n",
    "    DIR_PATH,\n",
    "    glob=\"**/*.md\",\n",
    "    exclude=\"README.md\"\n",
    ")\n",
    "\n",
    "# The R2R framework uses a recursive character text splitter to split the documents.\n",
    "# For that purpose I use it to try to mimic the same behavior.\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=int(os.getenv(\"CHUNK_SIZE\")),\n",
    "    chunk_overlap=int(os.getenv(\"CHUNK_OVERLAP\")),\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "docs: list[Document] = loader.load_and_split(splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Construct knowledge graph\n",
    "\n",
    "- A **knowledge graph** is a fundamental concept when it comes to **RAGAs** and using its capabilities for **automatic synthetic data generation**.\n",
    "\n",
    "- A **knowledge graph** consists of **Node**s at first, which represent **documents/chunks** - their content and additionally metadata (optional).\n",
    "\n",
    "- Thereafter, one can enrich the graph by using various **extractors** and applying different **transformations**. Doing so additional attributes get added to the relevant nodes and **relationships can get built**, which express some kind of connection between Node objects. The transformations can be applied only through the use of **Extractor**s, **Splitter**s and or **RelationshipBuilder**s. They serve as a way to gather relevant data from the documents depending on the type of extractor and this way to logically connect 2 or more nodes together.\n",
    "\n",
    "- Finally, the graph is used to generate so called **Scenario**s and can also be used to generate **Persona**s.\n",
    "\n",
    "![Knowledge graph creation workflow RAGAs](../../img/ragas/kg_rag.webp \"Knowledge graph RAGAs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.graph import (\n",
    "    Node,\n",
    "    NodeType,\n",
    "    KnowledgeGraph,\n",
    ")\n",
    "\n",
    "kg = KnowledgeGraph()\n",
    "\n",
    "for doc in docs:\n",
    "    kg.add(\n",
    "        Node(\n",
    "            type=NodeType.CHUNK, # Since we already split the documents, we can use the chunk type.\n",
    "            properties={\"page_content\": doc.page_content, \"document_metadata\": doc.metadata}\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Instantiate required objects\n",
    "\n",
    "- **RAGAs** would require a **LLM** and an **Embedding model** depending on the type of **Transformation**s one would like to apply to the **Knowledge Graph**. For that purpose one must create *wrapper* objects for both of the models. `langchain`, `llama-index`, `haystack`, etc are supported. \n",
    "\n",
    "- Additionally, a **configuration** can be used to modify the default behaviour of the framework. For example timeout values can be modified, maximum retries for failed operations and so on can be configured from the **RunConfig**.\n",
    "    - **NOTE**: depending on the LLM model and GPU you may need to modify the `timeout` value, otherwise you will stumble upon `TimeoutException`\n",
    "\n",
    "- Lastly, there's a single implementation in **RAGAs** for caching intermediate steps onto disk. To use it the **DiskCacheBackend** class can come in play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "from ragas import RunConfig, DiskCacheBackend\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "test_id: str = input(\"Enter the test id (Ex. 1): \")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    timeout=86400,    # 24 hours on waiting for a single operation\n",
    "    max_retries=20,   # Max retries before giving up\n",
    "    max_wait=600,     # Max wait between retries\n",
    "    max_workers=4,    # Concurrent requests\n",
    "    log_tenacity=True # Print retry attempts\n",
    ")\n",
    "\n",
    "# This stores data generation and evaluation results locally on disk\n",
    "# When using it for the first time, it will create a .cache folder\n",
    "# When using it again, it will read from that folder and finish almost instantly\n",
    "cacher = DiskCacheBackend(cache_dir=f\".cache-{test_id}\")\n",
    "\n",
    "ollama_llm = ChatOllama(\n",
    "    model=os.getenv(\"DATA_GENERATION_MODEL\"),\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=float(os.getenv(\"DATA_GENERATION_TEMPERATURE\")),\n",
    "    num_ctx=int(os.getenv(\"LLM_CONTEXT_WINDOW_TOKENS\")),\n",
    "    format=\"json\" # We need to enforce JSON output, since most outputs would be validated by a pydantic model\n",
    ")\n",
    "\n",
    "ollama_embeddings = OllamaEmbeddings(\n",
    "    model=os.getenv(\"EMBEDDING_MODEL\"),\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "ragas_llm = LangchainLLMWrapper(\n",
    "    langchain_llm=ollama_llm,\n",
    "    run_config=run_config,\n",
    "    cache=cacher\n",
    ")\n",
    "\n",
    "ragas_embeddings = LangchainEmbeddingsWrapper(\n",
    "    embeddings=ollama_embeddings,\n",
    "    run_config=run_config,\n",
    "    cache=cacher\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create the transformation pipeline\n",
    "\n",
    "The sequence of transformations:\n",
    "\n",
    "1. Named Entity Recognition (NER) and Keyphrases extraction \n",
    "    - NERExtractor identifies and extracts named entities (e.g., people, organizations, locations).  \n",
    "\n",
    "    - KeyphrasesExtractor extracts the main keyphrases to be found in the text\n",
    "\n",
    "2. NEROverlapBuilder and KeyphraseOverlapBuilder\n",
    "    - Used to establish a relationship between nodes containing similar:\n",
    "        \n",
    "        - entities\n",
    "        - keyphrases\n",
    "\n",
    "3. Parallel Processing for Efficiency:\n",
    "    - Certain transformations can run in parallel to improve performance.\n",
    "\n",
    "- Final Outcome:\n",
    "    - A structured set of document transformations that extract valuable information\n",
    "    - Used to enrich the knowledge graph for further generation of scenarios and finally samples\n",
    "\n",
    "**NOTE:** Some of the extractors *(LLM-based ones)* do receive an optional `prompt`, which one can use to modify the workflow. For instance the `NERExtractor` can receive a custom prompt, which could contain instructions that differ from the original one and extracts entities in a different way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.transforms import (\n",
    "    Parallel,\n",
    "    OverlapScoreBuilder,\n",
    "    KeyphrasesExtractor,\n",
    ")\n",
    "from ragas.testset.transforms.extractors import NERExtractor\n",
    "\n",
    "from prompts.extractors.custom_ner_prompt import MyNERPrompt\n",
    "from prompts.extractors.custom_keyphrases_prompt import MyKeyphrasesExtractorPrompt\n",
    "\n",
    "ner_extractor = NERExtractor(\n",
    "    llm=ragas_llm,\n",
    "    prompt=MyNERPrompt(\n",
    "        name=\"custom_ner_extractor_prompt\"\n",
    "    ),\n",
    "    max_num_entities=15\n",
    ")\n",
    "\n",
    "keyphrases_extractor = KeyphrasesExtractor(\n",
    "    llm=ragas_llm,\n",
    "    prompt=MyKeyphrasesExtractorPrompt(\n",
    "        name=\"custom_keyphrases_extractor_prompt\"\n",
    "    ),\n",
    "    max_num=15\n",
    ")\n",
    "\n",
    "ner_overlap_sim = OverlapScoreBuilder()\n",
    "\n",
    "keyphrases_overlap_sim = OverlapScoreBuilder(\n",
    "    property_name=\"keyphrases\",\n",
    ")\n",
    "\n",
    "transforms = [\n",
    "    Parallel(\n",
    "        ner_extractor,\n",
    "        keyphrases_extractor\n",
    "    ),\n",
    "    Parallel(\n",
    "        ner_overlap_sim,\n",
    "        keyphrases_overlap_sim\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Apply the transformations to the knowledge graph\n",
    "\n",
    "In the cell below the `apply_transforms` is going to apply all the previously defined transformations enriching the `knowledge graph` in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb89ae1f215d40c399d43f75c036bf6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [NERExtractor, KeyphrasesExtractor]:   0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3145f45aff2b4764b8bda65cdb0c5779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [OverlapScoreBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ragas.testset.transforms import apply_transforms\n",
    "\n",
    "apply_transforms(\n",
    "    kg,\n",
    "    transforms,\n",
    "    run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Generating personas\n",
    "\n",
    "- A **Persona** is an entity/role which interacts with the system. **Personas** provide context and perspective, ensuring that **generated queries are natural, user-specific, and diverse**.\n",
    "\n",
    "- Example: a Senior DevOps engineer, a Junior Data Scientist, a Marketing Manager in the context of an IT company\n",
    "\n",
    "- **Persona** object consists of a **name** and a **description**.\n",
    "    \n",
    "    - The name is used to identify the persona and the description is used to describe the role of the persona.\n",
    "\n",
    "- Do note that personas can also be generated by a **knowledge graph** if you have one available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.persona import Persona\n",
    "\n",
    "# This example is taken from `RAGAs`:\n",
    "# https://docs.ragas.io/en/latest/howtos/applications/singlehop_testset_gen/#configuring-personas-for-query-generation\n",
    "\n",
    "persona_first_time_flier = Persona(\n",
    "    name=\"First Time Flier\",\n",
    "    role_description=\"Is flying for the first time and may feel anxious. Needs clear guidance on flight procedures, safety protocols, and what to expect throughout the journey.\",\n",
    ")\n",
    "\n",
    "persona_frequent_flier = Persona(\n",
    "    name=\"Frequent Flier\",\n",
    "    role_description=\"Travels regularly and values efficiency and comfort. Interested in loyalty programs, express services, and a seamless travel experience.\",\n",
    ")\n",
    "\n",
    "persona_angry_business_flier = Persona(\n",
    "    name=\"Angry Business Class Flier\",\n",
    "    role_description=\"Demands top-tier service and is easily irritated by any delays or issues. Expects immediate resolutions and is quick to express frustration if standards are not met.\",\n",
    ")\n",
    "\n",
    "personas: list[Persona] = [\n",
    "    persona_first_time_flier,\n",
    "    persona_frequent_flier,\n",
    "    persona_angry_business_flier\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate query types \n",
    "\n",
    "- There are two main types of queries in **RAGAs**:\n",
    "    \n",
    "    - **SingleHopQuery** where the **context** relevant for answering a question lies in a **single document/chunk**\n",
    "\n",
    "    - **MultiHopQuery** where the **context** relevant for answering a question lies in **multiple documents/chunks**\n",
    "\n",
    "- Additionally, for each of those queries there's a **Specific** or **Abstract** query variant:\n",
    "    \n",
    "    - **Specific** one which pertains to a **fact**. \n",
    "\n",
    "        - Example: When did WW1 break out? (Can be precisely answered, there's no room for guessing/interpretation)\n",
    "    \n",
    "    - **Abstract** one which is more about testing the **reasoning** capabilities of the LLM. \n",
    "\n",
    "        - Example: Why did WW1 break out? (There's room for interpretation in this case)\n",
    "\n",
    "- **Specific** vs. **Abstract Queries** in a RAG\n",
    "    - Specific Query: Focuses on clear, fact-based retrieval. The goal in RAG is to retrieve highly relevant information from one or more documents that directly address the specific question.\n",
    "\n",
    "    - Abstract Query: Requires a broader, more interpretive response. In RAG, abstract queries challenge the retrieval system to pull from documents that contain higher-level reasoning, explanations, or opinions, rather than simple facts.\n",
    "\n",
    "![Query tpes in RAGAs](../../img/ragas/ragas_query_types.png  \"Queries\")\n",
    "\n",
    "**Synthesizers** are responsible for **converting enriched nodes and personas into queries**. They achieve this by **selecting a node property (e.g., \"entities\" or \"keyphrases\"), pairing it with a persona, style, and query length**, and then using a LLM to generate a query-answer pair based on the content of the node.\n",
    "\n",
    "* Query lengths may vary:\n",
    "    - short\n",
    "    - medium\n",
    "    - long\n",
    "\n",
    "* Query style:\n",
    "    - misspelled\n",
    "    - websearch-like\n",
    "    - perfect-grammar\n",
    "    - poor-grammar\n",
    "\n",
    "Note that **synthesizers** can additionally be extended/modified by specifying custom **prompts**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.synthesizers.multi_hop.specific import MultiHopSpecificQuerySynthesizer\n",
    "from ragas.testset.synthesizers.single_hop.specific import SingleHopSpecificQuerySynthesizer\n",
    "\n",
    "from prompts.synthesizers.custom_themes_matching import MyThemesPersonasMatchingPrompt\n",
    "from prompts.synthesizers.custom_multi_hop_qa_generation import MyMultiHopQAGenerationPrompt\n",
    "from prompts.synthesizers.custom_single_hop_qa_generation import MySingleHopQAGenerationPrompt\n",
    "\n",
    "single_hop_specific_entities = SingleHopSpecificQuerySynthesizer(\n",
    "    llm=ragas_llm,\n",
    "    generate_query_reference_prompt=MySingleHopQAGenerationPrompt(),\n",
    "    theme_persona_matching_prompt=MyThemesPersonasMatchingPrompt(),\n",
    "    property_name=\"entities\"\n",
    ")\n",
    "\n",
    "single_hop_specific_keyphrases = SingleHopSpecificQuerySynthesizer(\n",
    "    llm=ragas_llm,\n",
    "    generate_query_reference_prompt=MySingleHopQAGenerationPrompt(),\n",
    "    theme_persona_matching_prompt=MyThemesPersonasMatchingPrompt(),\n",
    "    property_name=\"keyphrases\"\n",
    ")\n",
    "\n",
    "multi_hop_specific_entities = MultiHopSpecificQuerySynthesizer(\n",
    "    llm=ragas_llm,\n",
    "    generate_query_reference_prompt=MyMultiHopQAGenerationPrompt(),\n",
    "    theme_persona_matching_prompt=MyThemesPersonasMatchingPrompt()\n",
    ")\n",
    "\n",
    "multi_hop_specific_keyphrases = MultiHopSpecificQuerySynthesizer(\n",
    "    llm=ragas_llm,\n",
    "    generate_query_reference_prompt=MyMultiHopQAGenerationPrompt(),\n",
    "    relation_type=\"keyphrases_overlap\",\n",
    "    property_name=\"keyphrases\",\n",
    "    theme_persona_matching_prompt=MyThemesPersonasMatchingPrompt()\n",
    ")\n",
    "\n",
    "query_distribution = [\n",
    "    (single_hop_specific_entities, 0.25),\n",
    "    (single_hop_specific_keyphrases, 0.25),\n",
    "    (multi_hop_specific_entities, 0.25),\n",
    "    (multi_hop_specific_keyphrases, 0.25)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Generate the samples\n",
    "\n",
    "#### Definition of evaluation sample\n",
    "\n",
    "An evaluation sample is a single structured data instance that is used to assess and measure the performance of your LLM application in specific scenarios. It represents a single unit of interaction or a specific use case that the AI application is expected to handle. In Ragas, evaluation samples are represented using the `SingleTurnSample` and `MultiTurnSample` classes.\n",
    "\n",
    "#### SingleTurnSample\n",
    "\n",
    "`SingleTurnSample` represents a single-turn interaction between a user, LLM, and expected results for evaluation. It is suitable for evaluations that involve a single question and answer pair, possibly with additional context or reference information.\n",
    "\n",
    "This type of sample is ideal for straightforward question-answering scenarios where a user asks a single question and expects a direct response.\n",
    "\n",
    "#### MultiTurnSample\n",
    "\n",
    "`MultiTurnSample` represents a multi-turn interaction between Human, AI and optionally a Tool and expected results for evaluation. It is suitable for representing conversational agents in more complex interactions for evaluation.\n",
    "\n",
    "In `MultiTurnSample`, the `user_input` attribute represents a sequence of messages that collectively form a multi-turn conversation between a human user and an AI system. These messages are instances of the classes `HumanMessage`, `AIMessage`, and `ToolMessage`.\n",
    "\n",
    "This type of sample is designed for evaluating more complex conversational flows where multiple turns of dialogue occur, potentially involving tool usage for gathering additional information.\n",
    "\n",
    "![Scenario generation workflow RAGAs](../../img/ragas/scenario_rag.webp \"Scenarios RAGAs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e0970145d44b77a5b0f4afeef0a743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Scenarios:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8a9a4039e84c458f4ee8d234d8c5d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Samples:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ragas.testset import TestsetGenerator, Testset\n",
    "\n",
    "generator = TestsetGenerator(\n",
    "    ragas_llm,\n",
    "    ragas_embeddings,\n",
    "    kg,\n",
    "    personas\n",
    ")\n",
    "\n",
    "dataset: Testset = generator.generate(\n",
    "    testset_size=50,\n",
    "    query_distribution=query_distribution,\n",
    "    num_personas=len(personas),\n",
    "    run_config=run_config,\n",
    "    with_debugging_logs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Save dataset containing goldens (no actual output and retrieval context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_jsonl(f\"./goldens/{test_id}_goldens.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Upload to the cloud (Optional)\n",
    "\n",
    "* To upload the data on **app.ragas.io** make sure you:\n",
    "    * First create an account\n",
    "    * Get an **API key**\n",
    "    * Finally, create a `.env` file in the parent folder like so and export it in your notebook:\n",
    "\n",
    "```bash\n",
    "RAGAS_APP_TOKEN=apt.1234a-......-9dfew\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testset uploaded! View at https://app.ragas.io/dashboard/alignment/testset/97c61f25-cde0-456c-b24c-45ebf86369ef\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://app.ragas.io/dashboard/alignment/testset/97c61f25-cde0-456c-b24c-45ebf86369ef'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the token\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "dataset.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Fill out the missing fields in the dataset to complete it.\n",
    "\n",
    "**NOTE**:\n",
    "\n",
    "- Make sure you set the proper values under `env/rag.env` relative to the experiment id you want to carry out.\n",
    "- For each experiment provide an intuitive name to distinguish between the experiments\n",
    "    - In my case I just use the experiment id\n",
    "- If you are not using `R2R` make sure you configure the parameters and your pipeline as needed. This notebook assumes you follow along all the steps sequentially and make use of `R2R`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from distutils.util import strtobool\n",
    "from typing import Dict, Union, Final\n",
    "\n",
    "# VANILLA_RAG=False would mean RAG-Fusion\n",
    "use_vanilla_rag: bool = bool(\n",
    "    strtobool(os.getenv(\"VANILLA_RAG\"))\n",
    ")\n",
    "\n",
    "if use_vanilla_rag:\n",
    "    search_strategy: str = \"vanilla\"\n",
    "else:\n",
    "    search_strategy: str = \"query_fusion\" # RAG-Fusion\n",
    "\n",
    "# Used after context had been fetched to generate the final response\n",
    "RAG_GENERATION_CONFIG: Final[Dict[str, Union[str, float, int]]] = {\n",
    "    \"model\": f\"ollama_chat/{os.getenv('CHAT_MODEL')}\",\n",
    "    \"temperature\": float(os.getenv(\"TEMPERATURE\")),\n",
    "    \"top_p\": float(os.getenv(\"TOP_P\")),\n",
    "    \"max_tokens_to_sample\": int(os.getenv(\"MAX_TOKENS\")),\n",
    "}\n",
    "\n",
    "# Relevant during the retrieval phase for fetching relevant context\n",
    "SEARCH_SETTINGS: Final[Dict[str, Union[bool, int, str]]] = {\n",
    "    \"use_semantic_search\": True,\n",
    "    \"limit\": int(os.getenv(\"TOP_K\")),\n",
    "    \"offset\": 0,\n",
    "    \"include_metadatas\": False,\n",
    "    \"include_scores\": True,\n",
    "    \"search_strategy\": search_strategy, # can be vanilla or hyde, (fusion is also supported)\n",
    "    \"chunk_settings\": {\n",
    "        \"index_measure\": \"cosine_distance\",\n",
    "        \"enabled\": True,\n",
    "        \"ef_search\": 80\n",
    "    }\n",
    "}\n",
    "\n",
    "if search_strategy == \"query_fusion\":\n",
    "    # This is only relevant when using `hyde` or `rag-fusion`\n",
    "    # Number of hypothetical documents to generate, by default it's 5 if not specified\n",
    "    # https://r2r-docs.sciphi.ai/api-and-sdks/retrieval/rag-app\n",
    "    SEARCH_SETTINGS['num_sub_queries'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can modify this template as needed\n",
    "TEMPLATE: Final[str] = \"\"\"You are a helpful RAG assistant.\n",
    "Your task is to provide an answer given a question using the context.\n",
    "Please make sure the answer is complete and relevant to the question.\n",
    "\n",
    "**IMPORTANT:\n",
    "1. BASE YOUR ANSWER ONLY ON THE GIVEN CONTEXT.\n",
    "2. IF THE CONTEXT IS NOT ENOUGH TO ANSWER THE QUESTION, SAY THAT YOU CANNOT ANSWER BASED ON THE AVAILABLE INFORMATION.\n",
    "3. DO NOT INCLUDE CITATIONS OR REFERENCES TO SPECIFIC LINES OR PARTS OF THE CONTEXT.\n",
    "4. ALWAYS KEEP YOUR ANSWER RELEVANT AND FOCUSED ON THE QUESTION.\n",
    "5. DO NOT PROVIDE ANY ADDITIONAL INFORMATION EXCEPT THE ANSWER.\n",
    "**\n",
    "\n",
    "### CONTEXT:\n",
    "{context}\n",
    "\n",
    "### QUESTION:\n",
    "{query}\n",
    "\n",
    "### ANSWER:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# First we need to authenticate the admin user and receive a token\n",
    "# The username and password are the default ones provided by `R2R`\n",
    "# Note that you can overwrite those in the `config.toml` file\n",
    "# If this fails it means there're either connectivity issues or the credentials are wrong\n",
    "authetication: requests.Response = requests.post(\n",
    "    url=\"http://localhost:7272/v3/users/login\", # This may vary depending on your setup\n",
    "    headers={\n",
    "        \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
    "    },\n",
    "    data=\"username=admin@example.com&password=change_me_immediately\",\n",
    ")\n",
    "token: str = authetication.json()['results']['access_token']['token'] # Token for further authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 documents\n",
      "Deleted document with ID: 15bea571-fbd0-5ed6-a1c2-5bb70b6b7f36\n",
      "Deleted document with ID: db0faff8-f57f-5459-91f4-92f611106fa1\n",
      "Deleted document with ID: 6530a034-daa1-5198-b340-563b5e45ca2b\n",
      "Deleted document with ID: a5ef7ed6-02c2-5a86-9299-fff661c1e7f7\n",
      "Deleted document with ID: 2a9978ac-84fd-5644-8633-dcc90e19c123\n",
      "Deleted document with ID: d7c24a75-99ba-5b84-8339-5a9188be0580\n",
      "Deleted document with ID: bf55e614-3330-5283-a759-ea1bfa15a655\n",
      "Deleted document with ID: 88fa13ca-5921-590f-8693-408b1ed047bf\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "# Retrieve the IDs of all currently ingested documents\n",
    "documents: requests.Response = requests.get(\n",
    "    url=\"http://localhost:7272/v3/documents\",\n",
    "    headers={\n",
    "        \"Authorization\": f\"Bearer {token}\"\n",
    "    }\n",
    ")\n",
    "\n",
    "doc_ids: List[str] = [document['id'] for document in documents.json()['results']]\n",
    "print(f\"Found {len(doc_ids)} documents\")\n",
    "\n",
    "# Delete all documents available\n",
    "for doc_id in doc_ids:\n",
    "    del_resp: requests.Response = requests.delete(\n",
    "        url=f\"http://localhost:7272/v3/documents/{doc_id}\",\n",
    "        headers={\n",
    "            \"Authorization\": f\"Bearer {token}\"\n",
    "        }\n",
    "    )\n",
    "    if del_resp.status_code == 200:\n",
    "        print(f\"Deleted document with ID: {doc_id}\")\n",
    "    else:\n",
    "        print(f\"Failed to delete document with ID: {doc_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]. Ingested: special_assistance.md\n",
      "[2]. Ingested: managing_reservations.md\n",
      "[3]. Ingested: flight_delays.md\n",
      "[4]. Ingested: baggage_policies.md\n",
      "[5]. Ingested: inflight_services.md\n",
      "[6]. Ingested: schedule_changes.md\n",
      "[7]. Ingested: bookings.md\n",
      "[8]. Ingested: flight_cancellations.md\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import mimetypes\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "# Load files\n",
    "loader = DirectoryLoader(\n",
    "    \"./data\", # The folder, where the documents are stored at.\n",
    "    glob=\"**/*.md\",\n",
    "    exclude=\"README.md\"\n",
    ")\n",
    "docs: list[Document] = loader.load()\n",
    "\n",
    "# Clean-up markdown\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    for doc in docs:\n",
    "        doc_filepath: str = doc.metadata['source'].split(\"/\")[-1]\n",
    "        temp_file_path = os.path.join(temp_dir, doc_filepath)\n",
    "        with open(temp_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(doc.page_content)\n",
    "\n",
    "    # Ingest individual files, on every run so that chunk size and chunk overlap match the experiment config\n",
    "    # If working with another dataset modify as required\n",
    "    for i, file in enumerate(os.listdir(temp_dir), 1):\n",
    "        if not file.endswith(\".md\"):\n",
    "            continue\n",
    "\n",
    "        filepath = os.path.join(temp_dir, file)\n",
    "\n",
    "        # Guess the content type (MIME type) based on file extension\n",
    "        mime_type, _ = mimetypes.guess_type(filepath)\n",
    "        if mime_type is None:\n",
    "            mime_type = \"application/octet-stream\"  # fallback if unknown\n",
    "\n",
    "        with open(filepath, \"rb\") as content:\n",
    "            # Ingest file - extract text, chunk it, generate embeddings and finally store in vector store\n",
    "            ingestion_resp: requests.Response = requests.post(\n",
    "                url=\"http://localhost:7272/v3/documents\",\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Bearer {token}\"\n",
    "                },\n",
    "                files={\n",
    "                    \"file\": (file, content, mime_type)\n",
    "                },\n",
    "                data={\n",
    "                    \"metadata\": \"{}\", # Feel free to add your own metadata\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            if ingestion_resp.status_code == 202:\n",
    "                print(f\"[{i}]. Ingested: {file}\")\n",
    "            else:\n",
    "                print(ingestion_resp.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_goldens(filepath: str) -> List[Dict]:\n",
    "    \"\"\"Loads the synthetically generated goldens from a JSONL file.\"\"\"\n",
    "    _goldens: List[Dict] = []\n",
    "    try:\n",
    "        with open(file=f\"./goldens/{filepath}.jsonl\", mode=\"r\", encoding=\"utf-8\") as file:\n",
    "            # Read the file line by line and parse each line as JSON\n",
    "            for line in file:\n",
    "                if line.strip():  # Skip empty lines\n",
    "                    _goldens.append(json.loads(line))\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"File `./goldens/{filepath}.jsonl` containing goldens not found.\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise json.JSONDecodeError from e\n",
    "\n",
    "    return _goldens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_deepseek_response(full_response: str):\n",
    "    \"\"\"\n",
    "    Extract the actual response from deepseek-r1 output by ignoring the <think>...</think> section.\n",
    "    \"\"\"\n",
    "    if \"</think>\" not in full_response:\n",
    "        raise ValueError(\"Response from deepseek-r1 is not full!\")\n",
    "\n",
    "    strings: List[str] = full_response.split(\"</think>\")\n",
    "    answer_without_section: str = strings[-1].lstrip()\n",
    "    return answer_without_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Generating dataset in ./datasets/1_dataset.jsonl\n",
      "TOP_K=3\n",
      "MAX_TOKENS_TO_SAMPLE=512\n",
      "CHUNK_SIZE=512\n",
      "CHUNK_OVERLAP=0\n",
      "CHAT_MODEL=llama3.1:8b\n",
      "TEMPERATURE=0.0\n",
      "VANILLA_RAG=True\n",
      "================================================================================\n",
      "\n",
      "Added data to sample: 1 out of 52\n",
      "Added data to sample: 2 out of 52\n",
      "Added data to sample: 3 out of 52\n",
      "Added data to sample: 4 out of 52\n",
      "Added data to sample: 5 out of 52\n",
      "Added data to sample: 6 out of 52\n",
      "Added data to sample: 7 out of 52\n",
      "Added data to sample: 8 out of 52\n",
      "Added data to sample: 9 out of 52\n",
      "Added data to sample: 10 out of 52\n",
      "Added data to sample: 11 out of 52\n",
      "Added data to sample: 12 out of 52\n",
      "Added data to sample: 13 out of 52\n",
      "Added data to sample: 14 out of 52\n",
      "Added data to sample: 15 out of 52\n",
      "Added data to sample: 16 out of 52\n",
      "Added data to sample: 17 out of 52\n",
      "Added data to sample: 18 out of 52\n",
      "Added data to sample: 19 out of 52\n",
      "Added data to sample: 20 out of 52\n",
      "Added data to sample: 21 out of 52\n",
      "Added data to sample: 22 out of 52\n",
      "Added data to sample: 23 out of 52\n",
      "Added data to sample: 24 out of 52\n",
      "Added data to sample: 25 out of 52\n",
      "Added data to sample: 26 out of 52\n",
      "Added data to sample: 27 out of 52\n",
      "Added data to sample: 28 out of 52\n",
      "Added data to sample: 29 out of 52\n",
      "Added data to sample: 30 out of 52\n",
      "Added data to sample: 31 out of 52\n",
      "Added data to sample: 32 out of 52\n",
      "Added data to sample: 33 out of 52\n",
      "Added data to sample: 34 out of 52\n",
      "Added data to sample: 35 out of 52\n",
      "Added data to sample: 36 out of 52\n",
      "Added data to sample: 37 out of 52\n",
      "Added data to sample: 38 out of 52\n",
      "Added data to sample: 39 out of 52\n",
      "Added data to sample: 40 out of 52\n",
      "Added data to sample: 41 out of 52\n",
      "Added data to sample: 42 out of 52\n",
      "Added data to sample: 43 out of 52\n",
      "Added data to sample: 44 out of 52\n",
      "Added data to sample: 45 out of 52\n",
      "Added data to sample: 46 out of 52\n",
      "Added data to sample: 47 out of 52\n",
      "Added data to sample: 48 out of 52\n",
      "Added data to sample: 49 out of 52\n",
      "Added data to sample: 50 out of 52\n",
      "Added data to sample: 51 out of 52\n",
      "Added data to sample: 52 out of 52\n",
      "================================================================================\n",
      "Generated dataset in ./datasets/1_dataset.jsonl\n"
     ]
    }
   ],
   "source": [
    "goldens_filepath: str = f\"{test_id}_goldens\" # This depends on how you name your goldens\n",
    "\n",
    "# Some debugging info\n",
    "print(f\"\"\"{'='*80}\\nGenerating dataset in ./datasets/{test_id}_dataset.jsonl\n",
    "TOP_K={int(os.getenv(\"TOP_K\"))}\n",
    "MAX_TOKENS_TO_SAMPLE={int(os.getenv(\"MAX_TOKENS\"))}\n",
    "CHUNK_SIZE={int(os.getenv(\"CHUNK_SIZE\"))}\n",
    "CHUNK_OVERLAP={int(os.getenv(\"CHUNK_OVERLAP\"))}\n",
    "CHAT_MODEL={os.getenv(\"CHAT_MODEL\")}\n",
    "TEMPERATURE={float(os.getenv(\"TEMPERATURE\"))}\n",
    "VANILLA_RAG={\"True\" if search_strategy == \"vanilla\"else \"False\"}\n",
    "{'='*80}\n",
    "\"\"\")\n",
    "\n",
    "goldens: List[Dict] = load_goldens(goldens_filepath)\n",
    "\n",
    "# Filling out the rest of our dataset, for each individual entry\n",
    "for i, golden in enumerate(goldens):\n",
    "    # [1] Embed the `user_input`\n",
    "    # [2] Perform semantic similarity search fetching the top-k most relevant contexts\n",
    "    # [3] Re-rank based on relevance relative to `user_input`\n",
    "    # [4] Use the template defined above and replace placeholders dynamically\n",
    "    # [5] Submit the augmented prompt to the LLM\n",
    "    # [6] LLM generates the response and returns an object containing it and the context\n",
    "    rag_response: requests.Response = requests.post(\n",
    "        url=\"http://localhost:7272/v3/retrieval/rag\",\n",
    "        headers={\n",
    "            \"Authorization\": f\"Bearer {token}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        },\n",
    "        json={\n",
    "            \"query\": golden[\"user_input\"], # Submit query from synthetically generated goldens\n",
    "            \"rag_generation_config\": RAG_GENERATION_CONFIG,\n",
    "            \"search_mode\": \"custom\",\n",
    "            \"search_settings\": SEARCH_SETTINGS,\n",
    "            \"task_prompt\": TEMPLATE\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if rag_response.status_code != 200:\n",
    "        raise Exception(f\"Request failed {rag_response.json()}\")\n",
    "    \n",
    "    response: Dict = rag_response.json()['results']\n",
    "\n",
    "    # Get the LLM response and context\n",
    "    actual_output: str = response['completion']\n",
    "    retrieved_contexts: List[str] = [\n",
    "        chunk['text']\n",
    "        for chunk in response['search_results']['chunk_search_results']\n",
    "    ]\n",
    "\n",
    "    # If deepseek-r1 is used regardless of parameters count\n",
    "    # remove the content between the <think> tags\n",
    "    if \"deepseek-r1\" in os.getenv(\"CHAT_MODEL\"):\n",
    "        actual_output = extract_deepseek_response(actual_output)\n",
    "\n",
    "    # Fill out the rest of your dataset\n",
    "    golden[\"response\"] = actual_output\n",
    "    golden[\"retrieved_contexts\"] = retrieved_contexts\n",
    "\n",
    "    print(f\"Added data to sample: {i + 1} out of {len(goldens)}\")\n",
    "\n",
    "# Persist the complete dataset\n",
    "os.makedirs(\"./datasets\", exist_ok=True)  # Create the directory if it doesn't exist\n",
    "with open(file=f\"./datasets/{test_id}_dataset.jsonl\", mode=\"w\", encoding=\"utf-8\") as f:\n",
    "    for golden in goldens:\n",
    "        f.write(json.dumps(golden, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"{'='*80}\\nGenerated dataset in ./datasets/{test_id}_dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Synthetic Data Generation completed\n",
    "\n",
    "Congratulations!\n",
    "\n",
    "With this you have created your own synthetic goldens, used various configurations to fill out the rest of the missing fields and saved all the data locally. Now you can move on to the next notebook, check out all the relevant (in my opinion) metrics and evaluate your application using the different experiments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
