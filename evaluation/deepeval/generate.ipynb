{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a synthetic dataset using DeepEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install ollama requests dotenv psycopg2-binary pgvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from ollama import EmbeddingsResponse\n",
    "\n",
    "def generate_embedding(query: str) -> str:\n",
    "    result: EmbeddingsResponse = ollama.embeddings(\n",
    "        model=\"mxbai-embed-large\",\n",
    "        prompt=query\n",
    "    )\n",
    "    return result.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' AI methodologies, such as fine-tuning and reinforcement learning, has further expanded its capabilities. Despite the progress in RAG technology, there are research opportunities to improve its robustness and its ability to handle extended contexts. RAG‚Äôs application scope is expanding into multimodal do- mains, adapting its principles to interpret and process diverse data forms like images, videos, and code. This expansion high- lights RAG‚Äôs significant practical implications for AI deploy- ment, attracting interest from academic and industrial sectors.\\n\\n16\\n\\nThe growing ecosystem of RAG is evidenced by the rise in RAG-centric AI applications and the continuous development of supportive tools. As RAG‚Äôs application landscape broadens, there is a need to refine evaluation methodologies to keep pace with its evolution. Ensuring accurate and representative performance assessments is crucial for fully capturing RAG‚Äôs contributions to the AI research and development community.\\n\\nREFERENCES\\n\\n[1] N. Kandpal, H. Deng, A. Roberts, E. Wallace, and C. Raffel, ‚ÄúLarge language models struggle to learn long-tail knowledge,‚Äù in Interna- tional Conference on Machine Learning. PMLR, 2023, pp. 15696‚Äì 15707.\\n\\n[2] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao, Y. Zhang, Y. Chen et al., ‚ÄúSiren‚Äôs song in the ai ocean: A survey on hal- lucination in large language models,‚Äù arXiv preprint arXiv:2309.01219, 2023.\\n\\n[3] D. Arora, A. Kini, S. R. Chowdhury, N. Natarajan, G. Sinha, and A. Sharma, ‚ÄúGar-meets-rag paradigm for zero-shot information re- trieval,‚Äù arXiv preprint arXiv:2310.20158, 2023.\\n\\n[4] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. K¬®uttler, M. Lewis, W.-t. Yih, T. Rockt¬®aschel et al., ‚ÄúRetrieval- augmented generation for knowledge-intensive nlp tasks,‚Äù Advances in Neural Information Processing Systems, vol. 33, pp. 9459‚Äì9474, 2020. [5] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Milli- can, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark et al., ‚ÄúImproving language models by retrieving from trillions of tokens,‚Äù in International conference on machine learning. PMLR, 2022, pp. 2206‚Äì2240.\\n\\n[6] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al., ‚ÄúTraining language models to follow instructions with human feedback,‚Äù Advances in neural information processing systems, vol. 35, pp. 27730‚Äì27744, 2022.\\n\\n[7] X. Ma, Y. Gong, P. He, H. Zhao, and N. Duan, ‚ÄúQuery rewrit- ing for retrieval-augmented large language models,‚Äù arXiv preprint arXiv:2305.14283, 2023.\\n\\n[8] I.\\n\\nILIN,\\n\\n‚ÄúAdvanced\\n\\nrag\\n\\nil- https://pub.towardsai.net/\\n\\ntechniques:\\n\\nan\\n\\nlustrated advanced-rag-techniques-an-illustrated-overview-04d193d8fec6, 2023.\\n\\noverview,‚Äù\\n\\n[9] W. Peng, G. Li, Y. Jiang, Z. Wang, D. Ou, X. Zeng, E. Chen et al., ‚ÄúLarge language model based long-tail query rewriting in taobao search,‚Äù arXiv preprint arXiv:2311.03758, 2023.\\n\\n[10] H. S. Zheng, S. Mishra, X. Chen, H.-T. Cheng, E. H. Chi, Q. V. Le, and D. Zhou, ‚ÄúTake a step back: Evoking reasoning via abstraction in large language models,‚Äù arXiv pre',)\n",
      "(' addressed [175].\\n\\nThe development of the RAG ecosystem is greatly impacted by the progression of its technical stack. Key tools like LangChain and LLamaIndex have quickly gained popularity with the emergence of ChatGPT, providing extensive RAG- related APIs and becoming essential in the realm of LLMs.The emerging technology stack, while not as rich in features as LangChain and LLamaIndex, stands out through its specialized products. For example, Flowise AI prioritizes a low-code approach, allowing users to deploy AI applications, including RAG, through a user-friendly drag-and-drop interface. Other technologies like HayStack, Meltano, and Cohere Coral are also gaining attention for their unique contributions to the field. In addition to AI-focused vendors, traditional software and cloud service providers are expanding their offerings to include RAG-centric services. Weaviate‚Äôs Verba 11 is designed for personal assistant applications, while Amazon‚Äôs Kendra 12 offers intelligent enterprise search services, enabling users to browse various content repositories using built-in connectors. there is a clear In the development of RAG technology, trend towards different specialization directions, such as: 1) Customization - tailoring RAG to meet specific requirements. 2) Simplification - making RAG easier to use to reduce the\\n\\n10https://github.com/inverse-scaling/prize\\n\\n11https://github.com/weaviate/Verba 12https://aws.amazon.com/cn/kendra/\\n\\n15\\n\\nFig. 6. Summary of RAG ecosystem\\n\\ninitial learning curve. 3) Specialization - optimizing RAG to better serve production environments.\\n\\nThe mutual growth of RAG models and their technology stacks is evident; technological advancements continuously establish new standards for existing infrastructure. In turn, enhancements to the technology stack drive the development of RAG capabilities. RAG toolkits are converging into a foundational laying the groundwork for advanced enterprise applications. However, a fully integrated, comprehensive platform concept is still in the future, requiring further innovation and development.\\n\\ntechnology stack,\\n\\nF. Multi-modal RAG\\n\\ntext-based question- answering confines, embracing a diverse array of modal data. This expansion has spawned innovative multimodal models that integrate RAG concepts across various domains:\\n\\nRAG has\\n\\ntranscended its\\n\\ninitial\\n\\nImage. RA-CM3 [176] stands as a pioneering multimodal model of both retrieving and generating text and images. BLIP-2 [177] leverages frozen image encoders alongside LLMs for efficient visual language pre-training, enabling zero- shot image-to-text conversions. The ‚ÄúVisualize Before You Write‚Äù method [178] employs image generation to steer the LM‚Äôs text generation, showing promise in open-ended text generation tasks.\\n\\nAudio and Video. The GSS method retrieves and stitches together audio clips to convert machine-translated data into speech-translated data [179]. UEOP marks a significant ad- vancement in end-to-end automatic speech recognition by incorporating external, offline strategies for voice-to-text con- version [180]. Additionally, KNN-based attention fusion lever- ages audio embeddings and semantically related text embed- dings to refine ASR, thereby accelerating domain adaptation.\\n\\nVid2Seq augments language models with specialized temporal markers, facilitating the prediction of event boundaries and textual descriptions within a unified output sequence [181].\\n\\nCode. RBPS [182] excels in small-scale learning tasks by retrieving code examples that align with developers‚Äô objectives through encoding and frequency analysis. This approach has demonstrated efficacy in tasks such as test assertion genera- tion and program repair. For structured knowledge, the CoK method [106] first extracts facts pertinent to the input query from a knowledge graph, then integrates these facts as hints within the input, enhancing performance in knowledge graph question-answering tasks.\\n\\nVIII. CONCLUSION\\n\\nThe summary of this paper, as depicted in Figure 6, empha- sizes RAG‚Äôs significant advancement in enhancing the capa- bilities of LLMs by integrating parameterized knowledge from language models with extensive non-parameterized data from external knowledge bases. The survey showcases the evolution of RAG technologies and their application on many different tasks. The analysis outlines three developmental paradigms within the RAG framework: Naive, Advanced, and Modu- lar RAG, each representing a progressive enhancement over its predecessors. RAG‚Äôs technical integration with other AI methodologies, such as fine-tuning and reinforcement learning, has further expanded its capabilities. Despite the progress in RAG technology, there are research opportunities to improve its robustness and its ability to handle extended contexts. RAG‚Äôs application scope is expanding into multimodal do- mains, adapting',)\n",
      "(' [21] AAR [47] RA-DIT [27] RAG-Robust [48] RA-Long-Form [49] CoN [50] Self-RAG [25] BGM [26] CoQ [51] Token-Elimination [52] PaperQA [53] NoiseRAG [54] IAG [55] NoMIRACL [56] ToC [57] SKR [58] ITRG [59] RAG-LongContext [60] ITER-RETGEN [14] IRCoT [61] LLM-Knowledge-Boundary [62] RAPTOR [63] RECITE [22] ICRALM [64] Retrieve-and-Sample [65] Zemi [66] CRAG [67] 1-PAGER [68] PRCA [69] QLM-Doc-ranking [70] Recomp [71] DSP [23] RePLUG [72] ARM-RAG [73] GenRead [13] UniMS-RAG [74] CREA-ICL [19] PKG [75] SANTA [76] SURGE [77] MK-ToD [78] Dual-Feedback-ToD [79] KnowledGPT [15] FABULA [80] HyKGE [81] KALMV [82] RoG [83] G-Retriever [84]\\n\\nTABLE I SUMMARY OF RAG METHODS\\n\\nRetrieval Source\\n\\nRetrieval Data Type\\n\\nWikipedia FactoidWiki Dataset-base Dataset-base Dataset-base Dataset-base Search Engine,Wikipedia Wikipedia Wikipedia Dataset-base Synthesized dataset Dataset-base Dataset-base Dataset-base Dataset-base Dataset-base Synthesized dataset Wikipedia, Common Crawl Wikipedia Pre-training Corpus Pre-training corpus Search Engine Dataset-base BEIR MSMARCO,Wikipedia Common Crawl,Wikipedia Wikipedia Dataset-base Wikipedia Wikipedia Wikipedia Wikipedia Wikipedia Arxiv,Online Database,PubMed FactoidWiki Search Engine,Wikipedia Wikipedia Search Engine,Wikipedia Dataset-base,Wikipedia Wikipedia Dataset-base Wikipedia Wikipedia Wikipedia Dataset-base LLMs Pile,Wikipedia Dataset-base C4 Arxiv Wikipedia Dataset-base Dataset-base Wikipedia Wikipedia Pile Dataset-base LLMs Dataset-base Dataset-base LLM Dataset-base Freebase Dataset-base Dataset-base Dataset-base Dataset-base,Graph CMeKG Wikipedia Freebase Dataset-base\\n\\nText Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Crosslingual,Text Tabular,Text Code,Text KG KG KG KG KG KG KG KG TextGraph\\n\\nRetrieval Granularity\\n\\nPhrase Proposition Sentence Sentence Sentence Sentence Sentence Sentence Sentence Sentence Sentence Sentence Pair Sentence Pair Item-base Item-base Item-base Item-base Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Doc Doc Doc Doc Doc Doc Doc Doc Doc Doc Doc Multi Sentence Chunk Item Sub-Graph Entity Entity Sequence Triplet Entity Entity Triplet Triplet Sub-Graph\\n\\nAugmentation Stage\\n\\nPre-training Inference Tuning Tuning Tuning Tuning Tuning Inference Inference Inference Inference Tuning Inference Pre-training Tuning Tuning Tuning Pre-training Pre-training Pre-training Pre-training Tuning Tuning Tuning Tuning Tuning Tuning Tuning Tuning Tuning Inference Inference Inference Inference Inference Inference Inference Inference Inference Inference Inference Inference Inference Inference Inference Inference Inference Tuning Tuning Inference Inference Inference Inference Inference Inference Inference Inference Inference Tuning Inference Inference Pre-training Tuning Tuning Tuning Inference Inference Inference Inference Inference Inference\\n\\nRetrieval process\\n\\nIterative Once Once Once Once Iterative Adaptive Once Once Once Once Once Iterative Once Once Once Once Iterative Once Iterative Iterative Once Once Once Once Once Once Once Once Adaptive Once Iterative Once Iterative Once Once Once Recursive Adapt',)\n",
      "('com/huggingface/transformers/blob/master/ examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/\\n\\n2\\n\\nby Œ∏ that generates a current token based on a context of the previous i ‚àí 1 tokens y1:i‚àí1, the original input x and a retrieved passage z.\\n\\nTo train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the pŒ∑ and pŒ∏ components, as well as the training and decoding procedure.\\n\\n2.1 Models\\n\\nRAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized,\\n\\npRAG-Sequence(y|x) ‚âà\\n\\n(cid:88)\\n\\npŒ∑(z|x)pŒ∏(y|x,z) =\\n\\n(cid:88)\\n\\npŒ∑(z|x)\\n\\nN (cid:89)\\n\\npŒ∏(yi|x,z,y1:i‚àí1)\\n\\nz‚ààtop-k(p(¬∑|x))\\n\\nz‚ààtop-k(p(¬∑|x))\\n\\ni\\n\\nRAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deÔ¨Åne:\\n\\npRAG-Token(y|x) ‚âà\\n\\nN (cid:89)\\n\\n(cid:88)\\n\\npŒ∑(z|x)pŒ∏(yi|x,z,y1:i‚àí1)\\n\\ni\\n\\nz‚ààtop-k(p(¬∑|x))\\n\\nFinally, we note that RAG can be used for sequence classiÔ¨Åcation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.\\n\\n2.2 Retriever: DPR\\n\\nThe retrieval component pŒ∑(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:\\n\\npŒ∑(z|x) ‚àù exp(cid:0)d(z)(cid:62)q(x)(cid:1)\\n\\nd(z) = BERTd(z), q(x) = BERTq(x)\\n\\nwhere d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pŒ∑(¬∑|x)), the list of k documents z with highest prior probability pŒ∑(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory.\\n\\n2.3 Generator: BART\\n\\nThe generator component pŒ∏(yi|x,z,y1:i‚àí1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising',)\n",
      "(' include query rewriting query transformation, query expansion and other techniques [7], [9]‚Äì[11].\\n\\nPost-Retrieval Process. Once relevant context is retrieved, it‚Äôs crucial to integrate it effectively with the query. The main methods in post-retrieval process include rerank chunks and context compressing. Re-ranking the retrieved information to relocate the most relevant content to the edges of the prompt is a key strategy. This concept has been implemented in frame- works such as LlamaIndex2, LangChain3, and HayStack [12]. Feeding all relevant documents directly into LLMs can lead to information overload, diluting the focus on key details with irrelevant content.To mitigate this, post-retrieval efforts con- centrate on selecting the essential information, emphasizing critical sections, and shortening the context to be processed.\\n\\n2https://www.llamaindex.ai 3https://www.langchain.com/\\n\\nC. Modular RAG\\n\\nThe modular RAG architecture advances beyond the for- mer two RAG paradigms, offering enhanced adaptability and versatility. It incorporates diverse strategies for improving its components, such as adding a search module for similarity searches and refining the retriever through fine-tuning. Inno- vations like restructured RAG modules [13] and rearranged RAG pipelines [14] have been introduced to tackle specific challenges. The shift towards a modular RAG approach is becoming prevalent, supporting both sequential processing and integrated end-to-end training across its components. Despite its distinctiveness, Modular RAG builds upon the foundational principles of Advanced and Naive RAG, illustrating a progres- sion and refinement within the RAG family.\\n\\n1) New Modules: The Modular RAG framework introduces additional specialized components to enhance retrieval and processing capabilities. The Search module adapts to spe- cific scenarios, enabling direct searches across various data sources like search engines, databases, and knowledge graphs, using LLM-generated code and query languages [15]. RAG- Fusion addresses traditional search limitations by employing a multi-query strategy that expands user queries into diverse perspectives, utilizing parallel vector searches and intelligent re-ranking to uncover both explicit and transformative knowl- edge [16]. The Memory module leverages the LLM‚Äôs memory to guide retrieval, creating an unbounded memory pool that\\n\\n4\\n\\naligns the text more closely with data distribution through iter- ative self-enhancement [17], [18]. Routing in the RAG system navigates through diverse data sources, selecting the optimal pathway for a query, whether it involves summarization, specific database searches, or merging different information streams [19]. The Predict module aims to reduce redundancy and noise by generating context directly through the LLM, ensuring relevance and accuracy [13]. Lastly, the Task Adapter module tailors RAG to various downstream tasks, automating prompt retrieval for zero-shot inputs and creating task-specific retrievers through few-shot query generation [20], [21] .This comprehensive approach not only streamlines the retrieval pro- cess but also significantly improves the quality and relevance of the information retrieved, catering to a wide array of tasks and queries with enhanced precision and flexibility.\\n\\n2) New Patterns: Modular RAG offers remarkable adapt- ability by allowing module substitution or reconfiguration to address specific challenges. This goes beyond the fixed structures of Naive and Advanced RAG, characterized by a simple ‚ÄúRetrieve‚Äù and ‚ÄúRead‚Äù mechanism. Moreover, Modular RAG expands this flexibility by integrating new modules or adjusting interaction flow among existing ones, enhancing its applicability across different tasks. Innovations such as the Rewrite-Retrieve-Read [7]model leverage the LLM‚Äôs capabilities to refine retrieval queries through a rewriting module and a LM-feedback mechanism to update rewriting model., improving task performance. Similarly, approaches like Generate-Read [13] replace tradi- tional retrieval with LLM-generated content, while Recite- Read [22] emphasizes retrieval from model weights, enhanc- ing the model‚Äôs ability to handle knowledge-intensive tasks. Hybrid retrieval strategies integrate keyword, semantic, and vector searches to cater to diverse queries. Additionally, em- ploying sub-queries and hypothetical document embeddings (HyDE) [11] seeks to improve retrieval relevance by focusing on embedding similarities between generated answers and real documents.\\n\\nAdjustments in module arrangement and interaction, such as the Demonstrate-Search-Predict (DSP) [23] framework and the iterative Retrieve-Read-Retrieve-Read flow of ITER- RETGEN [14], showcase the dynamic use of module out- puts to bolster another module‚Äôs functionality, illustrating a sophisticated understanding of enhancing module synergy. The flexible orchestration of Modular RAG Flow showcases the benefits of adaptive retrieval through techniques such as FL',)\n",
      "('com/huggingface/transformers/blob/master/ examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/\\n\\n2\\n\\nby Œ∏ that generates a current token based on a context of the previous i ‚àí 1 tokens y1:i‚àí1, the original input x and a retrieved passage z.\\n\\nTo train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the pŒ∑ and pŒ∏ components, as well as the training and decoding procedure.\\n\\n2.1 Models\\n\\nRAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized,\\n\\npRAG-Sequence(y|x) ‚âà\\n\\n(cid:88)\\n\\npŒ∑(z|x)pŒ∏(y|x,z) =\\n\\n(cid:88)\\n\\npŒ∑(z|x)\\n\\nN (cid:89)\\n\\npŒ∏(yi|x,z,y1:i‚àí1)\\n\\nz‚ààtop-k(p(¬∑|x))\\n\\nz‚ààtop-k(p(¬∑|x))\\n\\ni\\n\\nRAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deÔ¨Åne:\\n\\npRAG-Token(y|x) ‚âà\\n\\nN (cid:89)\\n\\n(cid:88)\\n\\npŒ∑(z|x)pŒ∏(yi|x,z,y1:i‚àí1)\\n\\ni\\n\\nz‚ààtop-k(p(¬∑|x))\\n\\nFinally, we note that RAG can be used for sequence classiÔ¨Åcation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.\\n\\n2.2 Retriever: DPR\\n\\nThe retrieval component pŒ∑(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:\\n\\npŒ∑(z|x) ‚àù exp(cid:0)d(z)(cid:62)q(x)(cid:1)\\n\\nd(z) = BERTd(z), q(x) = BERTq(x)\\n\\nwhere d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pŒ∑(¬∑|x)), the list of k documents z with highest prior probability pŒ∑(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory.\\n\\n2.3 Generator: BART\\n\\nThe generator component pŒ∏(yi|x,z,y1:i‚àí1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising',)\n",
      "(' include query rewriting query transformation, query expansion and other techniques [7], [9]‚Äì[11].\\n\\nPost-Retrieval Process. Once relevant context is retrieved, it‚Äôs crucial to integrate it effectively with the query. The main methods in post-retrieval process include rerank chunks and context compressing. Re-ranking the retrieved information to relocate the most relevant content to the edges of the prompt is a key strategy. This concept has been implemented in frame- works such as LlamaIndex2, LangChain3, and HayStack [12]. Feeding all relevant documents directly into LLMs can lead to information overload, diluting the focus on key details with irrelevant content.To mitigate this, post-retrieval efforts con- centrate on selecting the essential information, emphasizing critical sections, and shortening the context to be processed.\\n\\n2https://www.llamaindex.ai 3https://www.langchain.com/\\n\\nC. Modular RAG\\n\\nThe modular RAG architecture advances beyond the for- mer two RAG paradigms, offering enhanced adaptability and versatility. It incorporates diverse strategies for improving its components, such as adding a search module for similarity searches and refining the retriever through fine-tuning. Inno- vations like restructured RAG modules [13] and rearranged RAG pipelines [14] have been introduced to tackle specific challenges. The shift towards a modular RAG approach is becoming prevalent, supporting both sequential processing and integrated end-to-end training across its components. Despite its distinctiveness, Modular RAG builds upon the foundational principles of Advanced and Naive RAG, illustrating a progres- sion and refinement within the RAG family.\\n\\n1) New Modules: The Modular RAG framework introduces additional specialized components to enhance retrieval and processing capabilities. The Search module adapts to spe- cific scenarios, enabling direct searches across various data sources like search engines, databases, and knowledge graphs, using LLM-generated code and query languages [15]. RAG- Fusion addresses traditional search limitations by employing a multi-query strategy that expands user queries into diverse perspectives, utilizing parallel vector searches and intelligent re-ranking to uncover both explicit and transformative knowl- edge [16]. The Memory module leverages the LLM‚Äôs memory to guide retrieval, creating an unbounded memory pool that\\n\\n4\\n\\naligns the text more closely with data distribution through iter- ative self-enhancement [17], [18]. Routing in the RAG system navigates through diverse data sources, selecting the optimal pathway for a query, whether it involves summarization, specific database searches, or merging different information streams [19]. The Predict module aims to reduce redundancy and noise by generating context directly through the LLM, ensuring relevance and accuracy [13]. Lastly, the Task Adapter module tailors RAG to various downstream tasks, automating prompt retrieval for zero-shot inputs and creating task-specific retrievers through few-shot query generation [20], [21] .This comprehensive approach not only streamlines the retrieval pro- cess but also significantly improves the quality and relevance of the information retrieved, catering to a wide array of tasks and queries with enhanced precision and flexibility.\\n\\n2) New Patterns: Modular RAG offers remarkable adapt- ability by allowing module substitution or reconfiguration to address specific challenges. This goes beyond the fixed structures of Naive and Advanced RAG, characterized by a simple ‚ÄúRetrieve‚Äù and ‚ÄúRead‚Äù mechanism. Moreover, Modular RAG expands this flexibility by integrating new modules or adjusting interaction flow among existing ones, enhancing its applicability across different tasks. Innovations such as the Rewrite-Retrieve-Read [7]model leverage the LLM‚Äôs capabilities to refine retrieval queries through a rewriting module and a LM-feedback mechanism to update rewriting model., improving task performance. Similarly, approaches like Generate-Read [13] replace tradi- tional retrieval with LLM-generated content, while Recite- Read [22] emphasizes retrieval from model weights, enhanc- ing the model‚Äôs ability to handle knowledge-intensive tasks. Hybrid retrieval strategies integrate keyword, semantic, and vector searches to cater to diverse queries. Additionally, em- ploying sub-queries and hypothetical document embeddings (HyDE) [11] seeks to improve retrieval relevance by focusing on embedding similarities between generated answers and real documents.\\n\\nAdjustments in module arrangement and interaction, such as the Demonstrate-Search-Predict (DSP) [23] framework and the iterative Retrieve-Read-Retrieve-Read flow of ITER- RETGEN [14], showcase the dynamic use of module out- puts to bolster another module‚Äôs functionality, illustrating a sophisticated understanding of enhancing module synergy. The flexible orchestration of Modular RAG Flow showcases the benefits of adaptive retrieval through techniques such as FL',)\n",
      "(' AI methodologies, such as fine-tuning and reinforcement learning, has further expanded its capabilities. Despite the progress in RAG technology, there are research opportunities to improve its robustness and its ability to handle extended contexts. RAG‚Äôs application scope is expanding into multimodal do- mains, adapting its principles to interpret and process diverse data forms like images, videos, and code. This expansion high- lights RAG‚Äôs significant practical implications for AI deploy- ment, attracting interest from academic and industrial sectors.\\n\\n16\\n\\nThe growing ecosystem of RAG is evidenced by the rise in RAG-centric AI applications and the continuous development of supportive tools. As RAG‚Äôs application landscape broadens, there is a need to refine evaluation methodologies to keep pace with its evolution. Ensuring accurate and representative performance assessments is crucial for fully capturing RAG‚Äôs contributions to the AI research and development community.\\n\\nREFERENCES\\n\\n[1] N. Kandpal, H. Deng, A. Roberts, E. Wallace, and C. Raffel, ‚ÄúLarge language models struggle to learn long-tail knowledge,‚Äù in Interna- tional Conference on Machine Learning. PMLR, 2023, pp. 15696‚Äì 15707.\\n\\n[2] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao, Y. Zhang, Y. Chen et al., ‚ÄúSiren‚Äôs song in the ai ocean: A survey on hal- lucination in large language models,‚Äù arXiv preprint arXiv:2309.01219, 2023.\\n\\n[3] D. Arora, A. Kini, S. R. Chowdhury, N. Natarajan, G. Sinha, and A. Sharma, ‚ÄúGar-meets-rag paradigm for zero-shot information re- trieval,‚Äù arXiv preprint arXiv:2310.20158, 2023.\\n\\n[4] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. K¬®uttler, M. Lewis, W.-t. Yih, T. Rockt¬®aschel et al., ‚ÄúRetrieval- augmented generation for knowledge-intensive nlp tasks,‚Äù Advances in Neural Information Processing Systems, vol. 33, pp. 9459‚Äì9474, 2020. [5] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Milli- can, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark et al., ‚ÄúImproving language models by retrieving from trillions of tokens,‚Äù in International conference on machine learning. PMLR, 2022, pp. 2206‚Äì2240.\\n\\n[6] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al., ‚ÄúTraining language models to follow instructions with human feedback,‚Äù Advances in neural information processing systems, vol. 35, pp. 27730‚Äì27744, 2022.\\n\\n[7] X. Ma, Y. Gong, P. He, H. Zhao, and N. Duan, ‚ÄúQuery rewrit- ing for retrieval-augmented large language models,‚Äù arXiv preprint arXiv:2305.14283, 2023.\\n\\n[8] I.\\n\\nILIN,\\n\\n‚ÄúAdvanced\\n\\nrag\\n\\nil- https://pub.towardsai.net/\\n\\ntechniques:\\n\\nan\\n\\nlustrated advanced-rag-techniques-an-illustrated-overview-04d193d8fec6, 2023.\\n\\noverview,‚Äù\\n\\n[9] W. Peng, G. Li, Y. Jiang, Z. Wang, D. Ou, X. Zeng, E. Chen et al., ‚ÄúLarge language model based long-tail query rewriting in taobao search,‚Äù arXiv preprint arXiv:2311.03758, 2023.\\n\\n[10] H. S. Zheng, S. Mishra, X. Chen, H.-T. Cheng, E. H. Chi, Q. V. Le, and D. Zhou, ‚ÄúTake a step back: Evoking reasoning via abstraction in large language models,‚Äù arXiv pre',)\n",
      "(' addressed [175].\\n\\nThe development of the RAG ecosystem is greatly impacted by the progression of its technical stack. Key tools like LangChain and LLamaIndex have quickly gained popularity with the emergence of ChatGPT, providing extensive RAG- related APIs and becoming essential in the realm of LLMs.The emerging technology stack, while not as rich in features as LangChain and LLamaIndex, stands out through its specialized products. For example, Flowise AI prioritizes a low-code approach, allowing users to deploy AI applications, including RAG, through a user-friendly drag-and-drop interface. Other technologies like HayStack, Meltano, and Cohere Coral are also gaining attention for their unique contributions to the field. In addition to AI-focused vendors, traditional software and cloud service providers are expanding their offerings to include RAG-centric services. Weaviate‚Äôs Verba 11 is designed for personal assistant applications, while Amazon‚Äôs Kendra 12 offers intelligent enterprise search services, enabling users to browse various content repositories using built-in connectors. there is a clear In the development of RAG technology, trend towards different specialization directions, such as: 1) Customization - tailoring RAG to meet specific requirements. 2) Simplification - making RAG easier to use to reduce the\\n\\n10https://github.com/inverse-scaling/prize\\n\\n11https://github.com/weaviate/Verba 12https://aws.amazon.com/cn/kendra/\\n\\n15\\n\\nFig. 6. Summary of RAG ecosystem\\n\\ninitial learning curve. 3) Specialization - optimizing RAG to better serve production environments.\\n\\nThe mutual growth of RAG models and their technology stacks is evident; technological advancements continuously establish new standards for existing infrastructure. In turn, enhancements to the technology stack drive the development of RAG capabilities. RAG toolkits are converging into a foundational laying the groundwork for advanced enterprise applications. However, a fully integrated, comprehensive platform concept is still in the future, requiring further innovation and development.\\n\\ntechnology stack,\\n\\nF. Multi-modal RAG\\n\\ntext-based question- answering confines, embracing a diverse array of modal data. This expansion has spawned innovative multimodal models that integrate RAG concepts across various domains:\\n\\nRAG has\\n\\ntranscended its\\n\\ninitial\\n\\nImage. RA-CM3 [176] stands as a pioneering multimodal model of both retrieving and generating text and images. BLIP-2 [177] leverages frozen image encoders alongside LLMs for efficient visual language pre-training, enabling zero- shot image-to-text conversions. The ‚ÄúVisualize Before You Write‚Äù method [178] employs image generation to steer the LM‚Äôs text generation, showing promise in open-ended text generation tasks.\\n\\nAudio and Video. The GSS method retrieves and stitches together audio clips to convert machine-translated data into speech-translated data [179]. UEOP marks a significant ad- vancement in end-to-end automatic speech recognition by incorporating external, offline strategies for voice-to-text con- version [180]. Additionally, KNN-based attention fusion lever- ages audio embeddings and semantically related text embed- dings to refine ASR, thereby accelerating domain adaptation.\\n\\nVid2Seq augments language models with specialized temporal markers, facilitating the prediction of event boundaries and textual descriptions within a unified output sequence [181].\\n\\nCode. RBPS [182] excels in small-scale learning tasks by retrieving code examples that align with developers‚Äô objectives through encoding and frequency analysis. This approach has demonstrated efficacy in tasks such as test assertion genera- tion and program repair. For structured knowledge, the CoK method [106] first extracts facts pertinent to the input query from a knowledge graph, then integrates these facts as hints within the input, enhancing performance in knowledge graph question-answering tasks.\\n\\nVIII. CONCLUSION\\n\\nThe summary of this paper, as depicted in Figure 6, empha- sizes RAG‚Äôs significant advancement in enhancing the capa- bilities of LLMs by integrating parameterized knowledge from language models with extensive non-parameterized data from external knowledge bases. The survey showcases the evolution of RAG technologies and their application on many different tasks. The analysis outlines three developmental paradigms within the RAG framework: Naive, Advanced, and Modu- lar RAG, each representing a progressive enhancement over its predecessors. RAG‚Äôs technical integration with other AI methodologies, such as fine-tuning and reinforcement learning, has further expanded its capabilities. Despite the progress in RAG technology, there are research opportunities to improve its robustness and its ability to handle extended contexts. RAG‚Äôs application scope is expanding into multimodal do- mains, adapting',)\n",
      "(' [21] AAR [47] RA-DIT [27] RAG-Robust [48] RA-Long-Form [49] CoN [50] Self-RAG [25] BGM [26] CoQ [51] Token-Elimination [52] PaperQA [53] NoiseRAG [54] IAG [55] NoMIRACL [56] ToC [57] SKR [58] ITRG [59] RAG-LongContext [60] ITER-RETGEN [14] IRCoT [61] LLM-Knowledge-Boundary [62] RAPTOR [63] RECITE [22] ICRALM [64] Retrieve-and-Sample [65] Zemi [66] CRAG [67] 1-PAGER [68] PRCA [69] QLM-Doc-ranking [70] Recomp [71] DSP [23] RePLUG [72] ARM-RAG [73] GenRead [13] UniMS-RAG [74] CREA-ICL [19] PKG [75] SANTA [76] SURGE [77] MK-ToD [78] Dual-Feedback-ToD [79] KnowledGPT [15] FABULA [80] HyKGE [81] KALMV [82] RoG [83] G-Retriever [84]\\n\\nTABLE I SUMMARY OF RAG METHODS\\n\\nRetrieval Source\\n\\nRetrieval Data Type\\n\\nWikipedia FactoidWiki Dataset-base Dataset-base Dataset-base Dataset-base Search Engine,Wikipedia Wikipedia Wikipedia Dataset-base Synthesized dataset Dataset-base Dataset-base Dataset-base Dataset-base Dataset-base Synthesized dataset Wikipedia, Common Crawl Wikipedia Pre-training Corpus Pre-training corpus Search Engine Dataset-base BEIR MSMARCO,Wikipedia Common Crawl,Wikipedia Wikipedia Dataset-base Wikipedia Wikipedia Wikipedia Wikipedia Wikipedia Arxiv,Online Database,PubMed FactoidWiki Search Engine,Wikipedia Wikipedia Search Engine,Wikipedia Dataset-base,Wikipedia Wikipedia Dataset-base Wikipedia Wikipedia Wikipedia Dataset-base LLMs Pile,Wikipedia Dataset-base C4 Arxiv Wikipedia Dataset-base Dataset-base Wikipedia Wikipedia Pile Dataset-base LLMs Dataset-base Dataset-base LLM Dataset-base Freebase Dataset-base Dataset-base Dataset-base Dataset-base,Graph CMeKG Wikipedia Freebase Dataset-base\\n\\nText Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text Crosslingual,Text Tabular,Text Code,Text KG KG KG KG KG KG KG KG TextGraph\\n\\nRetrieval Granularity\\n\\nPhrase Proposition Sentence Sentence Sentence Sentence Sentence Sentence Sentence Sentence Sentence Sentence Pair Sentence Pair Item-base Item-base Item-base Item-base Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Chunk Doc Doc Doc Doc Doc Doc Doc Doc Doc Doc Doc Multi Sentence Chunk Item Sub-Graph Entity Entity Sequence Triplet Entity Entity Triplet Triplet Sub-Graph\\n\\nAugmentation Stage\\n\\nPre-training Inference Tuning Tuning Tuning Tuning Tuning Inference Inference Inference Inference Tuning Inference Pre-training Tuning Tuning Tuning Pre-training Pre-training Pre-training Pre-training Tuning Tuning Tuning Tuning Tuning Tuning Tuning Tuning Tuning Inference Inference Inference Inference Inference Inference Inference Inference Inference Inference Inference Inference Inference Inference Inference Inference Inference Tuning Tuning Inference Inference Inference Inference Inference Inference Inference Inference Inference Tuning Inference Inference Pre-training Tuning Tuning Tuning Inference Inference Inference Inference Inference Inference\\n\\nRetrieval process\\n\\nIterative Once Once Once Once Iterative Adaptive Once Once Once Once Once Iterative Once Once Once Once Iterative Once Iterative Iterative Once Once Once Once Once Once Once Once Adaptive Once Iterative Once Iterative Once Once Once Recursive Adapt',)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "TOP_K: int = 5\n",
    "\n",
    "load_dotenv(dotenv_path=\"../../env/pgvector-db.env\")\n",
    "\n",
    "# Connect to PostgreSQL database\n",
    "with psycopg2.connect(\n",
    "    dbname=os.getenv(\"R2R_POSTGRES_DBNAME\"),\n",
    "    user=os.getenv(\"R2R_POSTGRES_USER\"),\n",
    "    password=os.getenv(\"R2R_POSTGRES_PASSWORD\"),\n",
    "    host=os.getenv(\"R2R_POSTGRES_HOST\"),\n",
    "    port=os.getenv(\"R2R_POSTGRES_PORT\"),\n",
    ") as conn:\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # The relevant table: bachelor.chunks\n",
    "    query: str = input(\"Please enter a query to perform similarity search on: \")\n",
    "    query_embedding: list[float] = generate_embedding(query)\n",
    "    \n",
    "    # Use \"<->\" for cosine similarity\n",
    "    # \"vec\" is the name of the column containing the embeddings\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT text FROM bachelor.chunks\n",
    "        ORDER BY vec <-> %s::vector\n",
    "        LIMIT %s;\n",
    "    \"\"\", (query_embedding, TOP_K))\n",
    "    \n",
    "    # Print results\n",
    "    results = cursor.fetchall()\n",
    "    for text in results:\n",
    "        print(text)\n",
    "        \n",
    "    # Re-rank the retrieved context\n",
    "    response = requests.post(\n",
    "        url=\"http://localhost:8080/rerank\",\n",
    "        headers={\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        },\n",
    "        json={\n",
    "            \"query\": query, \n",
    "            \"texts\": [result[0] for result in results],\n",
    "            \"raw_scores\": False\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        for entry in response.json():\n",
    "            print(results[entry['index']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"index\":1,\"score\":0.9988028},{\"index\":0,\"score\":0.022846196}]'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthesizer\n",
    "\n",
    "This object can be used to generate **Golden** instances, which consist out of **input**, **expected output** and **context**. It uses a LLM to come up with random input and thereafter tries to enhance those, by making them more complex and realistic.\n",
    "\n",
    "For a comprehensive guide on understanding how this object works please refer here: [Synthesizer](https://www.confident-ai.com/blog/the-definitive-guide-to-synthetic-data-generation-using-llms)\n",
    "\n",
    "### Summary\n",
    "\n",
    "I will try to summarize the most important information:\n",
    "\n",
    "* It uses a **LLM to come-up with a comprehensive dataset** much faster than a human can\n",
    "* The process starts with the LLM generating **synthetic queries** based on context from a knowledge base - usually documents\n",
    "* Those initial queries are then **evolved** to reflect real-life complexity and then together with the context can be used to generate a **target/expected output**\n",
    "\n",
    "![Dataset generation workflow](https://cdn.prod.website-files.com/64bd90bdba579d6cce245aec/670574639fc6b9d5c483d766_664050ef1eb43f5fb8f57ff8_diagram.png \"Synthetic generation\")\n",
    "\n",
    "* There exist two main methods:\n",
    "    - Self-improvement: Iteratively uses the LLMs output to generate more complex queries\n",
    "    - Distillation: A stronger model is being utilized \n",
    "\n",
    "* Constructing contexts:\n",
    "    - During this phase documents from the knowledge base are split using a token splitter\n",
    "    - A random chunk is selected\n",
    "    - Finally, additional chunks are retrieved based on **semantic similarity**, **knowledge graphs** or others\n",
    "    - Ensuring that **chunk size**, **chunk overlap** or other similar parameters here and in the **retrieval component** of the **RAG** application are identical will yield better results\n",
    "\n",
    "![Constructing contexts](https://cdn.prod.website-files.com/64bd90bdba579d6cce245aec/672cb28e9f8f60aabd382788_672cb201dadd3fd2de4451d2_context_generation.png \"Context construction\")\n",
    "\n",
    "* Constructing synthetic queries:\n",
    "    - Using the contexts the **Synthesizer** can now generate synthetic input\n",
    "    - Doing so we ensure that the input corresponds with the context enhancing the **relevancy** and **accuracy**\n",
    "\n",
    "![Constructing synthetic queries](https://cdn.prod.website-files.com/64bd90bdba579d6cce245aec/672cb28e9f8f60aabd382775_672cb23c502672c70e0372cd_asymmetry.png \"Synthetic queries creation\")\n",
    "\n",
    "* Data Filtering:\n",
    "    1. Context filtering: Removes low-quality chunks that may be unintelligible\n",
    "\n",
    "    ![Context filtering](https://cdn.prod.website-files.com/64bd90bdba579d6cce245aec/672cb28e9f8f60aabd38278b_672cb26b461b45b0b5a6cd30_context_filtering.png \"Filtering context\")\n",
    "\n",
    "    2. Input filtering: Ensures generated inputs meet quality standards\n",
    "\n",
    "    ![Input filtering](https://cdn.prod.website-files.com/64bd90bdba579d6cce245aec/672cb28e9f8f60aabd382772_672cb27b799642a337436c3f_input_filtering.png \"Filtering queries\")\n",
    "    \n",
    "* Customizing dataset generating:\n",
    "    - Depending on the scenario inputs and outputs can be tailored to specific use cases\n",
    "        - For example a medical chatbot would have a completely different behaviour than a scientific one. It would need to comfort patients.\n",
    "    \n",
    "* Data Evolution:\n",
    "    - **In-Depth Evolving**: Expands simple instructions into more detailed versions\n",
    "    - **In-Breadth Evolving**: Produces diverse instructions to enrich the dataset\n",
    "    - **Elimination Evolving**: Removes less effective instructions\n",
    "\n",
    "    ![Data evolution](https://cdn.prod.website-files.com/64bd90bdba579d6cce245aec/670574639fc6b9d5c483d763_6641a0d7ef709f365d888577_Screenshot%25202024-05-13%2520at%25201.10.30%2520PM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM provider\n",
    "\n",
    "**DeepEval** uses **OpenAI** by default as a LLM, however **Ollama** is also available. To use it execute the code cell below. This will generate a `.deepeval` file where key-value pairs will be stored about that particular LLM-provider like model name, base url and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üôå Congratulations! You're now using a local Ollama model for all evals that \n",
      "require an LLM.\n",
      "üôå Congratulations! You're now using Ollama embeddings for all evals that \n",
      "require text embeddings.\n"
     ]
    }
   ],
   "source": [
    "!deepeval set-ollama model-name=llama3.1:latest --base-url=\"http://localhost:11434/\"\n",
    "!deepeval set-ollama-embeddings model_name=mxbai-embed-large --base-url=\"http://localhost11434\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'data'...\n",
      "remote: Enumerating objects: 14, done.\u001b[K\n",
      "remote: Total 14 (delta 0), reused 0 (delta 0), pack-reused 14 (from 1)\u001b[K\n",
      "Unpacking objects: 100% (14/14), 16.16 KiB | 4.04 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://huggingface.co/datasets/explodinggradients/ragas-airline-dataset data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evolutions** are used to specify the type of approach to use when trying to complicate the synthetic queries. Since this is a **RAG** application I will only use the evolution types which use **context**. By setting `num_evolutions` to three, we make the **Synthesizer** go over iteratively over the process of complicating the queries 3 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/data/special_assistance.md',\n",
       " '/home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/data/managing_reservations.md',\n",
       " '/home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/data/flight_delays.md',\n",
       " '/home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/data/baggage_policies.md',\n",
       " '/home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/data/inflight_services.md',\n",
       " '/home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/data/schedule_changes.md',\n",
       " '/home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/data/bookings.md',\n",
       " '/home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/data/flight_cancellations.md']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import typing as t\n",
    "from pathlib import Path\n",
    "from deepeval.dataset import Golden\n",
    "from deepeval.synthesizer.config import (\n",
    "    Evolution,\n",
    "    EvolutionConfig,\n",
    "    ContextConstructionConfig\n",
    ")\n",
    "from deepeval.synthesizer import Synthesizer\n",
    "\n",
    "# TODO: Define different scenarious to get a more comprehensive dataset\n",
    "# ChromaDB missing\n",
    "# Supported file types -> docs, pdf, txt, NOT md\n",
    "# Instead of using generate_goldens_from_docs we can go for the other approach from contexts\n",
    "# To do so use R2R to fetch all chunks from ingested files\n",
    "# Thereafter use the contexts to have the synthetic queries be generated\n",
    "# The final maximum number of goldens to be generated is the max_goldens_per_context multiplied by the \n",
    "# max_contexts_per_document as specified in the context_construction_config, and NOT simply max_goldens_per_context.\n",
    "\n",
    "synthesizer = Synthesizer(\n",
    "    evolution_config = EvolutionConfig(\n",
    "        num_evolutions=3,\n",
    "        evolutions={\n",
    "            Evolution.MULTICONTEXT: 0.25,\n",
    "            Evolution.CONCRETIZING: 0.25,\n",
    "            Evolution.CONSTRAINED: 0.25,\n",
    "            Evolution.COMPARATIVE: 0.25,\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "doc_paths = []\n",
    "docs_dir = Path(\"./data\")\n",
    "for file in docs_dir.iterdir():\n",
    "    if file.is_file() and file.suffix in [\".md\"] and file.name != \"README.md\":\n",
    "        doc_paths.append(str(file.absolute()))    \n",
    "\n",
    "goldens: t.List[Golden] = synthesizer.generate_goldens_from_docs(\n",
    "    document_paths=doc_paths,\n",
    "    max_goldens_per_context=5,\n",
    "    context_construction_config=ContextConstructionConfig(\n",
    "        max_contexts_per_document=5,\n",
    "        max_context_length=5,\n",
    "        chunk_size=1024,\n",
    "        chunk_overlap=128,\n",
    "        max_retries=5\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesizer.generate_goldens_from_contexts(\n",
    "    \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepeval_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
