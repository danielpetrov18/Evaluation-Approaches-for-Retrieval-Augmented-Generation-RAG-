{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a synthetic dataset using DeepEval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthesizer\n",
    "\n",
    "This object can be used to generate **Golden** instances, which consist out of **input**, **expected output** and **context**. It uses a LLM to come up with random input and thereafter tries to enhance those, by making them more complex and realistic.\n",
    "\n",
    "For a comprehensive guide on understanding how this object works please refer here: [Synthesizer](https://www.confident-ai.com/blog/the-definitive-guide-to-synthetic-data-generation-using-llms)\n",
    "\n",
    "### Summary\n",
    "\n",
    "I will try to summarize the most important information:\n",
    "\n",
    "* It uses a **LLM to come-up with a comprehensive dataset** much faster than a human can\n",
    "* The process starts with the LLM generating **synthetic queries** based on context from a knowledge base - usually documents\n",
    "* Those initial queries are then **evolved** to reflect real-life complexity and then together with the context can be used to generate a **target/expected output**\n",
    "\n",
    "![Dataset generation workflow](https://cdn.prod.website-files.com/64bd90bdba579d6cce245aec/670574639fc6b9d5c483d766_664050ef1eb43f5fb8f57ff8_diagram.png \"Synthetic generation\")\n",
    "\n",
    "* There exist two main methods:\n",
    "    - Self-improvement: Iteratively uses the LLMs output to generate more complex queries\n",
    "    - Distillation: A stronger model is being utilized \n",
    "\n",
    "* Constructing contexts:\n",
    "    - During this phase documents from the knowledge base are split using a token splitter\n",
    "    - A random chunk is selected\n",
    "    - Finally, additional chunks are retrieved based on **semantic similarity**, **knowledge graphs** or others\n",
    "    - Ensuring that **chunk size**, **chunk overlap** or other similar parameters here and in the **retrieval component** of the **RAG** application are identical will yield better results\n",
    "\n",
    "![Constructing contexts](https://cdn.prod.website-files.com/64bd90bdba579d6cce245aec/672cb28e9f8f60aabd382788_672cb201dadd3fd2de4451d2_context_generation.png \"Context construction\")\n",
    "\n",
    "* Constructing synthetic queries:\n",
    "    - Using the contexts the **Synthesizer** can now generate synthetic input\n",
    "    - Doing so we ensure that the input corresponds with the context enhancing the **relevancy** and **accuracy**\n",
    "\n",
    "![Constructing synthetic queries](https://cdn.prod.website-files.com/64bd90bdba579d6cce245aec/672cb28e9f8f60aabd382775_672cb23c502672c70e0372cd_asymmetry.png \"Synthetic queries creation\")\n",
    "\n",
    "* Data Filtering:\n",
    "    1. Context filtering: Removes low-quality chunks that may be unintelligible\n",
    "\n",
    "    ![Context filtering](https://cdn.prod.website-files.com/64bd90bdba579d6cce245aec/672cb28e9f8f60aabd38278b_672cb26b461b45b0b5a6cd30_context_filtering.png \"Filtering context\")\n",
    "\n",
    "    2. Input filtering: Ensures generated inputs meet quality standards\n",
    "\n",
    "    ![Input filtering](https://cdn.prod.website-files.com/64bd90bdba579d6cce245aec/672cb28e9f8f60aabd382772_672cb27b799642a337436c3f_input_filtering.png \"Filtering queries\")\n",
    "    \n",
    "* Customizing dataset generating:\n",
    "    - Depending on the scenario inputs and outputs can be tailored to specific use cases\n",
    "        - For example a medical chatbot would have a completely different behaviour than a scientific one. It would need to comfort patients.\n",
    "    \n",
    "* Data Evolution:\n",
    "    - **In-Depth Evolving**: Expands simple instructions into more detailed versions\n",
    "    - **In-Breadth Evolving**: Produces diverse instructions to enrich the dataset\n",
    "    - **Elimination Evolving**: Removes less effective instructions\n",
    "\n",
    "    ![Data evolution](https://cdn.prod.website-files.com/64bd90bdba579d6cce245aec/670574639fc6b9d5c483d763_6641a0d7ef709f365d888577_Screenshot%25202024-05-13%2520at%25201.10.30%2520PM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM provider\n",
    "\n",
    "**DeepEval** uses **OpenAI** by default as a LLM, however **Ollama** is also available. To use it execute the code cell below. This will generate a `.deepeval` file where key-value pairs will be stored about that particular LLM-provider like model name, base url and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ™Œ Congratulations! You're now using a local Ollama model for all evals that \n",
      "require an LLM.\n",
      "ðŸ™Œ Congratulations! You're now using Ollama embeddings for all evals that \n",
      "require text embeddings.\n"
     ]
    }
   ],
   "source": [
    "!deepeval set-ollama model-name=llama3.1:latest --base-url=\"http://localhost:11434/\"\n",
    "!deepeval set-ollama-embeddings model_name=mxbai-embed-large --base-url=\"http://localhost11434\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'data'...\n",
      "remote: Enumerating objects: 14, done.\u001b[K\n",
      "remote: Total 14 (delta 0), reused 0 (delta 0), pack-reused 14 (from 1)\u001b[K\n",
      "Unpacking objects: 100% (14/14), 16.16 KiB | 4.04 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://huggingface.co/datasets/explodinggradients/ragas-airline-dataset data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evolutions** are used to specify the type of approach to use when trying to complicate the synthetic queries. Since this is a **RAG** application I will only use the evolution types which use **context**. By setting `num_evolutions` to three, we make the **Synthesizer** go over iteratively over the process of complicating the queries 3 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/data/special_assistance.md',\n",
       " '/home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/data/managing_reservations.md',\n",
       " '/home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/data/flight_delays.md',\n",
       " '/home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/data/baggage_policies.md',\n",
       " '/home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/data/inflight_services.md',\n",
       " '/home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/data/schedule_changes.md',\n",
       " '/home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/data/bookings.md',\n",
       " '/home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/data/flight_cancellations.md']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import typing as t\n",
    "from pathlib import Path\n",
    "from deepeval.dataset import Golden\n",
    "from deepeval.synthesizer.config import (\n",
    "    Evolution,\n",
    "    EvolutionConfig,\n",
    "    ContextConstructionConfig\n",
    ")\n",
    "from deepeval.synthesizer import Synthesizer\n",
    "\n",
    "# TODO: Define different scenarious to get a more comprehensive dataset\n",
    "# ChromaDB missing\n",
    "# Supported file types -> docs, pdf, txt, NOT md\n",
    "# Instead of using generate_goldens_from_docs we can go for the other approach from contexts\n",
    "# To do so use R2R to fetch all chunks from ingested files\n",
    "# Thereafter use the contexts to have the synthetic queries be generated\n",
    "# The final maximum number of goldens to be generated is the max_goldens_per_context multiplied by the \n",
    "# max_contexts_per_document as specified in the context_construction_config, and NOT simply max_goldens_per_context.\n",
    "\n",
    "synthesizer = Synthesizer(\n",
    "    evolution_config = EvolutionConfig(\n",
    "        num_evolutions=3,\n",
    "        evolutions={\n",
    "            Evolution.MULTICONTEXT: 0.25,\n",
    "            Evolution.CONCRETIZING: 0.25,\n",
    "            Evolution.CONSTRAINED: 0.25,\n",
    "            Evolution.COMPARATIVE: 0.25,\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "doc_paths = []\n",
    "docs_dir = Path(\"./data\")\n",
    "for file in docs_dir.iterdir():\n",
    "    if file.is_file() and file.suffix in [\".md\"] and file.name != \"README.md\":\n",
    "        doc_paths.append(str(file.absolute()))    \n",
    "\n",
    "goldens: t.List[Golden] = synthesizer.generate_goldens_from_docs(\n",
    "    document_paths=doc_paths,\n",
    "    max_goldens_per_context=5,\n",
    "    context_construction_config=ContextConstructionConfig(\n",
    "        max_contexts_per_document=5,\n",
    "        max_context_length=5,\n",
    "        chunk_size=1024,\n",
    "        chunk_overlap=128,\n",
    "        max_retries=5\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesizer.generate_goldens_from_contexts(\n",
    "    \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
