{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a synthetic dataset using DeepEval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthesizer\n",
    "\n",
    "This object can be used to generate **Golden** instances, which consist out of **input**, **expected output** and **context**. It uses a LLM to come up with random input and thereafter tries to enhance those, by making them more complex and realistic.\n",
    "\n",
    "For a comprehensive guide on understanding how this object works please refer here: [Synthesizer](https://www.confident-ai.com/blog/the-definitive-guide-to-synthetic-data-generation-using-llms)\n",
    "\n",
    "### Summary\n",
    "\n",
    "I will try to summarize the most important information:\n",
    "\n",
    "* It uses a **LLM to come-up with a comprehensive dataset** much faster than a human can\n",
    "* The process starts with the LLM generating **synthetic queries** based on context from a knowledge base - usually documents\n",
    "* Those initial queries are then **evolved** to reflect real-life complexity and then together with the context can be used to generate a **target/expected output**\n",
    "\n",
    "![Dataset generation workflow](https://cdn.prod.website-files.com/64bd90bdba579d6cce245aec/670574639fc6b9d5c483d766_664050ef1eb43f5fb8f57ff8_diagram.png \"Synthetic generation\")\n",
    "\n",
    "* There exist two main methods:\n",
    "    - Self-improvement: Iteratively uses the LLMs output to generate more complex queries\n",
    "    - Distillation: A stronger model is being utilized \n",
    "\n",
    "* Constructing contexts:\n",
    "    - During this phase documents from the knowledge base are split using a token splitter\n",
    "    - A random chunk is selected\n",
    "    - Finally, additional chunks are retrieved based on **semantic similarity**, **knowledge graphs** or others\n",
    "    - Ensuring that **chunk size**, **chunk overlap** or other similar parameters here and in the **retrieval component** of the **RAG** application are identical will yield better results\n",
    "\n",
    "![Constructing contexts](https://cdn.prod.website-files.com/64bd90bdba579d6cce245aec/672cb28e9f8f60aabd382788_672cb201dadd3fd2de4451d2_context_generation.png \"Context construction\")\n",
    "\n",
    "* Constructing synthetic queries:\n",
    "    - Using the contexts the **Synthesizer** can now generate synthetic input\n",
    "    - Doing so we ensure that the input corresponds with the context enhancing the **relevancy** and **accuracy**\n",
    "\n",
    "![Constructing synthetic queries](https://cdn.prod.website-files.com/64bd90bdba579d6cce245aec/672cb28e9f8f60aabd382775_672cb23c502672c70e0372cd_asymmetry.png \"Synthetic queries creation\")\n",
    "\n",
    "* Data Filtering:\n",
    "    1. Context filtering: Removes low-quality chunks that may be unintelligible\n",
    "\n",
    "    ![Context filtering](https://cdn.prod.website-files.com/64bd90bdba579d6cce245aec/672cb28e9f8f60aabd38278b_672cb26b461b45b0b5a6cd30_context_filtering.png \"Filtering context\")\n",
    "\n",
    "    2. Input filtering: Ensures generated inputs meet quality standards\n",
    "\n",
    "    ![Input filtering](https://cdn.prod.website-files.com/64bd90bdba579d6cce245aec/672cb28e9f8f60aabd382772_672cb27b799642a337436c3f_input_filtering.png \"Filtering queries\")\n",
    "    \n",
    "* Customizing dataset generating:\n",
    "    - Depending on the scenario inputs and outputs can be tailored to specific use cases\n",
    "        - For example a medical chatbot would have a completely different behaviour than a scientific one. It would need to comfort patients.\n",
    "    \n",
    "* Data Evolution:\n",
    "    - **In-Depth Evolving**: Expands simple instructions into more detailed versions\n",
    "    - **In-Breadth Evolving**: Produces diverse instructions to enrich the dataset\n",
    "    - **Elimination Evolving**: Removes less effective instructions\n",
    "\n",
    "    ![Data evolution](https://cdn.prod.website-files.com/64bd90bdba579d6cce245aec/670574639fc6b9d5c483d763_6641a0d7ef709f365d888577_Screenshot%25202024-05-13%2520at%25201.10.30%2520PM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies:\n",
    "\n",
    "* To install the dependencies run the `setup` bash script in the root of the `evaluation` folder.\n",
    "* Make sure you select the correct kernel in your notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: deepeval\n",
      "Version: 2.8.2\n",
      "Summary: The LLM Evaluation Framework\n",
      "Home-page: https://github.com/confident-ai/deepeval\n",
      "Author: Jeffrey Ip\n",
      "Author-email: jeffreyip@confident-ai.com\n",
      "License: Apache-2.0\n",
      "Location: /home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/eval/lib/python3.12/site-packages\n",
      "Requires: aiohttp, anthropic, black, coverage, google-genai, grpcio, nest_asyncio, ollama, openai, opentelemetry-api, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-sdk, portalocker, posthog, pytest, pytest-asyncio, pytest-repeat, pytest-rerunfailures, pytest-xdist, requests, rich, sentry-sdk, setuptools, tabulate, tenacity, tqdm, twine, typer, wheel\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# After installing the dependencies and selecting the kernel you should be good to go.\n",
    "# Make sure the package is installed before continuing further.\n",
    "! pip3 show deepeval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM provider\n",
    "\n",
    "**DeepEval** uses **OpenAI** by default as a LLM provider, however **Ollama** is also available. To use it execute the code cell below. This will generate a `.deepeval` file where key-value pairs will be stored about that particular LLM-provider like model name, base url and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üôå Congratulations! You're now using a local Ollama model for all evals that \n",
      "require an LLM.\n",
      "üôå Congratulations! You're now using Ollama embeddings for all evals that \n",
      "require text embeddings.\n"
     ]
    }
   ],
   "source": [
    "! deepeval set-ollama llama3.1 --base-url=\"http://localhost:11434/\"\n",
    "! deepeval set-ollama-embeddings mxbai-embed-large --base-url=\"http://localhost:11434\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the dataset\n",
    "\n",
    "The dataset is a simple collection of documents containing information about a fictional airline company called RAGAs. The dataset is taken from the **RAGAs** documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error loading file data/special_assistance.md\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      8\u001b[39m path: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mdata/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m loader = DirectoryLoader(\n\u001b[32m     10\u001b[39m     path,\n\u001b[32m     11\u001b[39m     glob=\u001b[33m\"\u001b[39m\u001b[33m**/*.md\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m     exclude=\u001b[33m\"\u001b[39m\u001b[33mREADME.md\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     13\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m docs: \u001b[38;5;28mlist\u001b[39m[Document] = \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/eval/lib/python3.12/site-packages/langchain_community/document_loaders/directory.py:117\u001b[39m, in \u001b[36mDirectoryLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> List[Document]:\n\u001b[32m    116\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load documents.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/eval/lib/python3.12/site-packages/langchain_community/document_loaders/directory.py:195\u001b[39m, in \u001b[36mDirectoryLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m items:\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lazy_load_file(i, p, pbar)\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pbar:\n\u001b[32m    198\u001b[39m     pbar.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/eval/lib/python3.12/site-packages/langchain_community/document_loaders/directory.py:233\u001b[39m, in \u001b[36mDirectoryLoader._lazy_load_file\u001b[39m\u001b[34m(self, item, path, pbar)\u001b[39m\n\u001b[32m    231\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    232\u001b[39m         logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError loading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(item)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pbar:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/eval/lib/python3.12/site-packages/langchain_community/document_loaders/directory.py:223\u001b[39m, in \u001b[36mDirectoryLoader._lazy_load_file\u001b[39m\u001b[34m(self, item, path, pbar)\u001b[39m\n\u001b[32m    221\u001b[39m loader = \u001b[38;5;28mself\u001b[39m.loader_cls(\u001b[38;5;28mstr\u001b[39m(item), **\u001b[38;5;28mself\u001b[39m.loader_kwargs)\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubdoc\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/eval/lib/python3.12/site-packages/langchain_community/document_loaders/unstructured.py:107\u001b[39m, in \u001b[36mUnstructuredBaseLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlazy_load\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Iterator[Document]:\n\u001b[32m    106\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load file.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     elements = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     \u001b[38;5;28mself\u001b[39m._post_process_elements(elements)\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33melements\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/eval/lib/python3.12/site-packages/langchain_community/document_loaders/unstructured.py:228\u001b[39m, in \u001b[36mUnstructuredFileLoader._get_elements\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.file_path, Path):\n\u001b[32m    227\u001b[39m     \u001b[38;5;28mself\u001b[39m.file_path = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.file_path)\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43munstructured_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/eval/lib/python3.12/site-packages/unstructured/partition/auto.py:289\u001b[39m, in \u001b[36mpartition\u001b[39m\u001b[34m(filename, file, encoding, content_type, url, headers, ssl_verify, request_timeout, strategy, skip_infer_table_types, ocr_languages, languages, detect_language_per_element, pdf_infer_table_structure, extract_images_in_pdf, extract_image_block_types, extract_image_block_output_dir, extract_image_block_to_payload, data_source_metadata, metadata_filename, hi_res_model_name, model_name, starting_page_number, **kwargs)\u001b[39m\n\u001b[32m    286\u001b[39m partitioning_kwargs[\u001b[33m\"\u001b[39m\u001b[33mextract_image_block_types\u001b[39m\u001b[33m\"\u001b[39m] = extract_image_block_types\n\u001b[32m    287\u001b[39m partitioning_kwargs[\u001b[33m\"\u001b[39m\u001b[33mextract_image_block_to_payload\u001b[39m\u001b[33m\"\u001b[39m] = extract_image_block_to_payload\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m partition = \u001b[43mpartitioner_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m elements = partition(filename=filename, file=file, **partitioning_kwargs)\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m augment_metadata(elements)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/eval/lib/python3.12/site-packages/unstructured/partition/auto.py:362\u001b[39m, in \u001b[36m_PartitionerLoader.get\u001b[39m\u001b[34m(self, file_type)\u001b[39m\n\u001b[32m    359\u001b[39m \u001b[38;5;66;03m# -- if the partitioner is not in the cache, load it; note this raises if one or more of\u001b[39;00m\n\u001b[32m    360\u001b[39m \u001b[38;5;66;03m# -- the partitioner's dependencies is not installed.\u001b[39;00m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._partitioners:\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m     \u001b[38;5;28mself\u001b[39m._partitioners[file_type] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_partitioner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._partitioners[file_type]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/eval/lib/python3.12/site-packages/unstructured/partition/auto.py:371\u001b[39m, in \u001b[36m_PartitionerLoader._load_partitioner\u001b[39m\u001b[34m(self, file_type)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pkg_name \u001b[38;5;129;01min\u001b[39;00m file_type.importable_package_dependencies:\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dependency_exists(pkg_name):\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    372\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_type.partitioner_function_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m() is not available because one or\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    373\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m more dependencies are not installed. Use:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    374\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m pip install \u001b[39m\u001b[33m\"\u001b[39m\u001b[33munstructured[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_type.extra_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m (including quotes)\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    375\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m to install the required dependencies\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    376\u001b[39m         )\n\u001b[32m    378\u001b[39m \u001b[38;5;66;03m# -- load the partitioner and return it --\u001b[39;00m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m file_type.is_partitionable  \u001b[38;5;66;03m# -- would be a programming error if this failed --\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies"
     ]
    }
   ],
   "source": [
    "#! git clone https://huggingface.co/datasets/explodinggradients/ragas-airline-dataset data\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "# The path is the folder, where the documents are stored at.\n",
    "# Make sure you select the proper one.\n",
    "path: str = \"data/\"\n",
    "loader = DirectoryLoader(\n",
    "    path,\n",
    "    glob=\"**/*.md\",\n",
    "    exclude=\"README.md\"\n",
    ")\n",
    "docs: list[Document] = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting chunks from knowledge base to be used as context in data generation\n",
    "\n",
    "**Before executing the next cell:**\n",
    "* Make sure Ollama is up and running.\n",
    "* Download the required models for generation and embedding.\n",
    "* Make sure docker is up and running.\n",
    "* Activate the compose file in the root of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory already exists. Skipping download.\n",
      "Virtual environment already exists. Skipping creation.\n",
      "Dependencies already installed. Skipping installation.\n",
      "Environment is set and ready to be used\n",
      "================================================================================\n",
      "Generating context in /contexts/chunks_1.json.\n",
      "TOP_K=5\n",
      "MAX_TOKENS_TO_SAMPLE=512\n",
      "CHUNK_SIZE=512\n",
      "CHUNK_OVERLAP=0\n",
      "CHAT_MODEL=llama3.1\n",
      "TEMPERATURE=0.0\n",
      "================================================================================\n",
      "\n",
      "DELETION STEP COMPLETED...\n",
      "data/special_assistance.md: Document created and ingested successfully.\n",
      "data/managing_reservations.md: Document created and ingested successfully.\n",
      "data/flight_delays.md: Document created and ingested successfully.\n",
      "data/baggage_policies.md: Document created and ingested successfully.\n",
      "data/inflight_services.md: Document created and ingested successfully.\n",
      "data/schedule_changes.md: Document created and ingested successfully.\n",
      "data/bookings.md: Document created and ingested successfully.\n",
      "data/flight_cancellations.md: Document created and ingested successfully.\n",
      "INGESTION STEP COMPLETED...\n",
      "Extracted context 1 from document 1 and chunk 1\n",
      "Extracted context 2 from document 1 and chunk 2\n",
      "Extracted context 3 from document 1 and chunk 3\n",
      "Extracted context 4 from document 2 and chunk 1\n",
      "Extracted context 5 from document 2 and chunk 2\n",
      "Extracted context 6 from document 2 and chunk 3\n",
      "Extracted context 7 from document 3 and chunk 1\n",
      "Extracted context 8 from document 3 and chunk 2\n",
      "Extracted context 9 from document 3 and chunk 3\n",
      "Extracted context 10 from document 4 and chunk 1\n",
      "Extracted context 11 from document 4 and chunk 2\n",
      "Extracted context 12 from document 4 and chunk 3\n",
      "Extracted context 13 from document 5 and chunk 1\n",
      "Extracted context 14 from document 5 and chunk 2\n",
      "Extracted context 15 from document 5 and chunk 3\n",
      "Extracted context 16 from document 6 and chunk 1\n",
      "Extracted context 17 from document 6 and chunk 2\n",
      "Extracted context 18 from document 6 and chunk 3\n",
      "Extracted context 19 from document 7 and chunk 1\n",
      "Extracted context 20 from document 7 and chunk 2\n",
      "Extracted context 21 from document 7 and chunk 3\n",
      "Extracted context 22 from document 8 and chunk 1\n",
      "Extracted context 23 from document 8 and chunk 2\n",
      "Extracted context 24 from document 8 and chunk 3\n",
      "EXTRACTION STEP COMPLETED...\n",
      "SAVED TO JSON FILE...\n",
      "Data extracted and saved to /contexts/chunks_1.json\n"
     ]
    }
   ],
   "source": [
    "! chmod u+x ./extract_chunks.sh\n",
    "! ./extract_chunks.sh \"chunks_1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtration config** serves as a way to configure the quality of the generated synthetic input queries. Having higher threshold would ensure that the input queries are of higher quality.\n",
    "\n",
    "If the **quality_score** is still lower than the **synthetic_input_quality_threshold** after **max_quality_retries**, the **golden with the highest quality_score** will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.synthesizer.config import FiltrationConfig\n",
    "\n",
    "# (This step is completely OPTIONAL)\n",
    "# https://www.deepeval.com/docs/synthesizer-introduction\n",
    "filtration_config = FiltrationConfig(\n",
    "    synthetic_input_quality_threshold=0.7,\n",
    "    max_quality_retries=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evolutions** are used to specify the type of approach to use when trying to complicate the synthetic queries. Since this is a **RAG** application I will only use the evolution types which use **context**. The `num_evolutions` parameter can be configured to specify the number of iterations for performing those evolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.synthesizer.config import (\n",
    "    Evolution,\n",
    "    EvolutionConfig,\n",
    ")\n",
    "\n",
    "# (This step is completely OPTIONAL)\n",
    "# https://www.deepeval.com/docs/synthesizer-introduction\n",
    "evolution_config = EvolutionConfig(\n",
    "    num_evolutions=1,\n",
    "    evolutions={\n",
    "        Evolution.MULTICONTEXT: 0.25,\n",
    "        Evolution.CONCRETIZING: 0.25,\n",
    "        Evolution.CONSTRAINED: 0.25,\n",
    "        Evolution.COMPARATIVE: 0.25,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.synthesizer import Synthesizer\n",
    "\n",
    "# https://www.deepeval.com/docs/synthesizer-introduction\n",
    "synthesizer = Synthesizer(\n",
    "    filtration_config=filtration_config,\n",
    "    evolution_config=evolution_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load all the contexts that were previously generated\n",
    "with open(file=\"./contexts/chunks_1.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    context_chunks = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ú® Generating up to 48 goldens using DeepEval (using llama3.1 (Ollama), method=default):  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 39/48 [36:43<06:34, 43.88s/it]   "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepeval\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgolden\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Golden\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m goldens: \u001b[38;5;28mlist\u001b[39m[Golden] = \u001b[43msynthesizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_goldens_from_contexts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext_chunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_expected_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_goldens_per_context\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/eval/lib/python3.12/site-packages/deepeval/synthesizer/synthesizer.py:279\u001b[39m, in \u001b[36mSynthesizer.generate_goldens_from_contexts\u001b[39m\u001b[34m(self, contexts, include_expected_output, max_goldens_per_context, source_files, _context_scores, _progress_bar, _send_data, _reset_cost)\u001b[39m\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_mode:\n\u001b[32m    277\u001b[39m     loop = get_or_create_event_loop()\n\u001b[32m    278\u001b[39m     goldens.extend(\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m         \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43ma_generate_goldens_from_contexts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m                \u001b[49m\u001b[43minclude_expected_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_expected_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmax_goldens_per_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_goldens_per_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m                \u001b[49m\u001b[43msource_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m     )\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    289\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m synthesizer_progress_context(\n\u001b[32m    290\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    291\u001b[39m         num_evolutions=\u001b[38;5;28mself\u001b[39m.evolution_config.num_evolutions,\n\u001b[32m   (...)\u001b[39m\u001b[32m    297\u001b[39m         async_mode=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    298\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m progress_bar:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/eval/lib/python3.12/site-packages/nest_asyncio.py:92\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     90\u001b[39m     f._log_destroy_pending = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/eval/lib/python3.12/site-packages/nest_asyncio.py:115\u001b[39m, in \u001b[36m_patch_loop.<locals>._run_once\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m     heappop(scheduled)\n\u001b[32m    110\u001b[39m timeout = (\n\u001b[32m    111\u001b[39m     \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[32m    113\u001b[39m         scheduled[\u001b[32m0\u001b[39m]._when - \u001b[38;5;28mself\u001b[39m.time(), \u001b[32m0\u001b[39m), \u001b[32m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._process_events(event_list)\n\u001b[32m    118\u001b[39m end_time = \u001b[38;5;28mself\u001b[39m.time() + \u001b[38;5;28mself\u001b[39m._clock_resolution\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/selectors.py:468\u001b[39m, in \u001b[36mEpollSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    466\u001b[39m ready = []\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    470\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ú® Generating up to 48 goldens using DeepEval (using llama3.1 (Ollama), method=default):  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 45/48 [39:40<02:38, 52.90s/it]\n"
     ]
    }
   ],
   "source": [
    "from deepeval.dataset.golden import Golden\n",
    "\n",
    "goldens: list[Golden] = synthesizer.generate_goldens_from_contexts(\n",
    "    contexts=context_chunks,\n",
    "    include_expected_output=True,\n",
    "    max_goldens_per_context=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confident AI\n",
    "\n",
    "1. In short **Confident AI** is a cloud-based platform part of the **DeepEval** framework, which stores **datasets**, **evaluations** and **monitoring data**. \n",
    "\n",
    "2. If you want to use **Confident AI** platform create an account from here: [Confident AI](https://www.confident-ai.com/)\n",
    "\n",
    "3. After signing-up an **API key** will be generated, which can be used to interact with the platform from inside the notebook.\n",
    "\n",
    "---\n",
    "\n",
    "Example of .env file:\n",
    "```bash\n",
    "DEEPEVAL_RESULTS_FOLDER=<folder> # Results of evaluations can be saved locally\n",
    "DEEPEVAL_API_KEY=<your api key>  # Relevant if you want to use Confident AI\n",
    "DEEPEVAL_TELEMETRY_OPT_OUT=\"YES\" # Remove telemetry\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">üéâü•≥ Congratulations! You've successfully logged in! üôå \n",
       "</pre>\n"
      ],
      "text/plain": [
       "üéâü•≥ Congratulations! You've successfully logged in! üôå \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from deepeval import login_with_confident_api_key\n",
    "\n",
    "# Loads the environment variables from a `.env` file.\n",
    "# If you want to use Confident AI be sure to create one in this directory.\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "deepeval_api_key: str = os.getenv(\"DEEPEVAL_API_KEY\")\n",
    "\n",
    "# You should get a message letting you know you are logged-in.\n",
    "login_with_confident_api_key(deepeval_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to visit the link provided, upon invoking the `push` method. This will redirect you to the page containing the `goldens`. Then you can clean-up the data and that would almost always be mandatory, since we are using a weak model in the project and the input will not always be **clean**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚úÖ Dataset successfully pushed to Confident AI! View at \n",
       "<a href=\"https://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/datasets/cma2e7zy50rj4vkijvxke9jra\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/datasets/cma2e7zy50rj4vkijvxke9jra</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚úÖ Dataset successfully pushed to Confident AI! View at \n",
       "\u001b]8;id=648580;https://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/datasets/cma2e7zy50rj4vkijvxke9jra\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/datasets/cma2e7zy50rj4vkijvxke9jra\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gtk-Message: 12:57:34.499: Failed to load module \"canberra-gtk-module\"\n",
      "Gtk-Message: 12:57:34.500: Failed to load module \"canberra-gtk-module\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening in existing browser session.\n"
     ]
    }
   ],
   "source": [
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "dataset = EvaluationDataset(goldens=goldens)\n",
    "# Make sure to rename it in the folder since the name is auto-generated\n",
    "dataset.save_as(\n",
    "    file_type=\"json\",\n",
    "    directory=\"./datasets\",\n",
    "    include_test_cases=False # Since this is still a golden (actual_output is missing)\n",
    ")\n",
    "dataset.push(\n",
    "    alias=os.getenv(\"DATASET_ALIAS\"),\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "269be0f223ea4a9aa4d8412f80ae01fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "# I did some cleaning on the data since the input was not fully in the expected format on the ConfidentAI platform.\n",
    "final_dataset = EvaluationDataset()\n",
    "final_dataset.pull(alias=os.getenv(\"DATASET_ALIAS\"))\n",
    "\n",
    "# Saving the data locally so I can use it in a script.\n",
    "# Since R2R and DeepEval have conflicting dependencies a virtual environment with both of these \n",
    "# libraries doesn't work. They need to be separated. (`Ollama` is the conflicting package)\n",
    "json_out: list[dict] = []\n",
    "for golden in final_dataset.goldens:\n",
    "    json_out.append(golden.model_dump())\n",
    "\n",
    "# Save json data\n",
    "with open(\"deepeval_dataset.json\", \"w\") as f:\n",
    "    json.dump(json_out, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of `actual response` and `retrieval context`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If you already have a custom prompt which is ingested in R2R you can provide the name as an argument to the script.\n",
    "! chmod u+x ./fill_dataset.sh\n",
    "! ./fill_dataset.sh # Optional argument: custom_prompt_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚úÖ Dataset successfully pushed to Confident AI! View at \n",
       "<a href=\"https://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/datasets/cma40qrhu025kdfxvjjyc4a0c\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/datasets/cma40qrhu025kdfxvjjyc4a0c</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚úÖ Dataset successfully pushed to Confident AI! View at \n",
       "\u001b]8;id=809503;https://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/datasets/cma40qrhu025kdfxvjjyc4a0c\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/datasets/cma40qrhu025kdfxvjjyc4a0c\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gtk-Message: 16:15:48.222: Failed to load module \"canberra-gtk-module\"\n",
      "Gtk-Message: 16:15:48.223: Failed to load module \"canberra-gtk-module\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening in existing browser session.\n"
     ]
    }
   ],
   "source": [
    "# After having all of the data push the full dataset to ConfidentAI\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "# Make sure you specify the correct name below\n",
    "with open(\"full_deepeval_dataset.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "full_dataset = EvaluationDataset()\n",
    "full_dataset.add_goldens_from_json_file(\n",
    "    file_path=\"full_deepeval_dataset.json\",\n",
    "    input_key_name=\"input\",\n",
    "    actual_output_key_name=\"actual_output\",\n",
    "    expected_output_key_name=\"expected_output\",\n",
    "    context_key_name=\"context\",\n",
    "    retrieval_context_key_name=\"retrieval_context\"\n",
    ")\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "full_dataset.push(\n",
    "    alias=os.getenv(\"DATASET_ALIAS\"),\n",
    "    overwrite=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
