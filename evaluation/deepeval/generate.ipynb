{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a synthetic dataset using DeepEval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthesizer\n",
    "\n",
    "This object can be used to generate **Golden** instances, which consist out of **input**, **expected output** and **context**. It uses a LLM to come up with random input and thereafter tries to enhance those, by making them more complex and realistic.\n",
    "\n",
    "For a comprehensive guide on understanding how this object works please refer here: [Synthesizer](https://www.confident-ai.com/blog/the-definitive-guide-to-synthetic-data-generation-using-llms)\n",
    "\n",
    "### Summary\n",
    "\n",
    "I will try to summarize the most important information:\n",
    "\n",
    "* It uses a **LLM to come-up with a comprehensive dataset** much faster than a human can\n",
    "* The process starts with the LLM generating **synthetic queries** based on context from a knowledge base - usually documents\n",
    "* Those initial queries are then **evolved** to reflect real-life complexity and then together with the context can be used to generate a **target/expected output**\n",
    "\n",
    "![Dataset generation workflow](https://cdn.prod.website-files.com/64bd90bdba579d6cce245aec/670574639fc6b9d5c483d766_664050ef1eb43f5fb8f57ff8_diagram.png \"Synthetic generation\")\n",
    "\n",
    "* There exist two main methods:\n",
    "    - Self-improvement: Iteratively uses the LLMs output to generate more complex queries\n",
    "    - Distillation: A stronger model is being utilized \n",
    "\n",
    "* Constructing contexts:\n",
    "    - During this phase documents from the knowledge base are split using a token splitter\n",
    "    - A random chunk is selected\n",
    "    - Finally, additional chunks are retrieved based on **semantic similarity**, **knowledge graphs** or others\n",
    "    - Ensuring that **chunk size**, **chunk overlap** or other similar parameters here and in the **retrieval component** of the **RAG** application are identical will yield better results\n",
    "\n",
    "![Constructing contexts](https://cdn.prod.website-files.com/64bd90bdba579d6cce245aec/672cb28e9f8f60aabd382788_672cb201dadd3fd2de4451d2_context_generation.png \"Context construction\")\n",
    "\n",
    "* Constructing synthetic queries:\n",
    "    - Using the contexts the **Synthesizer** can now generate synthetic input\n",
    "    - Doing so we ensure that the input corresponds with the context enhancing the **relevancy** and **accuracy**\n",
    "\n",
    "![Constructing synthetic queries](https://cdn.prod.website-files.com/64bd90bdba579d6cce245aec/672cb28e9f8f60aabd382775_672cb23c502672c70e0372cd_asymmetry.png \"Synthetic queries creation\")\n",
    "\n",
    "* Data Filtering:\n",
    "    1. Context filtering: Removes low-quality chunks that may be unintelligible\n",
    "\n",
    "    ![Context filtering](https://cdn.prod.website-files.com/64bd90bdba579d6cce245aec/672cb28e9f8f60aabd38278b_672cb26b461b45b0b5a6cd30_context_filtering.png \"Filtering context\")\n",
    "\n",
    "    2. Input filtering: Ensures generated inputs meet quality standards\n",
    "\n",
    "    ![Input filtering](https://cdn.prod.website-files.com/64bd90bdba579d6cce245aec/672cb28e9f8f60aabd382772_672cb27b799642a337436c3f_input_filtering.png \"Filtering queries\")\n",
    "    \n",
    "* Customizing dataset generating:\n",
    "    - Depending on the scenario inputs and outputs can be tailored to specific use cases\n",
    "        - For example a medical chatbot would have a completely different behaviour than a scientific one. It would need to comfort patients.\n",
    "    \n",
    "* Data Evolution:\n",
    "    - **In-Depth Evolving**: Expands simple instructions into more detailed versions\n",
    "    - **In-Breadth Evolving**: Produces diverse instructions to enrich the dataset\n",
    "    - **Elimination Evolving**: Removes less effective instructions\n",
    "\n",
    "    ![Data evolution](https://cdn.prod.website-files.com/64bd90bdba579d6cce245aec/670574639fc6b9d5c483d763_6641a0d7ef709f365d888577_Screenshot%25202024-05-13%2520at%25201.10.30%2520PM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies:\n",
    "\n",
    "* To install the dependencies run the `setup` bash script in the root of the `evaluation` folder.\n",
    "* Make sure you select the correct kernel in your notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: deepeval\n",
      "Version: 2.7.9\n",
      "Summary: The LLM Evaluation Framework\n",
      "Home-page: https://github.com/confident-ai/deepeval\n",
      "Author: Jeffrey Ip\n",
      "Author-email: jeffreyip@confident-ai.com\n",
      "License: Apache-2.0\n",
      "Location: /home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/evaluation/eval/lib/python3.12/site-packages\n",
      "Requires: aiohttp, anthropic, black, coverage, google-genai, grpcio, langchain_community, langchain_openai, nest_asyncio, ollama, openai, opentelemetry-api, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-sdk, portalocker, posthog, pytest, pytest-asyncio, pytest-repeat, pytest-rerunfailures, pytest-xdist, requests, rich, sentry-sdk, setuptools, tabulate, tenacity, tqdm, twine, typer, wheel\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# After installing the dependencies and selecting the kernel you should be good to go.\n",
    "# Make sure the package is installed before continuing further.\n",
    "!pip3 show deepeval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM provider\n",
    "\n",
    "**DeepEval** uses **OpenAI** by default as a LLM provider, however **Ollama** is also available. To use it execute the code cell below. This will generate a `.deepeval` file where key-value pairs will be stored about that particular LLM-provider like model name, base url and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ™Œ Congratulations! You're now using a local Ollama model for all evals that \n",
      "require an LLM.\n",
      "ðŸ™Œ Congratulations! You're now using Ollama embeddings for all evals that \n",
      "require text embeddings.\n"
     ]
    }
   ],
   "source": [
    "!deepeval set-ollama llama3.1 --base-url=\"http://localhost:11434/\"\n",
    "!deepeval set-ollama-embeddings mxbai-embed-large --base-url=\"http://localhost:11434\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting chunks from knowledge base to be used as context in data generation\n",
    "\n",
    "**Before executing the next cell:**\n",
    "* Make sure Ollama is up and running.\n",
    "* Download the required models for generation and embedding.\n",
    "* Make sure docker is up and running.\n",
    "* Activate the compose file in the root of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory already exists. Skipping download.\n",
      "Virtual environment already exists. Skipping creation.\n",
      "Dependencies already installed. Skipping installation.\n",
      "Environment is set and ready to be used\n",
      "Error when creating document: {'message': 'Document 88fa13ca-5921-590f-8693-408b1ed047bf already exists. Submit a DELETE request to `/documents/{document_id}` to delete this document and allow for re-ingestion.', 'error_type': 'R2RException'}\n",
      "Error when creating document: {'message': 'Document bf55e614-3330-5283-a759-ea1bfa15a655 already exists. Submit a DELETE request to `/documents/{document_id}` to delete this document and allow for re-ingestion.', 'error_type': 'R2RException'}\n",
      "Error when creating document: {'message': 'Document d7c24a75-99ba-5b84-8339-5a9188be0580 already exists. Submit a DELETE request to `/documents/{document_id}` to delete this document and allow for re-ingestion.', 'error_type': 'R2RException'}\n",
      "Error when creating document: {'message': 'Document 2a9978ac-84fd-5644-8633-dcc90e19c123 already exists. Submit a DELETE request to `/documents/{document_id}` to delete this document and allow for re-ingestion.', 'error_type': 'R2RException'}\n",
      "Error when creating document: {'message': 'Document a5ef7ed6-02c2-5a86-9299-fff661c1e7f7 already exists. Submit a DELETE request to `/documents/{document_id}` to delete this document and allow for re-ingestion.', 'error_type': 'R2RException'}\n",
      "Error when creating document: {'message': 'Document 6530a034-daa1-5198-b340-563b5e45ca2b already exists. Submit a DELETE request to `/documents/{document_id}` to delete this document and allow for re-ingestion.', 'error_type': 'R2RException'}\n",
      "Error when creating document: {'message': 'Document db0faff8-f57f-5459-91f4-92f611106fa1 already exists. Submit a DELETE request to `/documents/{document_id}` to delete this document and allow for re-ingestion.', 'error_type': 'R2RException'}\n",
      "Error when creating document: {'message': 'Document 15bea571-fbd0-5ed6-a1c2-5bb70b6b7f36 already exists. Submit a DELETE request to `/documents/{document_id}` to delete this document and allow for re-ingestion.', 'error_type': 'R2RException'}\n",
      "INGESTION STEP COMPLETED...\n",
      "Extracted context 1 from document 1 and chunk 1\n",
      "Extracted context 2 from document 1 and chunk 2\n",
      "Extracted context 3 from document 1 and chunk 3\n",
      "Extracted context 4 from document 2 and chunk 1\n",
      "Extracted context 5 from document 2 and chunk 2\n",
      "Extracted context 6 from document 2 and chunk 3\n",
      "Extracted context 7 from document 3 and chunk 1\n",
      "Extracted context 8 from document 3 and chunk 2\n",
      "Extracted context 9 from document 3 and chunk 3\n",
      "Extracted context 10 from document 4 and chunk 1\n",
      "Extracted context 11 from document 4 and chunk 2\n",
      "Extracted context 12 from document 4 and chunk 3\n",
      "Extracted context 13 from document 5 and chunk 1\n",
      "Extracted context 14 from document 5 and chunk 2\n",
      "Extracted context 15 from document 5 and chunk 3\n",
      "Extracted context 16 from document 6 and chunk 1\n",
      "Extracted context 17 from document 6 and chunk 2\n",
      "Extracted context 18 from document 6 and chunk 3\n",
      "Extracted context 19 from document 7 and chunk 1\n",
      "Extracted context 20 from document 7 and chunk 2\n",
      "Extracted context 21 from document 7 and chunk 3\n",
      "Extracted context 22 from document 8 and chunk 1\n",
      "Extracted context 23 from document 8 and chunk 2\n",
      "Extracted context 24 from document 8 and chunk 3\n",
      "EXTRACTION STEP COMPLETED...\n",
      "SAVED TO JSON FILE...\n",
      "Data extracted and saved to chunks.json\n",
      "chunks.json\t       evaluate.ipynb\t  fill_dataset.py  prompts\n",
      "data\t\t       extract_chunks.py  fill_dataset.sh  r2r_venv\n",
      "deepeval_dataset.json  extract_chunks.sh  generate.ipynb\n"
     ]
    }
   ],
   "source": [
    "!chmod u+x ./extract_chunks.sh\n",
    "!./extract_chunks.sh\n",
    "!ls # There should be a chunks.json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtration config** serves as a way to configure the quality of the generated synthetic input queries. Having higher threshold would ensure that the input queries are of higher quality.\n",
    "\n",
    "If the **quality_score** is still lower than the **synthetic_input_quality_threshold** after **max_quality_retries**, the **golden with the highest quality_score** will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.synthesizer.config import FiltrationConfig\n",
    "\n",
    "# (This step is completely OPTIONAL)\n",
    "# https://www.deepeval.com/docs/synthesizer-introduction\n",
    "filtration_config = FiltrationConfig(\n",
    "    synthetic_input_quality_threshold=0.7,\n",
    "    max_quality_retries=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evolutions** are used to specify the type of approach to use when trying to complicate the synthetic queries. Since this is a **RAG** application I will only use the evolution types which use **context**. The `num_evolutions` parameter can be configured to specify the number of iterations for performing those evolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.synthesizer.config import (\n",
    "    Evolution,\n",
    "    EvolutionConfig,\n",
    ")\n",
    "\n",
    "# (This step is completely OPTIONAL)\n",
    "# https://www.deepeval.com/docs/synthesizer-introduction\n",
    "evolution_config = EvolutionConfig(\n",
    "    num_evolutions=1,\n",
    "    evolutions={\n",
    "        Evolution.MULTICONTEXT: 0.25,\n",
    "        Evolution.CONCRETIZING: 0.25,\n",
    "        Evolution.CONSTRAINED: 0.25,\n",
    "        Evolution.COMPARATIVE: 0.25,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.synthesizer import Synthesizer\n",
    "\n",
    "# https://www.deepeval.com/docs/synthesizer-introduction\n",
    "synthesizer = Synthesizer(\n",
    "    filtration_config=filtration_config,\n",
    "    evolution_config=evolution_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load all the contexts that were previously generated\n",
    "with open(file=\"chunks.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    context_chunks = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "âœ¨ Generating up to 48 goldens using DeepEval (using llama3.1 (Ollama), method=default): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [1:03:49<00:00, 79.79s/it]  \n"
     ]
    }
   ],
   "source": [
    "from deepeval.dataset.golden import Golden\n",
    "\n",
    "goldens: list[Golden] = synthesizer.generate_goldens_from_contexts(\n",
    "    contexts=context_chunks,\n",
    "    include_expected_output=True,\n",
    "    max_goldens_per_context=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confident AI\n",
    "\n",
    "1. In short **Confident AI** is a cloud-based platform part of the **DeepEval** framework, which stores **datasets**, **evaluations** and **monitoring data**. \n",
    "\n",
    "2. If you want to use **Confident AI** platform create an account from here: [Confident AI](https://www.confident-ai.com/)\n",
    "\n",
    "3. After signing-up an **API key** will be generated, which can be used to interact with the platform from inside the notebook.\n",
    "\n",
    "---\n",
    "\n",
    "Example of .env file:\n",
    "```bash\n",
    "DEEPEVAL_RESULTS_FOLDER=<folder> # Results of evaluations can be saved locally\n",
    "DEEPEVAL_API_KEY=<your api key>  # Relevant if you want to use Confident AI\n",
    "DEEPEVAL_TELEMETRY_OPT_OUT=\"YES\" # Remove telemetry\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ðŸŽ‰ðŸ¥³ Congratulations! You've successfully logged in! ðŸ™Œ \n",
       "</pre>\n"
      ],
      "text/plain": [
       "ðŸŽ‰ðŸ¥³ Congratulations! You've successfully logged in! ðŸ™Œ \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from deepeval import login_with_confident_api_key\n",
    "\n",
    "# Loads the environment variables from a `.env` file.\n",
    "# If you want to use Confident AI be sure to create one in this directory.\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "deepeval_api_key: str = os.getenv(\"DEEPEVAL_API_KEY\")\n",
    "\n",
    "# You should get a message letting you know you are logged-in.\n",
    "login_with_confident_api_key(deepeval_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to visit the link provided, upon invoking the `push` method. This will redirect you to the page containing the `goldens`. Then you can clean-up the data and that would almost always be mandatory, since we are using a weak model in the project and the input will not always be **clean**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ… Dataset successfully pushed to Confident AI! View at \n",
       "<a href=\"https://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/datasets/cma2e7zy50rj4vkijvxke9jra\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/datasets/cma2e7zy50rj4vkijvxke9jra</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âœ… Dataset successfully pushed to Confident AI! View at \n",
       "\u001b]8;id=648580;https://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/datasets/cma2e7zy50rj4vkijvxke9jra\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/datasets/cma2e7zy50rj4vkijvxke9jra\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gtk-Message: 12:57:34.499: Failed to load module \"canberra-gtk-module\"\n",
      "Gtk-Message: 12:57:34.500: Failed to load module \"canberra-gtk-module\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening in existing browser session.\n"
     ]
    }
   ],
   "source": [
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "dataset = EvaluationDataset(goldens=goldens)\n",
    "dataset.push(alias=os.getenv(\"DATASET_ALIAS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "269be0f223ea4a9aa4d8412f80ae01fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "# I did some cleaning on the data since the input was not fully in the expected format on the ConfidentAI platform.\n",
    "final_dataset = EvaluationDataset()\n",
    "final_dataset.pull(alias=os.getenv(\"DATASET_ALIAS\"))\n",
    "\n",
    "# Saving the data locally so I can use it in a script.\n",
    "# Since R2R and DeepEval have conflicting dependencies a virtual environment with both of these \n",
    "# libraries doesn't work. They need to be separated. (`Ollama` is the conflicting package)\n",
    "json_out: list[dict] = []\n",
    "for golden in final_dataset.goldens:\n",
    "    json_out.append(golden.model_dump())\n",
    "\n",
    "# Save json data\n",
    "with open(\"deepeval_dataset.json\", \"w\") as f:\n",
    "    json.dump(json_out, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of `actual response` and `retrieval context`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If you already have a custom prompt which is ingested in R2R you can provide the name as an argument to the script.\n",
    "!chmod u+x ./fill_dataset.sh\n",
    "!./fill_dataset.sh # Optional argument: custom_prompt_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After having all of the data push the full dataset to ConfidentAI\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from deepeval.dataset.golden import Golden\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "# Make sure you specify the correct name below\n",
    "with open(\"full_deepeval_dataset.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "full_dataset = EvaluationDataset()\n",
    "for golden in data:\n",
    "    full_dataset.goldens.append(Golden(**golden))\n",
    "    \n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "full_dataset.push(\n",
    "    alias=os.getenv(\"DATASET_ALIAS\"),\n",
    "    overwrite=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
