{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "* Execute the script in the root of the `evaluation` folder.\n",
    "* If executing the script fails run: `chmod u+x setup.sh`.\n",
    "* It will install all required dependencies.\n",
    "* Finally, make sure you select it in the notebook by specifying `eval` as kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confident AI\n",
    "\n",
    "1. In short **Confident AI** is a cloud-based platform fully compatible with the **DeepEval** project, which can store generated **datasets**, results of **evaluations**. \n",
    "\n",
    "2. If you want to use **Confident AI** platform create an account from here: [Confident AI](https://www.confident-ai.com/)\n",
    "\n",
    "3. After signing-up an **API key** will be generated, which can be used to interact with the platform from inside the notebook.\n",
    "\n",
    "4. Do bear in mind that if you want to use **ConfidentAI** with **DeepEval** you need to always keep `deepeval` as the most recent version, otherwise you will get an exception.\n",
    "\n",
    "---\n",
    "\n",
    "Example of `.env` file:\n",
    "```bash\n",
    "DEEPEVAL_RESULTS_FOLDER=<folder> # Results of evaluations can be saved locally\n",
    "DEEPEVAL_API_KEY=<your api key>  # Relevant if you want to use Confident AI\n",
    "DEEPEVAL_TELEMETRY_OPT_OUT=\"YES\" # Remove telemetry\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ðŸŽ‰ðŸ¥³ Congratulations! You've successfully logged in! ðŸ™Œ \n",
       "</pre>\n"
      ],
      "text/plain": [
       "ðŸŽ‰ðŸ¥³ Congratulations! You've successfully logged in! ðŸ™Œ \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from deepeval import login_with_confident_api_key\n",
    "\n",
    "# Loads the environment variables from a `.env` file.\n",
    "# If you want to use Confident AI be sure to create one parent directory with the variables.\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "deepeval_api_key: str = os.getenv(\"DEEPEVAL_API_KEY\")\n",
    "\n",
    "# You should get a message letting you know you are logged-in.\n",
    "login_with_confident_api_key(deepeval_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMTestCase\n",
    "\n",
    "Unlike **RAGAs**, where a single interaction between a user and the AI system is represented by either a **SingleTurnSample** or **MultiTurnSample**, in **DeepEval** there's the concept of so called **LLMTestCase** and **ConversationalTestCase**. For this project the **LLMTestCase** will be of relevance. Just like in **RAGAs**, **LLMTestCase** objects have the same fields just different names - input, actual_output, expected_output, etc.\n",
    "\n",
    "![Image showcasing what a LLMTestCase is.](https://deepeval-docs.s3.amazonaws.com/llm-test-case.svg \"LLMTestCase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "**Evaluation** should be a crucial component of every single application which uses **AI**. **DeepEval** provides more than 14 metrics for evaluation so that one can very easily iterate towards a better LLM application. Each default metric uses **LLM-As-A-Judge**. Optionally, one can use the **GEval** to set a custom criteria for evaluation if neither of the other metrics meet the requirements. Alternatively, there's the **DAGMetric**, whose purpose is similar to the **GEval**, however it uses a graph and it's fully **deterministic**.\n",
    "\n",
    "When evaluating a test case, multiple metrics can be used and the test would be **positive** iff all the **metrics thresholds** have been exceeded and **negative** in any other case. \n",
    "\n",
    "Evaluation workflow:\n",
    "![Image of evaluation workflox](https://d2lsxfc3p6r9rv.cloudfront.net/workflow.png \"Evaluation comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Dataset\n",
    "\n",
    "An **evaluation dataset** is just a collection of **LLMTestCase**- or so called **Golden** objects. A **Golden** is structurally the same as a **LLMTestCase**, however it has no `actual_output` and `retrieval_context` fields, which can be generated by your LLM at evaluation time.\n",
    "\n",
    "Datasets can be **pushed**, **stored** and **pulled** from **Confident AIs** platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM provider\n",
    "\n",
    "**DeepEval** uses **OpenAI** by default as a LLM, however **Ollama** is also available. To use it execute the code cell below. This will generate a `.deepeval` file where key-value pairs will be stored about that particular LLM-provider like model name, base url and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ™Œ Congratulations! You're now using a local Ollama model for all evals that \n",
      "require an LLM.\n",
      "ðŸ™Œ Congratulations! You're now using Ollama embeddings for all evals that \n",
      "require text embeddings.\n"
     ]
    }
   ],
   "source": [
    "!deepeval set-ollama llama3.1:latest --base-url=\"http://localhost:11434/\"\n",
    "!deepeval set-ollama-embeddings mxbai-embed-large --base-url=\"http://localhost11434\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling a **dataset** from **Confident AI**\n",
    "\n",
    "* If you already have a dataset on the platform just use the `pull` method and specify the name/alias.\n",
    "* Alternatively, you can save your synthetic dataset on disk and import it from a `json` or `csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "evaluation_dataset = EvaluationDataset()\n",
    "evaluation_dataset.pull(\n",
    "    os.getenv(\"DATASET_ALIAS\"),\n",
    "    auto_convert_goldens_to_test_cases=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for **RAGAs**\n",
    "\n",
    "**DeepEval** supports the original **RAGAs** metrics, so the cell below just initializes the required structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "load_dotenv(\"../../env/rag.env\")\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=os.getenv(\"CHAT_MODEL\"),\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=float(os.getenv(\"TEMPERATURE\")),\n",
    "    num_ctx=int(os.getenv(\"LLM_CONTEXT_WINDOW_TOKENS\")),\n",
    "    format=\"json\"\n",
    ")\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=os.getenv(\"EMBEDDING_MODEL\"),\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating a dataset using metrics\n",
    "\n",
    "Evaluation in **DeepEval** works as follow:\n",
    "* Pull or create a dataset containing test cases or goldens:\n",
    "    - In the case of **goldens** have the LLM generate the `actual_output` and `retrieval_context` fields.\n",
    "    - Usually the **goldens** are preferred since one can modify the `prompt template` during evaluation.\n",
    "* Think about the application and the different use cases:\n",
    "    - What is it doing?\n",
    "    - How could I assure it's doing what it's supposed to be doing?\n",
    "    - Check out the existing metrics or create your own one.\n",
    "    - Create metric/s.\n",
    "* Run the evaluation:\n",
    "    - In notebooks use the `evaluate` method.\n",
    "    - If you choose to create test cases in python file/s:\n",
    "        - The filename/s should start with `test_` and can be ran using `deepeval test run <filename>`.\n",
    "        - Optionally, pass in flags like `-c` to use the cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Answer Relevancy**\n",
    "\n",
    "It uses the **LLM as a judge** to determine how well the response answers the user input. This metric is especially relevant when it comes to testing the **RAG pipelines generator** by determining the degree of relevance of the generated output with respect to the query. The main **hyperparamter** being tested by this metric is the `prompt_template` that the **LLM** receives as instruction to create an output (can it instruct the LLM to output relevant and helpful outputs based on the `retrieval_context`).\n",
    "\n",
    "---\n",
    "\n",
    "### Approach:\n",
    "\n",
    "The LLM is used to decompose the `actual_output` into claims and then each of those claims get classified with respect to the `input` by the LLM:\n",
    "* `yes` if relevant\n",
    "* `no` if irrelevant\n",
    "* `idk` if non-determined or partially relevant.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula:\n",
    "\n",
    "* Number of relevant statements = Statements marked as `yes` or `idk`  \n",
    "* **Answer Relevancy** = $\\frac{\\text{Number of relevant statements}}{\\text{Total number of statements}}$\n",
    "\n",
    "---\n",
    "\n",
    "### Framework Comparison:\n",
    "\n",
    "This metric is very similar if not the same as the **Response Relevance** in **RAGAs** in terms of what it evaluates, however it does it in a diffrerent way. In **RAGAs**, hypothetical questions get generated based on the response, their embeddings are computed and using **Semantic similarity** the mean of those scores is computed. In **DeepEval** the response is decomposed into claims and for each the LLM determines its relevance to the query/input (no Embedding model is required). \n",
    "\n",
    "|           | Answer Relevancy | Response Relevancy |\n",
    "|-----------|------------------|-------------|\n",
    "| Framework | DeepEval | RAGAs |\n",
    "| Approach  | Statemement decomposition + LLM-as-a-judge  | Hypothetical questions + Semantic similarity  |\n",
    "| Score     |  $\\in{[0-1]} $ | $\\in{[0-1]} $ |\n",
    "| Component | Generator      | Generator     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.metrics.ragas import RAGASAnswerRelevancyMetric\n",
    "from prompts.custom_answer_relevancy_prompt import MyAnswerRelevancyTemplate\n",
    "\n",
    "answer_relevancy = AnswerRelevancyMetric(\n",
    "    evaluation_template=MyAnswerRelevancyTemplate,\n",
    "    include_reason=False\n",
    ")\n",
    "ragas_answer_relevancy = RAGASAnswerRelevancyMetric(\n",
    "    model=llm,\n",
    "    embeddings=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Faithfulness**\n",
    "\n",
    "The **Faithfulness** metric measures how factually consistent a response is with the retrieval context. It ranges from 0 to 1, with higher scores indicating better factual consistency. The metrics tests the `generator` component in the RAG pipeline and tries to verify if the output contradicts factual information from the **retrieval_context**.\n",
    "\n",
    "A response is considered faithful if its claims can be supported by the retrieved context (ideally we would want a response whose claims/statements are all supported by a node/chunk in the context).\n",
    "\n",
    "---\n",
    "\n",
    "### Approach:\n",
    "\n",
    "1. **Identify Claims**:\n",
    "   - Break down the response into individual statements\n",
    "\n",
    "2. **Identify Truths**:\n",
    "   - Break down the retrieval context into individual statements\n",
    "   - One can configure the number of `truths` to be extracted using the `truths_extraction_limit` parameter\n",
    "\n",
    "3. **Use LLM**\n",
    "    - Using the LLM to determine if a claim doesn't contradict a truth from the context\n",
    "\n",
    "---\n",
    "\n",
    "### Formula:\n",
    "\n",
    "* Number of truthful claims = Statements/claims from the response marked as `yes` or `idk` **(not contradicting a truth)**\n",
    "* **Faithfulness** = $\\frac{\\text{Number of truthful claims}}{\\text{Total number of claims}}$\n",
    "\n",
    "---\n",
    "\n",
    "### Framework Comparison:\n",
    "\n",
    "|              | Faithfulness | Faithfulness |\n",
    "|--------------|-------------------------|---------------------|\n",
    "| Framework    | DeepEval                | RAGAS               |\n",
    "| Approach     | Statement decomposition, LLM-as-a-judge | Statement decomposition, LLM-as-a-judge |\n",
    "| Components   | Generator             | Generator           |\n",
    "| Scoring      | $\\in{[0-1]}$          | $\\in{[0-1]}$        |\n",
    "| Customization| - | Can use HHEM-2.1-Open classifier |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import FaithfulnessMetric\n",
    "from deepeval.metrics.ragas import RAGASFaithfulnessMetric\n",
    "from prompts.custom_faithfulness_prompt import MyFaithfulnessTemplate \n",
    "\n",
    "faithfulness = FaithfulnessMetric(\n",
    "    include_reason=False,\n",
    "    evaluation_template=MyFaithfulnessTemplate\n",
    ")\n",
    "\n",
    "ragas_faithfulness = RAGASFaithfulnessMetric(\n",
    "    model=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Contextual Precision**\n",
    "\n",
    "The **Contextual Precision** metric measures the the **re-rankers capability to rank relevant** `nodes`/`pieces of context` higher than irrelevant ones. This ensures that important information is in the beginning of the context, which may lead to better generation. Usually a LLM focuses more on information which is found at the beginning of the context. Information which is ranked higher than it is supposed to be leads to penalties (lower score).\n",
    "\n",
    "evaluates whether the reranker in your retriever ranks more relevant nodes in your retrieval context higher than irrelevant ones.\n",
    "\n",
    "---\n",
    "\n",
    "### Approach:\n",
    "\n",
    "The **LLM** determines for each **piece of context** whether or not it's relevant for answering the **input** with respect to the **expected output**.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula:\n",
    "\n",
    "* **Note that not every node which was retrieved would be relevant**\n",
    "* **k** is the rank (position of retrieved node context)\n",
    "* **n** is the total number of retrieved nodes\n",
    "* **$r_{k}$** $\\in \\{0, 1\\} $\n",
    "* **Contextual Precision** = $ \\frac{1}{\\text{Number of relevant nodes}} \\times \\sum_{k=1}^{n} (\\frac{\\text{Number of relevant Nodes up to Rank k}}{\\text{k}} \\times r_{k}) $\n",
    "\n",
    "---\n",
    "\n",
    "### Framework Comparison:\n",
    "\n",
    "Both metrics work the exact same way and use the same formula, so one would expect almost identical results.\n",
    "\n",
    "|              | Contextual Precision | Context Precision |\n",
    "|--------------|-------------------------|---------------------|\n",
    "| Framework    | DeepEval                | RAGAS               |\n",
    "| Approach     | LLM-as-a-judge | LLM-as-a-judge |\n",
    "| Components   | Retriever             | Retriever           |\n",
    "| Scoring      | $\\in{[0-1]}$          | $\\in{[0-1]}$        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import ContextualPrecisionMetric\n",
    "from deepeval.metrics.ragas import RAGASContextualPrecisionMetric\n",
    "\n",
    "contextual_precision = ContextualPrecisionMetric(\n",
    "    threshold=0.7,\n",
    ")\n",
    "\n",
    "ragas_contextual_precision = RAGASContextualPrecisionMetric(\n",
    "    threshold=0.5,\n",
    "    model=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Contextual Recall**\n",
    "\n",
    "The **Contextual Recall** metric measures the extent to which the `retrieval_context` aligns with the `expected_output`. This metric penalizes a **retriever** which misses important/relevant nodes during retrieval, since that would lead to an incomplete response. It uses the `expected output` as a reference for the `retrieval context`.\n",
    "\n",
    "evaluates whether the embedding model in your retriever is able to accurately capture and retrieve relevant information based on the context of the input.\n",
    "\n",
    "---\n",
    "\n",
    "### Approach:\n",
    "\n",
    "The **LLM** determines for each statement in the `expected_output` whether or not it can be attributed to a node from the `retrieval context`.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula:\n",
    "\n",
    "**Attributable statement** is one, which contains information present in a node of the context. \n",
    "* **Contextual Recall** = $ \\frac{\\text{Number of attributable statements}}{\\text{Total number of statements}} $\n",
    "\n",
    "---\n",
    "\n",
    "### Framework Comparison:\n",
    "\n",
    "Both metrics work almost identically. The `expected output` is decomposed into statements and for each of them the LLM determines if it can be attributed to a node from the `retrieved context`. The only minor difference is that in **RAGAs** the input is also explicitly used in determining that.\n",
    "\n",
    "|              | Contextual Precision | Context Precision |\n",
    "|--------------|-------------------------|---------------------|\n",
    "| Framework    | DeepEval                | RAGAS               |\n",
    "| Approach     | LLM-as-a-judge | LLM-as-a-judge |\n",
    "| Components   | Retriever             | Retriever           |\n",
    "| Scoring      | $\\in{[0-1]}$          | $\\in{[0-1]}$        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import ContextualRecallMetric\n",
    "from deepeval.metrics.ragas import RAGASContextualRecallMetric\n",
    "from prompts.llama31_contextual_recall_prompt import Llama31ContextualRecallTemplate\n",
    "\n",
    "contextual_recall = ContextualRecallMetric(\n",
    "    threshold=0.7,\n",
    "    evaluation_template=Llama31ContextualRecallTemplate\n",
    ")\n",
    "\n",
    "ragas_contextual_precision = RAGASContextualRecallMetric(\n",
    "    threshold=0.5,\n",
    "    model=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Contextual Relevancy**\n",
    "\n",
    "The **Contextual Relevancy** metric measures the extent to which the `retrieval_context` is useful for generating a proper response to the **input**.\n",
    "\n",
    "evaluates whether the text chunk size and top-K of your retriever is able to retrieve information without much irrelevancies.\n",
    "\n",
    "---\n",
    "\n",
    "### Approach:\n",
    "\n",
    "The **LLM** receives the **input** and **retrieval_context** and is prompted to decide for each statement found in the context, whether or not it can be of use for answering the input.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula:\n",
    "\n",
    "**Relevant statement** is a statement found in the context, which can be utilized to answer the user input. \n",
    "* **Contextual Relevancy** = $ \\frac{\\text{Number of relevant statements}}{\\text{Total number of statements}} $\n",
    "\n",
    "---\n",
    "\n",
    "### Framework Comparison:\n",
    "\n",
    "No such metric is available in **RAGAs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import ContextualRelevancyMetric\n",
    "from prompts.llama31_contextual_relevancy_prompt import Llama31ContextualRelevancyTemplate\n",
    "\n",
    "contextual_relevancy = ContextualRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    evaluation_template=Llama31ContextualRelevancyTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ContextualEntitiesRecall**\n",
    "\n",
    "This metric is not available in **DeepEval**. For explanation of how it works, please go to `/evaluation/ragas/evalute.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics.ragas import RAGASContextualEntitiesRecall\n",
    "\n",
    "ragas_contextual_entities_recall = RAGASContextualEntitiesRecall(\n",
    "    threshold=0.5,\n",
    "    model=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hallucination**\n",
    "\n",
    "The **Hallucination** metric measures the ability of the **LLM** to properly utilize context. In some instances a perfect prompt template with all the relevant context, can still result in an invalid or partially invalid response, which contradicts data from the knowledge base. This metric ensures that such cases are penalized.\n",
    "\n",
    "---\n",
    "\n",
    "### Approach:\n",
    "\n",
    "The **LLM** receives a list of `context` nodes (data which will be part of the knowledge base) and the `actual_output`. Unlike the `Faithfulness` metric where statements from the `actual_output` are compared agains nodes in the `retrieval_context` this one uses nodes from the `context` and ensures there're no contradictions in the `actual_output`. A contradiction is considered a statement/claim in the `actual_output` which is fully inconsistent. Missing information is not a contradiction.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula:\n",
    " \n",
    "* **Hallucination** = $ \\frac{\\text{Number of contradicted contexts}}{\\text{Total number of contexts}} $\n",
    "\n",
    "---\n",
    "\n",
    "### Framework Comparison:\n",
    "\n",
    "No such metric is available in **RAGAs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import HallucinationMetric\n",
    "from prompts.llama31_hallucination_prompt import Llama31HallucinationTemplate\n",
    "\n",
    "hallucination = HallucinationMetric(\n",
    "    threshold=0.7,\n",
    "    evaluation_template=Llama31HallucinationTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepeval\n",
    "\n",
    "@deepeval.log_hyperparameters(model=\"gpt-4\", prompt_template=\"...\")\n",
    "def custom_parameters():\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.evaluate.configs import (\n",
    "    AsyncConfig,\n",
    "    CacheConfig,\n",
    "    DisplayConfig,\n",
    "    TestRunResultDisplay\n",
    ")\n",
    "\n",
    "async_conf = AsyncConfig(\n",
    "    run_async=True,\n",
    "    throttle_value=120,\n",
    "    max_concurrent=10\n",
    ")\n",
    "\n",
    "cache_conf = CacheConfig(\n",
    "    write_cache=True,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "display_conf = DisplayConfig(\n",
    "    show_indicator=True,\n",
    "    print_results=True,\n",
    "    verbose_mode=False,\n",
    "    # Displays only the failed tests\n",
    "    # The ones whose score didn't exceed the threshold\n",
    "    display_option=TestRunResultDisplay.FAILING\n",
    ")\n",
    "\n",
    "# There's also an `ErrorConfig` for further customization\n",
    "# evaluate(\n",
    "    \n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
