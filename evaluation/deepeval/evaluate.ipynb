{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "* Execute the script in the root of the `evaluation` folder.\n",
    "* If executing the script fails run: `chmod u+x setup.sh`.\n",
    "* It will install all required dependencies.\n",
    "* Finally, make sure you select it in the notebook by specifying `eval` as kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confident AI\n",
    "\n",
    "1. In short **Confident AI** is a cloud-based platform part of the **DeepEval** framework, which stores **datasets**, **evaluations** and **monitoring data**. \n",
    "\n",
    "2. If you want to use **Confident AI** platform create an account from here: [Confident AI](https://www.confident-ai.com/)\n",
    "\n",
    "3. After signing-up an **API key** will be generated, which can be used to interact with the platform from inside the notebook.\n",
    "\n",
    "---\n",
    "\n",
    "Example of .env file:\n",
    "```bash\n",
    "DEEPEVAL_RESULTS_FOLDER=<folder> # Results of evaluations can be saved locally\n",
    "DEEPEVAL_API_KEY=<your api key>  # Relevant if you want to use Confident AI\n",
    "DEEPEVAL_TELEMETRY_OPT_OUT=\"YES\" # Remove telemetry\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ðŸŽ‰ðŸ¥³ Congratulations! You've successfully logged in! ðŸ™Œ \n",
       "</pre>\n"
      ],
      "text/plain": [
       "ðŸŽ‰ðŸ¥³ Congratulations! You've successfully logged in! ðŸ™Œ \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from deepeval import login_with_confident_api_key\n",
    "\n",
    "# Loads the environment variables from a `.env` file.\n",
    "# If you want to use Confident AI be sure to create one parent directory.\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "deepeval_api_key: str = os.getenv(\"DEEPEVAL_API_KEY\")\n",
    "\n",
    "# You should get a message letting you know you are logged-in.\n",
    "login_with_confident_api_key(deepeval_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMTestCase\n",
    "\n",
    "Unlike **RAGAs**, where a single interaction between a user and the AI system is represented by either a **SingleTurnSample** or **MultiTurnSample**, in **DeepEval** there's the concept of so called **LLMTestCase** and **ConversationalTestCase**. For this project the **LLMTestCase** will be of relevance. Just like in **RAGAs**, **LLMTestCase** objects have the same fields just different names - input, actual_output, expected_output, etc.\n",
    "\n",
    "![Image showcasing what a LLMTestCase is.](https://deepeval-docs.s3.amazonaws.com/llm-test-case.svg \"LLMTestCase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "**Evaluation** should be a crucial component of every single application which uses **AI**. **DeepEval** provides more than 14 metrics for evaluation so that one can very easily iterate towards a better LLM application. Each default metric uses **LLM-As-A-Judge**. Optionally, one can use the **GEval** to set a custom criteria for evaluation if neither of the other metrics meet the requirements. Alternatively, there's the **DAGMetric**, whose purpose is similar to the **GEval**, however it uses a graph and it's fully **deterministic**.\n",
    "\n",
    "When evaluating a test case, multiple metrics can be used and the test would be **positive** iff all the **metrics thresholds** have been exceeded and **negative** in any other case. \n",
    "\n",
    "Evaluation workflow:\n",
    "![Image of evaluation workflox](https://d2lsxfc3p6r9rv.cloudfront.net/workflow.png \"Evaluation comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Dataset\n",
    "\n",
    "An **evaluation dataset** is just a collection of **LLMTestCase**- or so called **Golden** objects. A **Golden** is structurally the same as a **LLMTestCase**, however it has no `actual_output` and `retrieval_context` fields, which can be generated by your LLM at evaluation time.\n",
    "\n",
    "Datasets can be **pushed**, **stored** and **pulled** from **Confident AIs** platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM provider\n",
    "\n",
    "**DeepEval** uses **OpenAI** by default as a LLM, however **Ollama** is also available. To use it execute the code cell below. This will generate a `.deepeval` file where key-value pairs will be stored about that particular LLM-provider like model name, base url and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ™Œ Congratulations! You're now using a local Ollama model for all evals that \n",
      "require an LLM.\n",
      "ðŸ™Œ Congratulations! You're now using Ollama embeddings for all evals that \n",
      "require text embeddings.\n"
     ]
    }
   ],
   "source": [
    "!deepeval set-ollama llama3.1:latest --base-url=\"http://localhost:11434/\"\n",
    "!deepeval set-ollama-embeddings mxbai-embed-large --base-url=\"http://localhost11434\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling a **dataset** from **Confident AI**\n",
    "\n",
    "If you already have a dataset on the platform just use the `pull` method and specify the name/alias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "evaluation_dataset = EvaluationDataset()\n",
    "evaluation_dataset.pull(\n",
    "    os.getenv(\"DATASET_ALIAS\"),\n",
    "    auto_convert_goldens_to_test_cases=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating a dataset/set of test cases using metric/s\n",
    "\n",
    "Evaluation in **DeepEval** works as follow:\n",
    "* Pull or create a dataset containing test cases or goldens:\n",
    "    - In the case of **goldens** have the LLM generate the `actual_output` and `retrieval_context` fields.\n",
    "    - Usually the **goldens** are preferred since one can modify the `prompt template` during evaluation.\n",
    "* Think about the application and the different use cases:\n",
    "    - What is it doing?\n",
    "    - How could I assure it's doing what it's supposed to be doing?\n",
    "    - Check out the existing metrics or create your own one.\n",
    "    - Create metric/s.\n",
    "* Run the evaluation:\n",
    "    - In notebooks use the `evaluate` method.\n",
    "    - If you choose to create test cases in python file/s:\n",
    "        - The filename/s should start with `test_` and can be ran using `deepeval test run <filename>`.\n",
    "        - Optionally, pass in flags like `-c` to use the cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Answer Relevancy**\n",
    "\n",
    "It uses the **LLM as a judge** to determine how well the response answers the user input. This metric is especially relevant when it comes to testing the **RAG pipelines generator** by determining the degree of relevance of the generated output with respect to the query. \n",
    "\n",
    "---\n",
    "\n",
    "### Approach:\n",
    "\n",
    "The LLM is used to retrieve claims from the response and then they get compared with the user input. The LLM classifies each statement as follows:\n",
    "* `yes` if relevant\n",
    "* `no` if irrelevant\n",
    "* `idk` if non-determined or partially relevant.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula:\n",
    "\n",
    "* Number of relevant statements = Statements marked as `yes` or `idk`  \n",
    "* **Answer Relevancy** = $\\frac{\\text{Number of relevant statements}}{\\text{Total number of statements}}$\n",
    "\n",
    "---\n",
    "\n",
    "This metric is very similar if not the same as the **Response Relevance** in **RAGAs** in terms of what it evaluates, however it does it in a diffrerent way. In **RAGAs**, hypothetical questions get generated based on the response, their embeddings are computed and using **Semantic similarity** the mean of those scores is computed. In **DeepEval** the response is decomposed into claims and for each the LLM determines its relevance to the query/input (no Embedding model is required). \n",
    "\n",
    "|           | Answer Relevancy | Response Relevancy |\n",
    "|-----------|------------------|-------------|\n",
    "| Framework | DeepEval | RAGAs |\n",
    "| Approach  | Statemement decomposition + LLM-as-a-judge  | Hypothetical questions + Semantic similarity  |\n",
    "| Score     |  $\\in{[0-1]} $ | $\\in{[0-1]} $ |\n",
    "| Component | Generator      | Generator     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.evaluate import evaluate, EvaluationResult\n",
    "from prompts.llama31_answer_relevancy_prompt import Llama31AnswerRelevancyTemplate\n",
    "\n",
    "answer_relevancy = AnswerRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    evaluation_template=Llama31AnswerRelevancyTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Faithfulness**\n",
    "\n",
    "The **Faithfulness** metric measures how factually consistent a response is with the retrieval context. It ranges from 0 to 1, with higher scores indicating better factual consistency.\n",
    "\n",
    "A response is considered faithful if its claims can be supported by the retrieved context.\n",
    "\n",
    "---\n",
    "\n",
    "### Approach:\n",
    "\n",
    "1. **Identify Claims**:\n",
    "   - Break down the response into individual statements\n",
    "\n",
    "2. **Identify Truths**:\n",
    "   - Break down the retrieval context into individual statements\n",
    "   - One can configure the number of `truths` to be extracted using the `truths_extraction_limit` parameter\n",
    "\n",
    "3. **Use LLM**\n",
    "    - Using the LLM to determine if a claim doesn't contradict a truth from the context\n",
    "\n",
    "---\n",
    "\n",
    "### Formula:\n",
    "\n",
    "* Number of truthful claims = Statements/claims from the response marked as `yes` or `idk` **(not contradicting a truth)**\n",
    "* **Faithfulness** = $\\frac{\\text{Number of truthful claims}}{\\text{Total number of claims}}$\n",
    "\n",
    "---\n",
    "\n",
    "### Framework Comparison:\n",
    "\n",
    "|              | Faithfulness | Faithfulness |\n",
    "|--------------|-------------------------|---------------------|\n",
    "| Framework    | DeepEval                | RAGAS               |\n",
    "| Approach     | Statement decomposition, LLM-as-a-judge | Statement decomposition, LLM-as-a-judge |\n",
    "| Components   | Generator             | Generator           |\n",
    "| Scoring      | $\\in{[0-1]}$          | $\\in{[0-1]}$        |\n",
    "| Customization| - | Can use HHEM-2.1-Open classifier |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import FaithfulnessMetric\n",
    "from prompts.llama31_faithfulness_prompt import Llama31FaithfulnessTemplate \n",
    "\n",
    "faithfulness = FaithfulnessMetric(\n",
    "    threshold=0.7,\n",
    "    evaluation_template=Llama31FaithfulnessTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Contextual Precision**\n",
    "\n",
    "The **Contextual Precision** metric measures the the **re-rankers capability to rank relevant** `nodes`/`pieces of context` higher than irrelevant ones. This ensures that important information is in the beginning of the context, which may lead to better generation. Usually a LLM focuses more on information which is found at the beginning of the context. Information which is ranked higher than it is supposed to be leads to penalties (lower score).\n",
    "\n",
    "---\n",
    "\n",
    "### Approach:\n",
    "\n",
    "The **LLM** determines for each **piece of context** whether or not it's relevant for answering the **input** with respect to the **expected output**.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula:\n",
    "\n",
    "* **Note that not every node which was retrieved would be relevant**\n",
    "* **k** is the rank (position of retrieved node context)\n",
    "* **n** is the total number of retrieved nodes\n",
    "* **$r_{k}$** $\\in \\{0, 1\\} $\n",
    "* **Contextual Precision** = $ \\frac{1}{\\text{Number of relevant nodes}} \\times \\sum_{k=1}^{n} (\\frac{\\text{Number of relevant Nodes up to Rank k}}{\\text{k}} \\times r_{k}) $\n",
    "\n",
    "---\n",
    "\n",
    "### Framework Comparison:\n",
    "\n",
    "Both metrics work the exact same way and use the same formula, so one would expect almost identical results.\n",
    "\n",
    "|              | Contextual Precision | Context Precision |\n",
    "|--------------|-------------------------|---------------------|\n",
    "| Framework    | DeepEval                | RAGAS               |\n",
    "| Approach     | LLM-as-a-judge | LLM-as-a-judge |\n",
    "| Components   | Retriever             | Retriever           |\n",
    "| Scoring      | $\\in{[0-1]}$          | $\\in{[0-1]}$        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import ContextualPrecisionMetric\n",
    "\n",
    "contextual_precision = ContextualPrecisionMetric(\n",
    "    threshold=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Contextual Recall**\n",
    "\n",
    "The **Contextual Recall** metric measures the extent to which the `retrieval_context` aligns with the `expected_output`. This metric penalizes a **retriever** which misses important/relevant nodes during retrieval, since that would lead to an incomplete response. It uses the `expected output` as a reference for the `retrieval context`.\n",
    "\n",
    "---\n",
    "\n",
    "### Approach:\n",
    "\n",
    "The **LLM** determines for each statement in the `expected_output` whether or not it can be attributed to a node from the `retrieval context`.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula:\n",
    "\n",
    "**Attributable statement** is one, which contains information present in a node of the context. \n",
    "* **Contextual Precision** = $ \\frac{\\text{Number of attributable statements}}{\\text{Total number of statements}} $\n",
    "\n",
    "---\n",
    "\n",
    "### Framework Comparison:\n",
    "\n",
    "Both metrics work almost identically. The `expected output` is decomposed into statements and for each of them the LLM determines if it can be attributed to a node from the `retrieved context`. The only minor difference is that in **RAGAs** the input is also explicitly used in determining that.\n",
    "\n",
    "|              | Contextual Precision | Context Precision |\n",
    "|--------------|-------------------------|---------------------|\n",
    "| Framework    | DeepEval                | RAGAS               |\n",
    "| Approach     | LLM-as-a-judge | LLM-as-a-judge |\n",
    "| Components   | Retriever             | Retriever           |\n",
    "| Scoring      | $\\in{[0-1]}$          | $\\in{[0-1]}$        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import ContextualRecallMetric\n",
    "from prompts.llama31_contextual_recall_prompt import Llama31ContextualRecallTemplate\n",
    "\n",
    "contextual_recall = ContextualRecallMetric(\n",
    "    threshold=0.7,\n",
    "    evaluation_template=Llama31ContextualRecallTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Contextual Relevancy**\n",
    "\n",
    "The **Contextual Recall** metric measures the extent to which the `retrieval_context` aligns with the `expected_output`. This metric penalizes a **retriever** which misses important/relevant nodes during retrieval, since that would lead to an incomplete response. It uses the `expected output` as a reference for the `retrieval context`.\n",
    "\n",
    "---\n",
    "\n",
    "### Approach:\n",
    "\n",
    "The **LLM** determines for each statement in the `expected_output` whether or not it can be attributed to a node from the `retrieval context`.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula:\n",
    "\n",
    "**Attributable statement** is one, which contains information present in a node of the context. \n",
    "* **Contextual Precision** = $ \\frac{\\text{Number of attributable statements}}{\\text{Total number of statements}} $\n",
    "\n",
    "---\n",
    "\n",
    "### Framework Comparison:\n",
    "\n",
    "Both metrics work almost identically. The `expected output` is decomposed into statements and for each of them the LLM determines if it can be attributed to a node from the `retrieved context`. The only minor difference is that in **RAGAs** the input is also explicitly used in determining that.\n",
    "\n",
    "|              | Contextual Precision | Context Precision |\n",
    "|--------------|-------------------------|---------------------|\n",
    "| Framework    | DeepEval                | RAGAS               |\n",
    "| Approach     | LLM-as-a-judge | LLM-as-a-judge |\n",
    "| Components   | Retriever             | Retriever           |\n",
    "| Scoring      | $\\in{[0-1]}$          | $\\in{[0-1]}$        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import ContextualRelevancyMetric"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
