{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "* Execute the script in the root of the `evaluation` folder.\n",
    "* If executing the script fails run: `chmod u+x setup.sh`.\n",
    "* It will install all required dependencies.\n",
    "* Finally, make sure you select it in the notebook by specifying `eval` as kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confident AI\n",
    "\n",
    "1. In short **Confident AI** is a cloud-based platform part of the **DeepEval** framework, which stores **datasets**, **evaluations** and **monitoring data**. \n",
    "\n",
    "2. If you want to use **Confident AI** platform create an account from here: [Confident AI](https://www.confident-ai.com/)\n",
    "\n",
    "3. After signing-up an **API key** will be generated, which can be used to interact with the platform from inside the notebook.\n",
    "\n",
    "---\n",
    "\n",
    "Example of .env file:\n",
    "```bash\n",
    "DEEPEVAL_RESULTS_FOLDER=<folder> # Results of evaluations can be saved locally\n",
    "DEEPEVAL_API_KEY=<your api key>  # Relevant if you want to use Confident AI\n",
    "DEEPEVAL_TELEMETRY_OPT_OUT=\"YES\" # Remove telemetry\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ðŸŽ‰ðŸ¥³ Congratulations! You've successfully logged in! ðŸ™Œ \n",
       "</pre>\n"
      ],
      "text/plain": [
       "ðŸŽ‰ðŸ¥³ Congratulations! You've successfully logged in! ðŸ™Œ \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from deepeval import login_with_confident_api_key\n",
    "\n",
    "# Loads the environment variables from a `.env` file.\n",
    "# If you want to use Confident AI be sure to create one parent directory.\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "deepeval_api_key: str = os.getenv(\"DEEPEVAL_API_KEY\")\n",
    "\n",
    "# You should get a message letting you know you are logged-in.\n",
    "login_with_confident_api_key(deepeval_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMTestCase\n",
    "\n",
    "Unlike **RAGAs**, where a single interaction between a user and the AI system is represented by either a **SingleTurnSample** or **MultiTurnSample**, in **DeepEval** there's the concept of so called **LLMTestCase** and **ConversationalTestCase**. For this project the **LLMTestCase** will be of relevance. Just like in **RAGAs**, **LLMTestCase** objects have the same fields just different names - input, actual_output, expected_output, etc.\n",
    "\n",
    "![Image showcasing what a LLMTestCase is.](https://deepeval-docs.s3.amazonaws.com/llm-test-case.svg \"LLMTestCase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "**Evaluation** should be a crucial component of every single application which uses **AI**. **DeepEval** provides more than 14 metrics for evaluation so that one can very easily iterate towards a better LLM application. Each default metric uses **LLM-As-A-Judge**. Optionally, one can use the **GEval** to set a custom criteria for evaluation if neither of the other metrics meet the requirements. Alternatively, there's the **DAGMetric**, whose purpose is similar to the **GEval**, however it uses a graph and it's fully **deterministic**.\n",
    "\n",
    "When evaluating a test case, multiple metrics can be used and the test would be **positive** iff all the **metrics thresholds** have been exceeded and **negative** in any other case. \n",
    "\n",
    "Evaluation workflow:\n",
    "![Image of evaluation workflox](https://d2lsxfc3p6r9rv.cloudfront.net/workflow.png \"Evaluation comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Dataset\n",
    "\n",
    "An **evaluation dataset** is just a collection of **LLMTestCase**- or so called **Golden** objects. A **Golden** is structurally the same as a **LLMTestCase**, however it has no `actual_output` and `retrieval_context` fields, which can be generated by your LLM at evaluation time.\n",
    "\n",
    "Datasets can be **pushed**, **stored** and **pulled** from **Confident AIs** platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM provider\n",
    "\n",
    "**DeepEval** uses **OpenAI** by default as a LLM, however **Ollama** is also available. To use it execute the code cell below. This will generate a `.deepeval` file where key-value pairs will be stored about that particular LLM-provider like model name, base url and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ™Œ Congratulations! You're now using a local Ollama model for all evals that \n",
      "require an LLM.\n",
      "ðŸ™Œ Congratulations! You're now using Ollama embeddings for all evals that \n",
      "require text embeddings.\n"
     ]
    }
   ],
   "source": [
    "!deepeval set-ollama llama3.1:latest --base-url=\"http://localhost:11434/\"\n",
    "!deepeval set-ollama-embeddings mxbai-embed-large --base-url=\"http://localhost11434\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling a **dataset** from **Confident AI**\n",
    "\n",
    "If you already have a dataset on the platform just use the `pull` method and specify the name/alias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f8f92372ea4067a483ca4c28735753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "evaluation_dataset = EvaluationDataset()\n",
    "evaluation_dataset.pull(\n",
    "    os.getenv(\"DATASET_ALIAS\"),\n",
    "    auto_convert_goldens_to_test_cases=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating a dataset/set of test cases using metric/s\n",
    "\n",
    "Evaluation in **DeepEval** works as follow:\n",
    "* Pull or create a dataset containing test cases or goldens:\n",
    "    - In the case of **goldens** have the LLM generate the `actual_output` and `retrieval_context` fields.\n",
    "    - Usually the **goldens** are preferred since one can modify the `prompt template` during evaluation.\n",
    "* Think about the application and the different use cases:\n",
    "    - What is it doing?\n",
    "    - How could I assure it's doing what it's supposed to be doing?\n",
    "    - Check out the existing metrics or create your own one.\n",
    "    - Create metric/s.\n",
    "* Run the evaluation:\n",
    "    - In notebooks use the `evaluate` method.\n",
    "    - If you choose to create test cases in python file/s:\n",
    "        - The filename/s should start with `test_` and can be ran using `deepeval test run <filename>`.\n",
    "        - Optionally, pass in flags like `-c` to use the cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Answer Relevancy**\n",
    "\n",
    "It uses the **LLM as a judge** to determine how well the response answers the user input. This metric is especially relevant when it comes to testing the **RAG pipelines generator** by determining the degree of relevance of the generated output with respect to the query. \n",
    "\n",
    "---\n",
    "\n",
    "### Approach:\n",
    "\n",
    "The LLM is used to retrieve claims from the response and then they get compared with the user input. The LLM classifies each statement as follows:\n",
    "* `yes` if relevant\n",
    "* `no` if irrelevant\n",
    "* `idk` if non-determined or partially relevant.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula:\n",
    "\n",
    "* Number of relevant statements = Statements marked as `yes` or `idk`  \n",
    "* **Answer Relevancy** = $\\frac{\\text{Number of relevant statements}}{\\text{Total number of statements}}$\n",
    "\n",
    "---\n",
    "\n",
    "This metric is very similar if not the same as the **Response Relevance** in **RAGAs** in terms of what it evaluates, however it does it in a diffrerent way. In **RAGAs**, hypothetical questions get generated based on the response, their embeddings are computed and using **Semantic similarity** the mean of those scores is computed. In **DeepEval** the response is decomposed into claims and for each the LLM determines its relevance to the query/input (no Embedding model is required). \n",
    "\n",
    "|           | Answer Relevancy | Response Relevancy |\n",
    "|-----------|------------------|-------------|\n",
    "| Framework | DeepEval | RAGAs |\n",
    "| Approach  | Statemement decomposition + LLM-as-a-judge  | Hypothetical questions + Semantic similarity  |\n",
    "| Score     |  $\\in{[0-1]} $ | $\\in{[0-1]} $ |\n",
    "| Component | Generator      | Generator     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.evaluate import evaluate, EvaluationResult\n",
    "from prompts.llama31_answer_relevancy_prompt import Llama31AnswerRelevancyTemplate\n",
    "\n",
    "answer_relevancy = AnswerRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    evaluation_template=Llama31AnswerRelevancyTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Faithfulness**\n",
    "\n",
    "The **Faithfulness** metric measures how factually consistent a response is with the retrieval context. It ranges from 0 to 1, with higher scores indicating better consistency.\n",
    "\n",
    "A response is considered faithful if its claims can be supported by the retrieved context.\n",
    "\n",
    "---\n",
    "\n",
    "### Required arguments:\n",
    "\n",
    "* `user input`\n",
    "* `actual output`\n",
    "* `retrieval context`\n",
    "\n",
    "---\n",
    "\n",
    "### Approach:\n",
    "\n",
    "1. **Identify Claims**:\n",
    "   - Break down the response into individual statements\n",
    "\n",
    "2. **Identify Truths**:\n",
    "   - Break down the retrieval context into individual statements\n",
    "\n",
    "3. **Use NLI**\n",
    "    - Using NLI determine if a claim in the response is factually based on claim/s in the context\n",
    "\n",
    "---\n",
    "\n",
    "### Formula:\n",
    "\n",
    "__Faithfulness__ = $\\frac{\\text{Number of truthful claims}}{\\text{Total number of claims}}$\n",
    "\n",
    "---\n",
    "\n",
    "### Framework Comparison:\n",
    "\n",
    "|              | Faithfulness | Faithfulness |\n",
    "|--------------|-------------------------|---------------------|\n",
    "| Framework    | DeepEval                | RAGAS               |\n",
    "| Approach     | Statement decomposition, LLM-as-a-judge with NLI | Statement decomposition, LLM-as-a-judge with NLI |\n",
    "| Components   | Generator             | Generator           |\n",
    "| Scoring      | $\\in{[0-1]}$          | $\\in{[0-1]}$        |\n",
    "| Customization| - | Can use HHEM-2.1-Open classifier |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using llama3.</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">1</span><span style=\"color: #374151; text-decoration-color: #374151\">:latest </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">Ollama</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing llama3.\u001b[0m\u001b[1;38;2;55;65;81m1\u001b[0m\u001b[38;2;55;65;81m:latest \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mOllama\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 10 test case(s) sequentially: |          |  0% (0/10) [Time Taken: 00:00, ?test case/s]"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/\n",
       "lib/python3.12/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/\n",
       "lib/python3.12/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 10 test case(s) sequentially: |          |  0% (0/10) [Time Taken: 02:46, ?test case/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Llama31FaithfulnessTemplate.generate_reason() got an unexpected keyword argument 'contradictions'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/deepeval/evaluate.py:350\u001b[39m, in \u001b[36mexecute_test_cases.<locals>.evaluate_test_cases\u001b[39m\u001b[34m(pbar)\u001b[39m\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m     \u001b[43mmetric\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmeasure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest_case\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_show_indicator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_metric_indicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m MissingTestCaseParamsError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/deepeval/metrics/faithfulness/faithfulness.py:81\u001b[39m, in \u001b[36mFaithfulnessMetric.measure\u001b[39m\u001b[34m(self, test_case, _show_indicator)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28mself\u001b[39m.score = \u001b[38;5;28mself\u001b[39m._calculate_score()\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[38;5;28mself\u001b[39m.reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_reason\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mself\u001b[39m.success = \u001b[38;5;28mself\u001b[39m.score >= \u001b[38;5;28mself\u001b[39m.threshold\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/deepeval/metrics/faithfulness/faithfulness.py:164\u001b[39m, in \u001b[36mFaithfulnessMetric._generate_reason\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    162\u001b[39m         contradictions.append(verdict.reason)\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m prompt = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluation_template\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_reason\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontradictions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontradictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.2f\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.using_native_model:\n",
      "\u001b[31mTypeError\u001b[39m: Llama31FaithfulnessTemplate.generate_reason() got an unexpected keyword argument 'contradictions'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mprompts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllama31_faithfulness_prompt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Llama31FaithfulnessTemplate \n\u001b[32m      5\u001b[39m faithfulness = FaithfulnessMetric(\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m#threshold=0.7,\u001b[39;00m\n\u001b[32m      7\u001b[39m     verbose_mode=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      8\u001b[39m     evaluation_template=Llama31FaithfulnessTemplate\n\u001b[32m      9\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m results: EvaluationResult = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_cases\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluation_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtest_cases\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfaithfulness\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43midentifier\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFaithfulness with my own template Cases [0:10]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_async\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/deepeval/evaluate.py:1083\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(test_cases, metrics, hyperparameters, run_async, show_indicator, print_results, write_cache, use_cache, ignore_errors, skip_on_missing_params, verbose_mode, identifier, throttle_value, max_concurrent, display)\u001b[39m\n\u001b[32m   1067\u001b[39m         test_results = loop.run_until_complete(\n\u001b[32m   1068\u001b[39m             a_execute_test_cases(\n\u001b[32m   1069\u001b[39m                 test_cases,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1080\u001b[39m             )\n\u001b[32m   1081\u001b[39m         )\n\u001b[32m   1082\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1083\u001b[39m         test_results = \u001b[43mexecute_test_cases\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtest_cases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1085\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[43m            \u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1087\u001b[39m \u001b[43m            \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[43m            \u001b[49m\u001b[43mverbose_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1089\u001b[39m \u001b[43m            \u001b[49m\u001b[43msave_to_disk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrite_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[43m            \u001b[49m\u001b[43mskip_on_missing_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_on_missing_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1091\u001b[39m \u001b[43m            \u001b[49m\u001b[43midentifier\u001b[49m\u001b[43m=\u001b[49m\u001b[43midentifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m            \u001b[49m\u001b[43mshow_indicator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_indicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m end_time = time.perf_counter()\n\u001b[32m   1096\u001b[39m run_duration = end_time - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/deepeval/evaluate.py:562\u001b[39m, in \u001b[36mexecute_test_cases\u001b[39m\u001b[34m(test_cases, metrics, skip_on_missing_params, ignore_errors, use_cache, show_indicator, save_to_disk, verbose_mode, identifier, test_run_manager, _use_bar_indicator)\u001b[39m\n\u001b[32m    555\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m show_indicator \u001b[38;5;129;01mand\u001b[39;00m _use_bar_indicator:\n\u001b[32m    556\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m tqdm(\n\u001b[32m    557\u001b[39m         desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_cases)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m test case(s) sequentially\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    558\u001b[39m         unit=\u001b[33m\"\u001b[39m\u001b[33mtest case\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    559\u001b[39m         total=\u001b[38;5;28mlen\u001b[39m(test_cases),\n\u001b[32m    560\u001b[39m         bar_format=\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{desc}\u001b[39;00m\u001b[33m: |\u001b[39m\u001b[38;5;132;01m{bar}\u001b[39;00m\u001b[33m|\u001b[39m\u001b[38;5;132;01m{percentage:3.0f}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{n_fmt}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{total_fmt}\u001b[39;00m\u001b[33m) [Time Taken: \u001b[39m\u001b[38;5;132;01m{elapsed}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{rate_fmt}\u001b[39;00m\u001b[38;5;132;01m{postfix}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    561\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m         \u001b[43mevaluate_test_cases\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    564\u001b[39m     evaluate_test_cases()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/deepeval/evaluate.py:365\u001b[39m, in \u001b[36mexecute_test_cases.<locals>.evaluate_test_cases\u001b[39m\u001b[34m(pbar)\u001b[39m\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    364\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m365\u001b[39m         \u001b[43mmetric\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmeasure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_case\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    366\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m MissingTestCaseParamsError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    367\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m skip_on_missing_params:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/deepeval/metrics/faithfulness/faithfulness.py:81\u001b[39m, in \u001b[36mFaithfulnessMetric.measure\u001b[39m\u001b[34m(self, test_case, _show_indicator)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28mself\u001b[39m.verdicts = \u001b[38;5;28mself\u001b[39m._generate_verdicts()\n\u001b[32m     80\u001b[39m \u001b[38;5;28mself\u001b[39m.score = \u001b[38;5;28mself\u001b[39m._calculate_score()\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[38;5;28mself\u001b[39m.reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_reason\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mself\u001b[39m.success = \u001b[38;5;28mself\u001b[39m.score >= \u001b[38;5;28mself\u001b[39m.threshold\n\u001b[32m     83\u001b[39m \u001b[38;5;28mself\u001b[39m.verbose_logs = construct_verbose_logs(\n\u001b[32m     84\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     85\u001b[39m     steps=[\n\u001b[32m   (...)\u001b[39m\u001b[32m     90\u001b[39m     ],\n\u001b[32m     91\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/deepeval/metrics/faithfulness/faithfulness.py:164\u001b[39m, in \u001b[36mFaithfulnessMetric._generate_reason\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    161\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m verdict.verdict.strip().lower() == \u001b[33m\"\u001b[39m\u001b[33mno\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    162\u001b[39m         contradictions.append(verdict.reason)\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m prompt = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluation_template\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_reason\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontradictions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontradictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.2f\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.using_native_model:\n\u001b[32m    170\u001b[39m     res, cost = \u001b[38;5;28mself\u001b[39m.model.generate(prompt, schema=Reason)\n",
      "\u001b[31mTypeError\u001b[39m: Llama31FaithfulnessTemplate.generate_reason() got an unexpected keyword argument 'contradictions'"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import FaithfulnessMetric\n",
    "from deepeval.evaluate import evaluate, EvaluationResult\n",
    "#from prompts.llama31_faithfulness_prompt import Llama31FaithfulnessTemplate \n",
    "\n",
    "faithfulness = FaithfulnessMetric(\n",
    "    #threshold=0.7,\n",
    "    verbose_mode=True,\n",
    "    evaluation_template=Llama31FaithfulnessTemplate\n",
    ")\n",
    "\n",
    "results: EvaluationResult = evaluate(\n",
    "    test_cases=evaluation_dataset.test_cases[0:10],\n",
    "    metrics=[faithfulness],\n",
    "    identifier=\"Faithfulness with my own template Cases [0:10]\",\n",
    "    run_async=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Contextual Precision**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_case_3 - 0.75\n",
      "test_case_0 - 0.5714285714285714\n",
      "test_case_4 - 0.75\n",
      "test_case_5 - 0.6666666666666666\n",
      "test_case_9 - 0.6666666666666666\n",
      "test_case_8 - 1.0\n",
      "test_case_1 - 1.0\n",
      "test_case_7 - 0.7142857142857143\n",
      "test_case_2 - 0.8571428571428571\n",
      "test_case_6 - 1.0\n"
     ]
    }
   ],
   "source": [
    "for result in results.test_results:\n",
    "    print(f\"{result.name} - {result.metrics_data[0].score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
