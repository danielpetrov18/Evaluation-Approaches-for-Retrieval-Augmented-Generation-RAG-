{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "* Execute the script in the root of the `evaluation` folder.\n",
    "* If executing the script fails run: `chmod u+x setup.sh`.\n",
    "* It will install all required dependencies.\n",
    "* Finally, make sure you select it in the notebook by specifying `eval` as kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confident AI\n",
    "\n",
    "1. In short **Confident AI** is a cloud-based platform fully compatible with the **DeepEval** project, which can store generated **datasets**, results of **evaluations**. \n",
    "\n",
    "2. If you want to use **Confident AI** platform create an account from here: [Confident AI](https://www.confident-ai.com/)\n",
    "\n",
    "3. After signing-up an **API key** will be generated, which can be used to interact with the platform from inside the notebook.\n",
    "\n",
    "4. Do bear in mind that if you want to use **ConfidentAI** with **DeepEval** you need to always keep `deepeval` as the most recent version, otherwise you will get an exception.\n",
    "\n",
    "---\n",
    "\n",
    "Example of `.env` file:\n",
    "```bash\n",
    "DEEPEVAL_RESULTS_FOLDER=<folder> # Results of evaluations can be saved locally\n",
    "DEEPEVAL_API_KEY=<your api key>  # Relevant if you want to use Confident AI\n",
    "DEEPEVAL_TELEMETRY_OPT_OUT=\"YES\" # Remove telemetry\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ðŸŽ‰ðŸ¥³ Congratulations! You've successfully logged in! ðŸ™Œ \n",
       "</pre>\n"
      ],
      "text/plain": [
       "ðŸŽ‰ðŸ¥³ Congratulations! You've successfully logged in! ðŸ™Œ \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from deepeval import login_with_confident_api_key\n",
    "\n",
    "# Loads the environment variables from a `.env` file.\n",
    "# If you want to use Confident AI be sure to create one parent directory with the variables.\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "deepeval_api_key: str = os.getenv(\"DEEPEVAL_API_KEY\")\n",
    "\n",
    "# You should get a message letting you know you are logged-in.\n",
    "login_with_confident_api_key(deepeval_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMTestCase\n",
    "\n",
    "- Unlike **RAGAs**, where a single interaction between a user and the AI system is represented by a **SingleTurnSample** in **DeepEval** there's the concept of so called **LLMTestCase**.\n",
    "\n",
    "- Similarly to **RAGAs**, **LLMTestCase** objects have the same fields just different names - input, actual_output, expected_output, etc.\n",
    "One can very easily generate a synthetic dataset with one framework, convert it into the proper format valid for the other framework and use it.\n",
    "\n",
    "![Image showcasing what a LLMTestCase is.](../../img/LLM-testcase.png \"LLMTestCase\")\n",
    "\n",
    "- Put simply a **LLMTestCase** represents a **single, atomic unit of interaction with your LLM app**.\n",
    "    - An **interaction** can mean different things depending on the application being evaluated and the scope.\n",
    "        - If one evaluates an **AI Agent** an **interaction** can mean:\n",
    "            - **Agent Level**: The entire process initiated by the agent including all intermediary steps\n",
    "            - **RAG pipeline level**: Just the **RAG** workflow - **retriever** + **generator**\n",
    "            - Individual components level:\n",
    "                - **Retriever**: Retrieving relevant chunks and ranking them accordingly\n",
    "                - **Generator**: Generating relevant, complete answer free of hallucinations\n",
    "\n",
    "![Image showcasing what an interaction means](../../img/llm-interaction.png \"LLMTestCase interaction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "**Evaluation** should be a crucial component of every single application which uses **AI**. **DeepEval** provides more than 30 metrics for evaluation so that one can very easily iterate towards a better LLM application. Each default metric uses **LLM-As-A-Judge**. Optionally, one can use the **GEval** to set a custom criteria for evaluation if neither of the other metrics meet the requirements. Alternatively, there's the **DAGMetric**, whose purpose is similar to the **GEval**, however it uses a graph and it's fully **deterministic**.\n",
    "\n",
    "When evaluating a test case, multiple metrics can be used and the test would be **positive** iff all the **metrics thresholds** have been exceeded and **negative** in any other case. \n",
    "\n",
    "Evaluation workflow:\n",
    "\n",
    "![Image showcasing the evaluation steps](../../img/evaluation-workflow.png \"Evaluation workflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Dataset\n",
    "\n",
    "An **evaluation dataset** is just a collection of **LLMTestCase**- or so called **Golden** objects. A **Golden** is structurally the same as a **LLMTestCase**, however it has no `actual_output` and `retrieval_context` fields, which can be generated by your LLM at evaluation time.\n",
    "\n",
    "Datasets can be **pushed**, **stored** and **pulled** from **Confident AIs** platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM provider\n",
    "\n",
    "**DeepEval** uses **OpenAI** by default as a LLM, however **Ollama** is also available. To use it execute the code cell below. This will generate a `.deepeval` file where key-value pairs will be stored about that particular LLM-provider like model name, base url and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ™Œ Congratulations! You're now using a local Ollama model for all evals that \n",
      "require an LLM.\n",
      "ðŸ™Œ Congratulations! You're now using Ollama embeddings for all evals that \n",
      "require text embeddings.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Final\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../../env/rag.env\")\n",
    "\n",
    "CHAT_MODEL: Final[str] = os.getenv(\"CHAT_MODEL\")\n",
    "EMBEDDING_MODEL: Final[str] = os.getenv(\"EMBEDDING_MODEL\")\n",
    "\n",
    "! deepeval set-ollama {CHAT_MODEL} --base-url=\"http://localhost:11434/\"\n",
    "! deepeval set-ollama-embeddings {EMBEDDING_MODEL} --base-url=\"http://localhost:11434\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset\n",
    "\n",
    "* If you already have a dataset on the platform just use the `pull` method and specify the name/alias.\n",
    "* Alternatively, you can save your synthetic dataset on disk and import it from a `json` or `csv` file.\n",
    "* Finally, you can store a dataset on **ragas.io**, however before using it you need to convert it.\n",
    "* Note also that if your dataset consists of **goldens** you need to generate the `actual_output` and `retrieval_context` first.\n",
    "\n",
    "For this project I use **RAGAs** for synthetic testdata generation and the full datasets are stored under: `/ragas/datasets` and can be loaded from `jsonl` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b9809b282b4cc39b40c77840b1ee1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from typing import List, Dict\n",
    "from ragas import EvaluationDataset, SingleTurnSample\n",
    "\n",
    "# The are located under `./datasets`\n",
    "filepath: str = input(\"Please specify which dataset to evaluate (only the file name): \")\n",
    "\n",
    "goldens: List[Dict] = []\n",
    "try:\n",
    "    with open(file=f\"../ragas/datasets/{filepath}.jsonl\", mode=\"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if line.strip():  # Skip empty lines\n",
    "                goldens.append(json.loads(line))\n",
    "\n",
    "    samples: List[SingleTurnSample] = []\n",
    "    for golden in goldens:\n",
    "        single_turn_sample = SingleTurnSample(**golden)\n",
    "        samples.append(single_turn_sample)\n",
    "        \n",
    "    evaluation_dataset = EvaluationDataset(samples)\n",
    "except FileNotFoundError:\n",
    "    print(f\"File: `./datasets/{filepath}.jsonl` containing goldens not found!\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error parsing JSONL file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for **RAGAs**\n",
    "\n",
    "**DeepEval** supports the original **RAGAs** metrics, so the cell below just initializes the required structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "from ragas import RunConfig, DiskCacheBackend\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "run_config = RunConfig(\n",
    "    timeout=86400,    # 24 hours on waiting for a single operation\n",
    "    max_retries=20,   # Max retries before giving up\n",
    "    max_wait=600,     # Max wait between retries\n",
    "    max_workers=4,    # Concurrent requests\n",
    "    log_tenacity=True # Print retry attempts\n",
    ")\n",
    "\n",
    "# This stores data generation and evaluation results locally on disk\n",
    "# When using it for the first time, it will create a .cache folder\n",
    "# When using it again, it will read from that folder and finish almost instantly\n",
    "# For each dataset, a different cache folder is created\n",
    "cacher = DiskCacheBackend(cache_dir=f\".cache-{filepath}\")\n",
    "\n",
    "load_dotenv(\"../../env/rag.env\")\n",
    "\n",
    "ragas_llm = LangchainLLMWrapper(\n",
    "    langchain_llm=ChatOllama(\n",
    "        model=os.getenv(\"CHAT_MODEL\"),\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        temperature=float(os.getenv(\"TEMPERATURE\")),\n",
    "        num_ctx=int(os.getenv(\"LLM_CONTEXT_WINDOW_TOKENS\")),\n",
    "        format=\"json\"\n",
    "    ),\n",
    "    run_config=run_config,\n",
    "    cache=cacher\n",
    ")\n",
    "\n",
    "ragas_embeddings = LangchainEmbeddingsWrapper(\n",
    "    embeddings=OllamaEmbeddings(\n",
    "        model=os.getenv(\"EMBEDDING_MODEL\"),\n",
    "        base_url=\"http://localhost:11434\"\n",
    "    ),\n",
    "    run_config=run_config,\n",
    "    cache=cacher\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating a dataset using metrics\n",
    "\n",
    "Evaluation in **DeepEval** works as follow:\n",
    "\n",
    "* Pull or create a dataset containing test cases or goldens:\n",
    "    - In the case of **goldens** have the LLM generate the `actual_output` and `retrieval_context` fields.\n",
    "    - Usually the **goldens** are preferred since one can modify the `prompt template` during evaluation.\n",
    "\n",
    "* Think about the application and the different use cases:\n",
    "    - What is it doing?\n",
    "    - How could I assure it's doing what it's supposed to be doing?\n",
    "    - Check out the existing metrics or create your own one.\n",
    "    - Select metric/s.\n",
    "\n",
    "* Run the evaluation:\n",
    "    - In notebooks use the `evaluate` method.\n",
    "    - If you choose to create test cases in python file/s:\n",
    "        - The filename/s should start with `test_` and can be ran using `deepeval test run <filename>`.\n",
    "        - Optionally, pass in flags like `-c` to use the cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Triad\n",
    "\n",
    "The RAG triad is composed of three metrics:\n",
    "- **Answer Relevancy**\n",
    "    - This particular metric can assess the `prompt template`.\n",
    "    - If the score is insufficient it might be worth playing around with the template itself - try to be more detailed by providing more fine-granular instructions or provide few-shot-examples.\n",
    "- **Faithfulness**\n",
    "    - This metric evaluates the `LLM` and its ability to incorporate context into the generation process.\n",
    "    - Low scores would usually signify a bad `LLM model` that **hallucinates** or fails to use the context properly.\n",
    "- **Contextual Relevancy**\n",
    "    - This metric verifies if the `retrieved context` is relevant for providing a good/complete answer to the query.\n",
    "    - It measures the `top-k`, `chunk_size` and `embedding model`.\n",
    "        - The `embedding model` would be useful to capture relevant chunks of context.\n",
    "        - `top-k` and `chunk-size` will either help or hinder the context retrieval by including or missing important information.\n",
    "\n",
    "If a given RAG application scores high on all 3 metrics, one can be confident that the optimal `hyperparameters` are being used. `Hyperparamaters` are parameters, which can either positively or negatively influence the RAG pipeline. For example a good `embedding model` would retrieve relevant data and also be able to pick-up on nuances, whereas a bad one wouldn't. If you have a simple RAG application that doesn't use `tool calls` or doesn't employ an agent then those 3 metrics can be a good starting point.\n",
    "\n",
    "![RAG Triad](../../img/RAG-triad.png  \"RAG Triad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Answer Relevancy**\n",
    "\n",
    "It uses the **LLM as a judge** to determine how well the response answers the user input. Is the answer complete, on-topic and concise? This metric is especially relevant when it comes to testing the **RAG pipelines generator** by determining the degree of relevance of the generated output with respect to the query. The main **hyperparameter** being tested by this metric is the `prompt_template` that the **LLM** receives as instruction to create an output (can it instruct the LLM to output relevant and helpful answers based on the `retrieval_context`). Tweaking the template submitted to the LLM can provide higher scores.\n",
    "\n",
    "---\n",
    "\n",
    "### Approach:\n",
    "\n",
    "The LLM is used to decompose the `actual_output` into claims and then each of those claims get classified with respect to the `input` by the LLM:\n",
    "\n",
    "* `yes` if relevant\n",
    "\n",
    "* `no` if irrelevant\n",
    "\n",
    "* `idk` if non-determined or partially relevant.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula:\n",
    "\n",
    "* Number of relevant statements = Statements marked as `yes` or `idk`\n",
    "\n",
    "* **Answer Relevancy** = $\\frac{\\text{Number of relevant statements}}{\\text{Total number of statements}}$\n",
    "\n",
    "---\n",
    "\n",
    "### Framework Comparison:\n",
    "\n",
    "This metric is very similar if not the same as the **Response Relevancy** in **RAGAs** in terms of what it evaluates, however it does it in a diffrerent way. In **RAGAs**, hypothetical questions get generated based on the response, their embeddings are computed and using **Semantic similarity** the mean of those scores is computed. In **DeepEval** the response is decomposed into claims and for each the LLM determines its relevance to the query/input (no Embedding model is required). \n",
    "\n",
    "|           | Answer Relevancy | Response Relevancy |\n",
    "|-----------|------------------|-------------|\n",
    "| Framework | DeepEval | RAGAs |\n",
    "| Approach  | Statemement decomposition + LLM-as-a-judge  | Hypothetical questions (LLM) + Semantic similarity  |\n",
    "| Score     |  $\\in{[0-1]} $ | $\\in{[0-1]} $ |\n",
    "| Component | Generator      | Generator     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from prompts.custom_answer_relevancy_prompt import MyAnswerRelevancyTemplate\n",
    "\n",
    "answer_relevancy = AnswerRelevancyMetric(\n",
    "    include_reason=False,\n",
    "    evaluation_template=MyAnswerRelevancyTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Faithfulness**\n",
    "\n",
    "The **Faithfulness** metric measures how factually consistent a response is with the retrieval context. It verifies, whether or not the LLM can use the retrieved context properly and not hallucinate. Output that contains hallucinations (some sort of contradiction or made up information) are penalized. The score ranges between 0 to 1, with higher scores indicating better factual consistency. The metric tests the `generator` component in the RAG pipeline and tries to verify if the output contradicts factual information from the **retrieval_context**. A response is considered faithful if its claims can be supported by the retrieved context (ideally we would want a response whose claims/statements are all supported by a node/chunk in the context).\n",
    "\n",
    "---\n",
    "\n",
    "### Approach:\n",
    "\n",
    "1. **Identify Claims**:\n",
    "   - Break down the response into individual statements\n",
    "\n",
    "2. **Identify Truths**:\n",
    "   - Break down the retrieval context into individual statements\n",
    "   - One can configure the number of `truths` to be extracted using the `truths_extraction_limit` parameter\n",
    "\n",
    "3. **Use LLM**\n",
    "    - Using the LLM to determine if all claims in the response are truthful (do not contradict information in the retrieval context)\n",
    "\n",
    "---\n",
    "\n",
    "### Formula:\n",
    "\n",
    "* Number of truthful claims = Statements/claims from the response marked as `yes` or `idk` **(not contradicting a truth)**\n",
    "\n",
    "* **Faithfulness** = $\\frac{\\text{Number of truthful claims}}{\\text{Total number of claims}}$\n",
    "\n",
    "---\n",
    "\n",
    "### Framework Comparison:\n",
    "\n",
    "|              | Faithfulness | Faithfulness |\n",
    "|--------------|-------------------------|---------------------|\n",
    "| Framework    | DeepEval                | RAGAS               |\n",
    "| Approach     | Statement decomposition, NLI, LLM-as-a-judge | Statement decomposition, NLI, LLM-as-a-judge |\n",
    "| Components   | Generator             | Generator           |\n",
    "| Scoring      | $\\in{[0-1]}$          | $\\in{[0-1]}$        |\n",
    "| Customization| - | Can use HHEM-2.1-Open classifier |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import FaithfulnessMetric\n",
    "from prompts.custom_faithfulness_prompt import MyFaithfulnessTemplate \n",
    "\n",
    "faithfulness = FaithfulnessMetric(\n",
    "    include_reason=False,\n",
    "    evaluation_template=MyFaithfulnessTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Contextual Precision**\n",
    "\n",
    "The **Contextual Precision** metric measures the the **re-rankers capability to rank relevant** `nodes`/`pieces of context` higher than irrelevant ones. This ensures that important information is in the beginning of the context, which may lead to better generation. Usually a LLM focuses more on information which is found at the beginning of the context. Information which is ranked higher than it is supposed to be leads to penalties (lower score).\n",
    "\n",
    "---\n",
    "\n",
    "### Approach:\n",
    "\n",
    "The **LLM** determines for each **piece of context** whether or not it's relevant for answering the **input** with respect to the **expected output**.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula:\n",
    "\n",
    "* **Note that not every node which was retrieved would be relevant**\n",
    "* **k** is the rank (position of retrieved node context)\n",
    "* **n** is the total number of retrieved nodes\n",
    "* **$r_{k}$** $\\in \\{0, 1\\} $\n",
    "* **Contextual Precision** = $ \\frac{1}{\\text{Number of relevant nodes}} \\times \\sum_{k=1}^{n} (\\frac{\\text{Number of relevant Nodes up to Rank k}}{\\text{k}} \\times r_{k}) $\n",
    "\n",
    "---\n",
    "\n",
    "### Framework Comparison:\n",
    "\n",
    "Both metrics work the exact same way and use the same formula, so one would expect almost identical results.\n",
    "\n",
    "|              | Contextual Precision | Context Precision |\n",
    "|--------------|-------------------------|---------------------|\n",
    "| Framework    | DeepEval                | RAGAS               |\n",
    "| Approach     | LLM-as-a-judge | LLM-as-a-judge |\n",
    "| Components   | Retriever             | Retriever           |\n",
    "| Scoring      | $\\in{[0-1]}$          | $\\in{[0-1]}$        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import ContextualPrecisionMetric\n",
    "from prompts.custom_contextual_precision_prompt import MyContextualPrecisionTemplate\n",
    "\n",
    "contextual_precision = ContextualPrecisionMetric(\n",
    "    include_reason=False,\n",
    "    evaluation_template=MyContextualPrecisionTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Contextual Recall**\n",
    "\n",
    "The **Contextual Recall** metric measures the extent to which the `retrieval_context` aligns with the `expected_output`. This metric penalizes a **retriever** which misses important/relevant nodes during retrieval, since that would lead to an incomplete response. It uses the `expected output` as a reference for the `retrieval context`. The main `hyperparameter` which is evaluated by the metric is the `embedding model`. Since the `embedding model` is used by the retriever when performing semantic similarity search a good model can positively affect the application. The inverse is also true.\n",
    "\n",
    "---\n",
    "\n",
    "### Approach:\n",
    "\n",
    "The **LLM** determines for each statement in the `expected_output` whether or not it can be attributed to a node from the `retrieval context`.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula:\n",
    "\n",
    "**Attributable statement** is one, which contains information present in a node of the context. \n",
    "* **Contextual Recall** = $ \\frac{\\text{Number of attributable statements}}{\\text{Total number of statements}} $\n",
    "\n",
    "---\n",
    "\n",
    "### Framework Comparison:\n",
    "\n",
    "Both metrics work almost identically. The `expected output` is decomposed into statements and for each of them the LLM determines if it can be attributed to a node from the `retrieved context`. The only minor difference is that in **RAGAs** the input is also explicitly used in determining that.\n",
    "\n",
    "|              | Contextual Precision | Context Precision |\n",
    "|--------------|-------------------------|---------------------|\n",
    "| Framework    | DeepEval                | RAGAS               |\n",
    "| Approach     | LLM-as-a-judge | LLM-as-a-judge |\n",
    "| Components   | Retriever             | Retriever           |\n",
    "| Scoring      | $\\in{[0-1]}$          | $\\in{[0-1]}$        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import ContextualRecallMetric\n",
    "from prompts.custom_contextual_recall_prompt import MyContextualRecallTemplate\n",
    "\n",
    "contextual_recall = ContextualRecallMetric(\n",
    "    include_reason=False,\n",
    "    evaluation_template=MyContextualRecallTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Contextual Relevancy**\n",
    "\n",
    "The **Contextual Relevancy** metric measures the extent to which the `retrieval_context` is useful for generating a proper response to the **input**. The hyperparameters which are measured in this case are the `chunk size` and `top-k`. In the case that `chunk size` is large one might fetch various nodes that contain the required information, however also redundant information causing the LLM to hallucinate. The other parameter `top-k` plays an important role in limiting the number of nodes to consider as context for the generation phase.\n",
    "\n",
    "---\n",
    "\n",
    "### Approach:\n",
    "\n",
    "The **LLM** receives the **input** and **retrieval_context** and is prompted to decide for each statement found in the context, whether or not it can be of use for answering the input.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula:\n",
    "\n",
    "**Relevant statement** is a statement found in the context, which can be utilized to answer the user input. \n",
    "* **Contextual Relevancy** = $ \\frac{\\text{Number of relevant statements}}{\\text{Total number of statements}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import ContextualRelevancyMetric\n",
    "from prompts.custom_contextual_relevancy_prompt import MyContextualRelevancyTemplate\n",
    "\n",
    "contextual_relevancy = ContextualRelevancyMetric(\n",
    "    include_reason=False,\n",
    "    evaluation_template=MyContextualRelevancyTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Contextual Entities Recall**\n",
    "\n",
    "This metric is not available in **DeepEval**. For explanation of how it works, please go to the notebook which contains evaluations in the `ragas` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics.ragas import RAGASContextualEntitiesRecall\n",
    "\n",
    "ragas_contextual_entities_recall = RAGASContextualEntitiesRecall(\n",
    "    model=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we need to evaluate all our application using our dataset and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.evaluate.configs import (\n",
    "    AsyncConfig,\n",
    "    CacheConfig,\n",
    "    DisplayConfig,\n",
    "    TestRunResultDisplay\n",
    ")\n",
    "from deepeval.evaluate.evaluate import EvaluationResult\n",
    "\n",
    "async_conf = AsyncConfig(\n",
    "    run_async=True,\n",
    "    throttle_value=120,\n",
    "    max_concurrent=10\n",
    ")\n",
    "\n",
    "cache_conf = CacheConfig(\n",
    "    write_cache=True,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "display_conf = DisplayConfig(\n",
    "    show_indicator=True,\n",
    "    print_results=True,\n",
    "    verbose_mode=False,\n",
    "    # Displays only the failed tests\n",
    "    # The ones whose score didn't exceed the threshold\n",
    "    display_option=TestRunResultDisplay.FAILING\n",
    ")\n",
    "\n",
    "# There's also an `ErrorConfig` for further customization\n",
    "results: EvaluationResult = evaluate(\n",
    "    test_cases=evaluation_dataset.test_cases,\n",
    "    metrics=[\n",
    "        answer_relevancy,\n",
    "        ragas_answer_relevancy,\n",
    "        faithfulness,\n",
    "        ragas_faithfulness,\n",
    "        contextual_precision,\n",
    "        ragas_contextual_precision,\n",
    "        contextual_recall,\n",
    "        ragas_contextual_recall,\n",
    "        ragas_contextual_entities_recall,\n",
    "        contextual_relevancy,\n",
    "    ],\n",
    "    async_config=async_conf,\n",
    "    display_config=display_conf,\n",
    "    cache_config=cache_conf\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
