services:
  # The vector store, where vector embeddings will be stored.
  # It has support for semantic search.
  pgvector:
    image: 'pgvector/pgvector:pg16'
    container_name: pgvector-db
    env_file:
      - ./env/pgvector-db.env
      - ./env/r2r.env
    pull_policy: always
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 5s
    restart: on-failure
    command: >
      postgres
      -c max_connections=${R2R_POSTGRES_MAX_CONNECTIONS}
    volumes:
      - 'postgres_data:/var/lib/postgresql/data'
    networks:
      - r2r_network

  # https://github.com/huggingface/text-embeddings-inference
  #
  # This container will hold a Sequence Classification cross-encoders model.
  # After context retrieval the re-ranker will re-order the nodes position based
  # on their relevance relative to the user input.
  #
  # If you want to use a different model, change the `RE_RANKER_MODEL` in the `reranker.env` file.
  # 
  # If you want to use a GPU, make sure to uncomment the `runtime: nvidia` line.
  # Additionally, uncomment the NVIDIA_VISIBLE_DEVICES line in the `reranker.env` file.
  # Change the image to: 'ghcr.io/huggingface/text-embeddings-inference:1.7'
  # Finally, download the nvidia container toolkit (uncomment the section in the run.sh).
  reranker:
    image: 'ghcr.io/huggingface/text-embeddings-inference:cpu-1.7'
    container_name: hf-reranker
    env_file:
      - ./env/reranker.env
    # runtime: nvidia # Uncomment if you want to use a GPU
    pull_policy: always
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 20s
      timeout: 5s
      retries: 50
      start_period: 30s
    restart: on-failure
    # https://huggingface.co/docs/text-embeddings-inference/cli_arguments
    command: --model-id ${RE_RANKER_MODEL} --max-batch-tokens 16384 --auto-truncate
    volumes:
      - 'reranker_data:/data'
    networks:
      - r2r_network

  # `R2R` will use this service to ingest documents into the vector store.
  # The service will receive requests containing the document to be ingested.
  # Thereafter, using configurable parameters from project/backend/config.toml,
  # under the `ingestion` section, it will parse the document and chunk it.
  unstructured:
    image: ragtoriches/unst-prod:latest
    container_name: unstructured
    pull_policy: always
    ports:
      - "127.0.0.1:7275:7275"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7275/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 5s
    restart: on-failure
    networks:
      - r2r_network
  
  # The `RAG` application server.
  r2r:
    # Make sure the tag of the image matches the version of `r2r` in `project/requirements.txt`.
    # Otherwise, there might be unexpected errors, due to changes in the source code.
    image: sciphiai/r2r:3.5.11
    container_name: r2r
    pull_policy: always
    env_file:
      - ./env/r2r.env
    ports:
      - "127.0.0.1:7272:7272"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7272/v3/health"]
      interval: 10s
      timeout: 10s
      retries: 50
      start_period: 30s
    command: uvicorn core.main.app_entry:app --host ${R2R_HOST} --port ${R2R_PORT}
    restart: on-failure
    networks:
      - r2r_network
    volumes:
      # Custom configuration file will be mapped inside the container.
      - ./project/backend/config.toml:/app/custom_config_ollama.toml
    extra_hosts:
      # This makes sure the container can communicate with `ollama` running on localhost.
      # The server hosted in a docker container will be able to access the host machine's `ollama` service.
      - host.docker.internal:host-gateway
    depends_on:
      pgvector:
        condition: service_healthy
      reranker:
        condition: service_healthy
      unstructured:
        condition: service_healthy

  frontend:
    build:
      context: ./project
      dockerfile: Dockerfile
    image: frontend:latest # Give a tag after the image is built
    container_name: frontend
    env_file:
      - ./env/rag.env
      - ./env/r2r.env
    ports:
      - "127.0.0.1:8501:8501"
    restart: on-failure
    networks:
      - r2r_network
    extra_hosts:
      - host.docker.internal:host-gateway
    depends_on:
      r2r:
        condition: service_healthy
    
volumes:
  postgres_data:
  reranker_data:

networks:
  r2r_network:
