# Server side configuration for R2R as specified at: 
#   https://r2r-docs.sciphi.ai/documentation/configuration/introduction.

[database]
provider = "postgres" 

# LLM 
# Model: https://ollama.com/library/llama3.1
[completion]
provider = "litellm"
concurrent_request_limit = 1

    [completion.generation_config]
    model = "ollama/llama3.1"
    temperature = 0.1
    top_p = 1
    max_tokens_to_sample = 1_024
    stream = false
    add_generation_kwargs = {}

[ingestion]
provider = "unstructured_local"
strategy = "auto" # If images or complex PDFs are used -> High res is used, otherwise Fast.
chunking_strategy = "by_title" # Attempts to preserve section boundaries when determining chunks' contents.
max_characters = 1_024 # Absolute max characters in a single chunk.
combine_under_n_chars = 128 # Combines elements from a section into a chunk until a section reaches a length of this many chars
new_after_n_chars = 512 # Start a new chunk after this many characterrs.
overlap = 20 # Number of characters to overlap between chunks.
split_pdf_concurrency_level = 10

# This describes how text chunks are converted into vector embeddings.
# Model: https://ollama.com/library/mxbai-embed-large
[embedding]
provider = "ollama"
base_model = "mxbai-embed-large"
base_dimension = 1024
batch_size = 32
concurrent_request_limit = 32

[vector_store]
type = "pgvector"
dimension = 1024

[logging]
level = "DEBUG"