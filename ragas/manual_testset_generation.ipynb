{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"example_data/layout-parser-paper.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_ollama import Ollama\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import pandas as pd\n",
    "\n",
    "# Configure Ollama\n",
    "embedding_model = \"mxbai-embed-large\"\n",
    "generation_model = \"llama3.1\"\n",
    "\n",
    "ollama_embeddings = Ollama(model=embedding_model)\n",
    "ollama_llm = Ollama(model=generation_model)\n",
    "\n",
    "# Initialize vector store (ChromaDB)\n",
    "def initialize_vector_store(docs, persist_directory=\"vector_store\"):\n",
    "    vector_store = Chroma.from_documents(\n",
    "        docs,\n",
    "        ollama_embeddings,\n",
    "        persist_directory=persist_directory,\n",
    "    )\n",
    "    return vector_store\n",
    "\n",
    "# Load and Embed Data into Vector Store\n",
    "def prepare_vector_store(data):\n",
    "    docs = [Document(page_content=context) for context in data]\n",
    "    vector_store = initialize_vector_store(docs)\n",
    "    return vector_store\n",
    "\n",
    "# Generate Synthetic Data\n",
    "def generate_synthetic_data(vector_store, user_inputs, num_samples=1):\n",
    "    synthetic_data = []\n",
    "\n",
    "    for user_input in user_inputs:\n",
    "        # Retrieve reference context\n",
    "        retriever = vector_store.as_retriever(search_kwargs={\"k\": 1})\n",
    "        context = retriever.get_relevant_documents(user_input)[0].page_content\n",
    "\n",
    "        # Generate ground truth answer using the retrieved context\n",
    "        answer_prompt = f\"Context:\\n{context}\\n\\nQuestion:\\n{user_input}\\n\\nProvide a concise and accurate answer based on the context:\"\n",
    "        answer = ollama_llm.generate(answer_prompt)\n",
    "\n",
    "        # Store the synthetic dataset\n",
    "        synthetic_data.append({\n",
    "            \"user_input\": user_input,\n",
    "            \"reference_context\": context,\n",
    "            \"ground_truth\": answer,\n",
    "        })\n",
    "\n",
    "    return synthetic_data\n",
    "\n",
    "# Example Contexts (Replace with your dataset)\n",
    "example_contexts = [\n",
    "    \"The Eiffel Tower is located in Paris, France. It was completed in 1889 and is one of the most recognizable structures in the world.\",\n",
    "    \"The Great Wall of China stretches over 13,000 miles and was built to protect against invasions. It is a UNESCO World Heritage site.\"\n",
    "]\n",
    "\n",
    "# Embed contexts into the vector store\n",
    "vector_store = prepare_vector_store(example_contexts)\n",
    "\n",
    "# Generate User Inputs (Highly Automatic)\n",
    "user_input_prompts = [\n",
    "    \"Generate a question about the Eiffel Tower.\",\n",
    "    \"Generate a question about the Great Wall of China.\"\n",
    "]\n",
    "\n",
    "user_inputs = [ollama_llm.generate(prompt) for prompt in user_input_prompts]\n",
    "\n",
    "# Generate Synthetic Dataset\n",
    "synthetic_dataset = generate_synthetic_data(vector_store, user_inputs, num_samples=5)\n",
    "\n",
    "# Save Dataset to CSV\n",
    "df = pd.DataFrame(synthetic_dataset)\n",
    "df.to_csv(\"synthetic_rag_evaluation_data.csv\", index=False)\n",
    "print(\"Synthetic data saved to synthetic_rag_evaluation_data.csv.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
