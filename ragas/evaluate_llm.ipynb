{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"explodinggradients/amnesty_qa\",\n",
    "    \"english_v3\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset\n",
    "\n",
    "eval_dataset = EvaluationDataset.from_hf_dataset(dataset[\"eval\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_ollama import OllamaLLM, OllamaEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "CHAT_MODEL = os.getenv(\"OLLAMA_CHAT_MODEL\")\n",
    "OLLAMA_API_BASE = os.getenv(\"OLLAMA_API_BASE\")\n",
    "EMBEDDING_MODEL = os.getenv(\"OLLAMA_EMBEDDING_MODEL\")\n",
    "\n",
    "ollama_llm = OllamaLLM(\n",
    "    model=CHAT_MODEL, \n",
    "    base_url=OLLAMA_API_BASE\n",
    ")\n",
    "ollama_llm_wrapper = LangchainLLMWrapper(ollama_llm)\n",
    "\n",
    "ollama_embedding = OllamaEmbeddings(\n",
    "    model=EMBEDDING_MODEL, \n",
    "    base_url=OLLAMA_API_BASE\n",
    ")\n",
    "ollama_embedding_wrapper = LangchainEmbeddingsWrapper(ollama_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import LLMContextPrecisionWithReference\n",
    "\n",
    "\"\"\"\n",
    "    Context Precision measures the proportion of relevant context/chunks that have been retrieved.\n",
    "    Importance here is on retrieving all relevant chunks.\n",
    "    Presence of irrelevant data/chunks reduces the score.\n",
    "\"\"\"\n",
    "\n",
    "# Using this metric since we already have the ground truth and retrieved contexts in the eval dataset.\n",
    "# This metric uses LLM to compare the retrieved context chunks with the ground truth.\n",
    "context_precision_metric_with_reference = LLMContextPrecisionWithReference(\n",
    "    llm=ollama_llm_wrapper\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import LLMContextRecall\n",
    "\n",
    "\"\"\"\n",
    "    Context Recall is a metric which primarily tests whether or not all relevant context/chunks have been retrieved.\n",
    "    Missing chunks lead to a penalty and the score is reduced.\n",
    "    Importance here is on not missing anything relevant. \n",
    "    Presence of redundant/irrelevant data doesn't reduce the score.\n",
    "\"\"\"\n",
    "\n",
    "# Using this metric instead of NonLLMContextRecall since we cannot access the retrieved context from the R2R framework.\n",
    "context_recall_metric = LLMContextRecall(\n",
    "    llm=ollama_llm_wrapper    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import ContextEntityRecall\n",
    "\n",
    "\"\"\"\n",
    "    This metric checks if all the entities in the ground truth / reference are present in the retrieved context.\n",
    "    If so the score is 1 indicating a perfect match.\n",
    "    This metric can be useful in cases where entities matter, for example, a tourism help chatbot.\n",
    "\"\"\"\n",
    "\n",
    "context_entity_recall_metric = ContextEntityRecall(\n",
    "    llm=ollama_llm_wrapper\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import NoiseSensitivity\n",
    "\n",
    "\"\"\"\n",
    "    This metric checks how often the LLM provides invalid/incorrect responses \n",
    "        based on good or bad (noisy data / redundant data) retrieved context.\n",
    "    The lower the score, the more robust is the RAG system to noisy data.\n",
    "    This metric makes use of reference/ground truth, retrieved context, user_input and the actual LLM response.\n",
    "    The response is split into claims. Each claim is verified against the ground truth and also the retrieved context.\n",
    "    If either the ground truth or the retrieved context doesn't support the claim then the claim is marked as wrong.\n",
    "\"\"\"\n",
    "\n",
    "# focus='irrelevant' - Evaluates the effect of irrelevant contexts on the generated response. \n",
    "# Checks if irrelevant contexts introduce incorrect claims into the response.\n",
    "noise_sensitivity_metric = NoiseSensitivity(\n",
    "    llm=ollama_llm_wrapper,\n",
    "    focus=\"irrelevant\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import ResponseRelevancy\n",
    "\n",
    "\"\"\"\n",
    "    This metric measures how relevant/comprehensive the answer is to the user input/query.\n",
    "    Incomplete, irrelevant or redundant answers reduce the score.\n",
    "    Factuality is not considered in this metric.\n",
    "    To compute the metric one needs the user_input, retrieved_context and the LLM response.\n",
    "    One derives n hypothetical/artificial questions based on the answer. \n",
    "    Then using cosine-similarity we compare the similarity of the answer with the hypothetical questions.\n",
    "    Finally, we take the average/mean of the similarity scores.\n",
    "\"\"\"\n",
    "\n",
    "# Strictness refers to the number of questions generated per answer.\n",
    "response_relevancy_metric = ResponseRelevancy(\n",
    "    llm=ollama_llm_wrapper,\n",
    "    embeddings=ollama_embedding_wrapper,\n",
    "    strictness=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import Faithfulness\n",
    "\n",
    "\"\"\"\n",
    "    This metric measures how much of the retrieved context is actually relevant/helpful in answering the user query.\n",
    "    We take the number of claims in the generated answer and see how many of them can be inferred from the retrieved context.\n",
    "    Finally, we take the number of claims that can be inferred and divide it by the total number of claims in the answer.\n",
    "\"\"\"\n",
    "\n",
    "faithfulness_metric = Faithfulness(\n",
    "    llm=ollama_llm_wrapper\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import FactualCorrectness\n",
    "\n",
    "\"\"\"\n",
    "    This metric is like a fact-checker. It compares the ground truth/reference to the generated answer.\n",
    "    Both the answer and ground truth are split into claims and then using natural language inference we determine the factual overlap.\n",
    "    \n",
    "    TP: True positive - number of claims in the response that are present in the reference.\n",
    "    FP: False positive - number of claims in the response not part of the reference.\n",
    "    FN: False negative - number of claims in the reference not present in the response.\n",
    "    \n",
    "    Precision = TP / (TP + FP)\n",
    "    Recall = TP / (TP + FN)\n",
    "    F1 = 2 * Precision * Recall / (Precision + Recall)\n",
    "    \n",
    "    The mode can be used to compute F1, precision or recall.\n",
    "    Atomicity controls the granularity when splitting both the answer and the reference into claims. (low, high)\n",
    "    The lower the value the more a sentence is broken apart into its smallest, meaningful components.\n",
    "    Coverage can be used to either generalize the content or focus on the most important parts of the content. (low, high)\n",
    "\"\"\"\n",
    "\n",
    "factual_correctness_metric = FactualCorrectness(\n",
    "    llm=ollama_llm_wrapper,\n",
    "    mode=\"f1\",\n",
    "    atomicity=\"low\",\n",
    "    coverage=\"low\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/20 [01:58<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 18\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Added this since otherwise timeout exceptions are thrown. The model is to weak.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m config \u001b[38;5;241m=\u001b[39m RunConfig(\n\u001b[0;32m     15\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m600\u001b[39m \n\u001b[0;32m     16\u001b[0m )\n\u001b[1;32m---> 18\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mollama_llm_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mollama_embedding_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\n\u001b[0;32m     25\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\windows_venv\\Lib\\site-packages\\ragas\\_analytics.py:205\u001b[0m, in \u001b[0;36mtrack_was_completed.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m    204\u001b[0m     track(IsCompleteEvent(event_type\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[1;32m--> 205\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m     track(IsCompleteEvent(event_type\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\windows_venv\\Lib\\site-packages\\ragas\\evaluation.py:302\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size)\u001b[0m\n\u001b[0;32m    299\u001b[0m scores: t\u001b[38;5;241m.\u001b[39mList[t\u001b[38;5;241m.\u001b[39mDict[\u001b[38;5;28mstr\u001b[39m, t\u001b[38;5;241m.\u001b[39mAny]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;66;03m# get the results\u001b[39;00m\n\u001b[1;32m--> 302\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;241m==\u001b[39m []:\n\u001b[0;32m    304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\windows_venv\\Lib\\site-packages\\ragas\\executor.py:203\u001b[0m, in \u001b[0;36mExecutor.results\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    200\u001b[0m             nest_asyncio\u001b[38;5;241m.\u001b[39mapply()\n\u001b[0;32m    201\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nest_asyncio_applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 203\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_jobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m sorted_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(results, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [r[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m sorted_results]\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\windows_venv\\Lib\\site-packages\\nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\windows_venv\\Lib\\site-packages\\nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\windows_venv\\Lib\\site-packages\\nest_asyncio.py:115\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    108\u001b[0m     heappop(scheduled)\n\u001b[0;32m    110\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[0;32m    113\u001b[0m         scheduled[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_when \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 115\u001b[0m event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_events(event_list)\n\u001b[0;32m    118\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_resolution\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\selectors.py:323\u001b[0m, in \u001b[0;36mSelectSelector.select\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    321\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 323\u001b[0m     r, w, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_readers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_writers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\selectors.py:314\u001b[0m, in \u001b[0;36mSelectSelector._select\u001b[1;34m(self, r, w, _, timeout)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_select\u001b[39m(\u001b[38;5;28mself\u001b[39m, r, w, _, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 314\u001b[0m     r, w, x \u001b[38;5;241m=\u001b[39m \u001b[43mselect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r, w \u001b[38;5;241m+\u001b[39m x, []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[4]: RemoteProtocolError(Server disconnected without sending a response.)\n",
      "Exception raised in Job[5]: ConnectError(All connection attempts failed)\n",
      "Exception raised in Job[0]: ConnectError(All connection attempts failed)\n",
      "Exception raised in Job[13]: ConnectError(All connection attempts failed)\n",
      "Exception raised in Job[12]: ConnectError(All connection attempts failed)\n",
      "Exception raised in Job[15]: ConnectError(All connection attempts failed)\n",
      "Exception raised in Job[14]: ConnectError(All connection attempts failed)\n",
      "Exception raised in Job[8]: ConnectError(All connection attempts failed)\n",
      "Exception raised in Job[1]: ConnectError(All connection attempts failed)\n",
      "Exception raised in Job[9]: ConnectError(All connection attempts failed)\n",
      "Exception raised in Job[2]: ConnectError(All connection attempts failed)\n",
      "Exception raised in Job[3]: ConnectError(All connection attempts failed)\n",
      "Exception raised in Job[10]: ReadError()\n",
      "Exception raised in Job[11]: ReadError()\n",
      "Exception raised in Job[7]: ReadError()\n",
      "Exception raised in Job[6]: ReadError()\n"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate, RunConfig\n",
    "\n",
    "metrics = [\n",
    "   # context_precision_metric_with_reference,\n",
    "   # context_recall_metric,\n",
    "   # context_entity_recall_metric,\n",
    "   # noise_sensitivity_metric,\n",
    "   # response_relevancy_metric,\n",
    "    faithfulness_metric,\n",
    "   # factual_correctness_metric\n",
    "]\n",
    "\n",
    "# Added this since otherwise timeout exceptions are thrown. The model is to weak.\n",
    "config = RunConfig(\n",
    "    timeout=600 \n",
    ")\n",
    "\n",
    "results = evaluate(\n",
    "    dataset=eval_dataset, \n",
    "    metrics=metrics,\n",
    "    llm=ollama_llm_wrapper,\n",
    "    embeddings=ollama_embedding_wrapper,\n",
    "    run_config=config,\n",
    "    batch_size=16\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "windows_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
