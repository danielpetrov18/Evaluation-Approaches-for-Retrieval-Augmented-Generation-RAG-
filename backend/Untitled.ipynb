{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21947df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from utility.ascrapper import AsyncScraper\n",
    "\n",
    "urls = [\n",
    "    \"https://www.scrapingbee.com/blog/how-to-make-pythons-beautiful-soup-faster-performance/\",\n",
    "    \"https://www.scrapingbee.com/blog/async-scraping-in-python/\",\n",
    "    \"https://docs.python.org/3/library/asyncio.html#module-asyncio\",\n",
    "    \"https://realpython.com/python-xml-parser/#lxml-use-elementtree-on-steroids\",\n",
    "    \"https://github.com/psf/requests-html\"\n",
    "]\n",
    "\n",
    "ascrapper = AsyncScraper(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37fe3bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 5/5 [00:00<00:00,  6.85it/s]\n"
     ]
    }
   ],
   "source": [
    "documents = ascrapper.fetch_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8c24d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\_\\_ \\* Login \\* Sign Up \\* Pricing \\* FAQ \\* Blog \\* Other Features \\* AI Web Scraping \\* Screenshots \\* Google search API \\* Data extraction \\* JavaScript scenario \\* No code web scraping \\* Developers \\* Tutorials \\* Documentation \\* Knowledge Base \\# How to use asyncio to scrape websites with Python Try ScrapingBee for Free \\*\\*Alexander M | 01 July 2024 | 10 min read\\*\\* Table of contents In this article, we'll take a look at how you can use Python and its coroutines, with their \\`async\\`/\\`await\\` syntax, to efficiently scrape websites, without having to go all-in on threads 🧵 and semaphores 🚦. For this purpose, we'll check out asyncio , along with the asynchronous HTTP library aiohttp . \\#\\# What is asyncio? asyncio is part of Python's standard library \\(yay, no additional dependency to manage 🥳\\) which enables the implementation of concurrency using the same asynchronous patterns you may already know from JavaScript and other languages: \\`async\\` and \\`await\\` Asynchronous programming is a convenient alternative to Python threads , as it allows you to run tasks in parallel without the need to fully dive into multi- threading, with all the complexities this might involve. When using the asynchronous approach, you program your code in a seemingly good-old synchronous/blocking fashion and just sprinkle mentioned keywords at the relevant spots in your code and the Python runtime will automatically take care of your code being executed concurrently. \\#\\#\\# Asynchronous Python basics asyncio uses the following three key concepts to provide asynchronicity: \\* Coroutines \\* Tasks \\* Futures \\*\\*Coroutines\\*\\* are the basic building blocks and allow you to declare asynchronous functions, which are executed concurrently by asyncio's event loop and provide a \\_Future\\_ as response \\(more on that in a second\\). A coroutine is declared by prefixing the function declaration with \\`async\\` \\(i.e. \\`async def my\\_function\\(\\):\\`\\) and it typically uses \\`await\\` itself, to invoke other asynchronous functions. \\*\\*Tasks\\*\\* are the components used for scheduling and the actual concurrent execution of coroutines in an asyncio context. They are instantiated with \\`asyncio.create\\_task\\(\\)\\` and automatically handled by the event loop. \\*\\*Futures\\*\\* are the return value of a coroutine and represent the \\_future\\_ value computed by the coroutine. You can find a full list of technical details at https://docs.python.org/3/library/asyncio-task.html\\#awaitables . \\#\\#\\# How does \\`async\\`/\\`await\\` work? If you happen to be already familiar with \\`async\\`/\\`await\\` in JavaScript, you'll feel right at home as the underlying concept is the same. While asynchronous programming per se is not something new, this was usually achieved with callbacks and the eventual pyramid of doom with all the nested and chained callbacks. This was pretty unmanageable in both, JavaScript and Python. \\`async\\`/\\`await\\` came to the rescue here. When you have a task which takes longer to compute \\(typical example when to use multi-threading\\), you can mark the function with \\`async\\` and turn it into a coroutine. Let's take the following code as quick example. import asyncio async def wait\\_and\\_print\\(str\\): await asyncio.sleep\\(1\\) print\\(str\\) async def main\\(\\): tasks = \\[\\] for i in range\\(1, 10\\): tasks.append\\(asyncio.create\\_task\\(wait\\_and\\_print\\(i\\)\\)\\) for task in tasks: await task asyncio.run\\(main\\(\\)\\) Here, we define a function which prints a value and we call the function ten times. Pretty simple, but the catch is the function waits a second before it prints its text. If we called this in a regular, sequential fashion, the whole execution would take ten times one second. However, with coroutines, we use \\`create\\_task\\` to set up a list of tasks, which we subsequently execute all at the same time using the \\`await\\` statement. Each function still pauses for one second, but given they all run at the same time, the whole script will have completed after a second. Yet, the code does not utilise any \\(obvious\\) multi- threading and looks mostly like traditional single-threaded code. One thing to note is, that we need to encapsulate our top-level code in its own \\`async\\` function \\`main\\(\\)\\`, which we then call with \\`run\\(\\)\\`. This starts the event loop and provides us with all the asynchronous goodies. Let's take a quick look at what asyncio provides out-of-the-box\\! \\#\\#\\# Asyncio feature/function overview The following table provides a quick overview of the core features and functions of asyncio, which you'll mostly come across when programming asynchronously with asyncio. Feature| Description \\---|--- as\\_completed\\(\\)| Executes a list of coroutines and returns an iterator object for their results create\\_task\\(\\)| Executes the given coroutine concurrently in the context of a task ensure\\_future\\(\\)| Accepts a list of different parameters types and verifies they are all Future-like objects gather\\(\\)| Concurrently executes the passed awaitables using tasks and returns their combined results get\\_event\\_loop\\(\\)| Provides access to the currently active event loop instance run\\(\\)| Executes the given coroutine - typically used for the main function sleep\\(\\)| Suspends the current task for the indicated number of seconds - akin to the standard \\`time.sleep\\(\\)\\` function wait\\(\\)| Execute a list of awaitables and waits until the condition specified in the \\`when\\` parameter is met wait\\_for\\(\\)| Similar to \\`wait\\`, but cancels the future when a timeout occurs Lock\\(\\)| Provides access to a mutex object Semaphore\\(\\)| Provides access to a semaphore object Task\\(\\)| The task object, as returned by \\`create\\_task\\(\\)\\` \\#\\# Scrape Wikipedia asynchronously with Python and asyncio Now, that we have a basic understanding of how asynchronous calls work in Python and the features asyncio provides, let's put our knowledge to use with a real-world scraping example, shall we? The idea of the following example is to compile a list of creators of programming languages. For this purpose, we first crawl Wikipedia for the articles it has on programming languages. With that list, we then scrape these pages in the second step and extract the respective information from the pages' infoboxes. Voilà, that should then get us a list of programming languages with their respective creators. Let's get coding\\! 👨🏻‍💻 \\#\\#\\# Installing dependencies For starters, let's install the necessary library dependencies using pip: pip install aiohttp pip install BeautifulSoup Splendid\\! We have the basic libraries installed and can continue with getting the links to scrape. \\#\\#\\# Crawling Create a new file \\`scraper.py\\` and save the following code: import asyncio import aiohttp as aiohttp from bs4 import BeautifulSoup from urllib.parse import urljoin import pprint BASE\\_URL = 'https://en.wikipedia.org' async def fetch\\(url\\): async with aiohttp.ClientSession\\(\\) as session: async with session.get\\(url\\) as resp: return await resp.text\\(\\) async def crawl\\(\\): pages = \\[\\] content = await fetch\\(urljoin\\(BASE\\_URL, '/wiki/List\\_of\\_programming\\_languages'\\)\\) soup = BeautifulSoup\\(content, 'html.parser'\\) for link in soup.select\\('div.div-col a'\\): pages.append\\(urljoin\\(BASE\\_URL, link\\['href'\\]\\)\\) return pages async def main\\(\\): links = await crawl\\(\\) pp = pprint.PrettyPrinter\\(\\) pp.pprint\\(links\\) asyncio.run\\(main\\(\\)\\) Lovely\\! That code is ready to run, but before we do that, let's take a look at the individual parts we are performing here: 1\\. The usual imports 2\\. Then, we declare \\`BASE\\_URL\\` for our base URL 3\\. Next, we define a basic \\`fetch\\(\\)\\` function handling the asynchronous aiohttp calls and returning the URL's content 4\\. We also define our central \\`crawl\\(\\)\\` function, which does the crawling and uses the CSS selector \\`div.div-col a\\` to get a list of all relevant language pages 5\\. Lastly, we call our asynchronous \\`main\\(\\)\\` function, using \\`asyncio.run\\(\\)\\`, to run \\`crawl\\(\\)\\` and print the list of the links we found > 💡 \\*\\*Using CSS selectors with Python\\*\\* > > If you want to learn more about how to use CSS selectors specifically in > Python, please check out How to use CSS Selectors in Python? . Great so far, but just theory. Let's run it and check if we actually get the links we are after .... $ python3 scraper.py \\['https://en.wikipedia.org/wiki/A\\_Sharp\\_\\(.NET\\)', 'https://en.wikipedia.org/wiki/A-0\\_System', 'https://en.wikipedia.org/wiki/A%2B\\_\\(programming\\_language\\)', 'https://en.wikipedia.org/wiki/ABAP', 'https://en.wikipedia.org/wiki/ABC\\_\\(programming\\_language\\)', 'https://en.wikipedia.org/wiki/ABC\\_ALGOL', 'https://en.wikipedia.org/wiki/ACC\\_\\(programming\\_language\\)', 'https://en.wikipedia.org/wiki/Accent\\_\\(programming\\_language\\)', LOTS MORE Well, we did, and it was easier than one might think - not lots of boilerplate code \\(\\`public static void main\\(String\\[\\] args\\)\\`, anyone? ☕\\). But crawling was only the first step on our journey. It's certainly crucial, because without a list of URLs we won't be able to scrape them, but what we are really interested in is the information of each individual language. So let's continue to the scraping bit of our project\\! \\#\\#\\# Scraping All right, so far we managed to get the list of URLs we want to scrape and now we are going to implement the code which will perform the actual scraping. For this, let's start with the core function that implement the scraping logic and add the following asynchronous function to our \\`scraper.py\\` file. async def scrape\\(link\\): content = await fetch\\(link\\) soup = BeautifulSoup\\(content, 'html.parser'\\) \\# Select name name = soup.select\\_one\\('caption.infobox-title'\\) if name is not None: name = name.text creator = soup.select\\_one\\('table.infobox tr:has\\(th a:-soup-contains\\(\"Developer\", \"Designed by\"\\)\\) td'\\) if creator is not None: creator = creator.text return \\[name, creator\\] return \\[\\] Once again, quite some manageable piece of code, isn't it? What do we do here in detail, though? 1\\. First, \\`scrape\\(\\)\\` takes one argument: \\`link\\`, the URL to scrape 2\\. Next, we call our \\`fetch\\(\\)\\` function to get the content of that URL and save it into \\`content\\` 3\\. Now, we instantiate an instance of Beautiful Soup and use it to parse \\`content\\` into \\`soup\\` 4\\. We quickly use the CSS selector \\`caption.infobox-title\\` to get language name 5\\. As last step, we use the \\`:-soup-contains\\` pseudo-selector to precisely select the table entry with the name of the language author and return everything as array - or an empty array if we did not find the information Now that we have this in place, we just need to pass the links obtained with our crawler to \\`scrape\\(\\)\\` and our scraper is almost ready\\! Let's quickly adjust the \\`main\\(\\)\\` function as follows: async def main\\(\\): links = await crawl\\(\\) tasks = \\[\\] for link in links: tasks.append\\(scrape\\(link\\)\\) authors = await asyncio.gather\\(\\*tasks\\) pp = pprint.PrettyPrinter\\(\\) pp.pprint\\(authors\\) We still call \\`crawl\\(\\)\\`, but instead of printing the links, we schedule a \\`scrape\\(\\)\\` call for each link as an asyncio task and add it to the \\`tasks\\` queue. The real magic then happens when we pass the queue to \\`asyncio.gather\\` , which runs all the tasks asynchronously and in parallel. And here is the full code: import asyncio import aiohttp as aiohttp from bs4 import BeautifulSoup from urllib.parse import urljoin import pprint BASE\\_URL = 'https://en.wikipedia.org' async def fetch\\(url\\): async with aiohttp.ClientSession\\(\\) as session: async with session.get\\(url\\) as resp: return await resp.text\\(\\) async def crawl\\(\\): pages = \\[\\] content = await fetch\\(urljoin\\(BASE\\_URL, '/wiki/List\\_of\\_programming\\_languages'\\)\\) soup = BeautifulSoup\\(content, 'html.parser'\\) for link in soup.select\\('div.div-col a'\\): pages.append\\(urljoin\\(BASE\\_URL, link\\['href'\\]\\)\\) return pages async def scrape\\(link\\): content = await fetch\\(link\\) soup = BeautifulSoup\\(content, 'html.parser'\\) \\# Select name name = soup.select\\_one\\('caption.infobox-title'\\) if name is not None: name = name.text creator = soup.select\\_one\\('table.infobox tr:has\\(th a:-soup-contains\\(\"Developer\", \"Designed by\"\\)\\) td'\\) if creator is not None: creator = creator.text return \\[name, creator\\] return \\[\\] async def main\\(\\): links = await crawl\\(\\) tasks = \\[\\] for link in links: tasks.append\\(scrape\\(link\\)\\) authors = await asyncio.gather\\(\\*tasks\\) pp = pprint.PrettyPrinter\\(\\) pp.pprint\\(authors\\) asyncio.run\\(main\\(\\)\\) > 💡 Love web scraping in Python? Check out our expert list of the Best Python > web scraping libraries. \\#\\# Summary What we learned in this article is that Python provides an excellent environment for running concurrent tasks without the need to implement full multi-threading. Its asynchronous \\`async\\`/\\`await\\` syntax enables you to implement your scraping logic in a straightforward, blocking fashion and, nonetheless, run an efficient scraping pipeline and fully utilise the available CPU cores. Our examples provide a good initial overview on how to approach async programming in Python, but there are still a few factors to take into account to make sure your scraper is successful \\* the user-agent - you can simply pass a \\`headers\\` dictionary as argument to \\`session.get\\` and indicate the desired user-agent \\* request throttling - make sure you are not overwhelming the server and send your requests with reasonable delays \\* IP addresses - some sites may be limited to certain geographical regions or import restrictions on concurrent or total requests from one, single IP address \\* JavaScript - some sites \\(especially SPAs\\) make heavy use of JavaScript and you need a proper JavaScript engine to support your scraping If you wish to find out more on these issues, please drop by our other article on this very subject: Web Scraping without getting blocked If you don't feel like having to deal with all the scraping bureaucracy of IP address rotation, user-agents, geo-fencing, browser management for JavaScript and rather want to focus on the data extraction and analysis, then please feel free to take a look at our specialised web scraping API . The platform handles all these issues on its own and comes with proxy support, a full-fledged JavaScript environment, and straightforward scraping rules using CSS selectors and XPath expressions . Registering an account is absolutely free and comes with the first 1,000 scraping requests on the house - plenty of room to discover how ScrapingBee can help you with your projects. Happy asynchronous scraping with Python\\! \\*\\*Alexander M\\*\\* Alexander is a software engineer and technical writer with a passion for everything network related. \\#\\#\\# You might also like: \\#\\#\\#\\# Web Scraping Tutorial Using Selenium & Python \\(+ examples\\) \\*\\*Ilya Krukowski\\*\\* 31 min read Lean how to scrape the web with Selenium and Python with this step by step tutorial. We will use Selenium to automate Hacker News login. \\#\\#\\#\\# How to Use a Proxy with Python Requests? \\*\\*Maxine Meurer\\*\\* 9 min read In this tutorial we will see how to use a proxy with the Requests package. We will also discuss on how to choose the right proxy provider. \\#\\#\\#\\# Study of Amazon’s Best Selling & Most Read Book Charts Since 2017 \\*\\*Karthik Devan\\*\\* 31 min read Discover the most popular books on Amazon\\! \\(since 2017\\) Our in-depth analysis reveals the top-selling and most-read titles, scraped directly from Amazon data. \\#\\# Tired of getting blocked while scraping the web? ScrapingBee API handles headless browsers and rotates proxies for you. Get access to 1,000 free API credits, no credit card required\\! Try ScrapingBee for Free ScrapingBee API handles headless browsers and rotates proxies for you. \\* \\_\\_ \\* \\_\\_ \\#\\#\\#\\# Company \\* Team \\* Company's journey \\* Blog \\* Rebranding \\* Affiliate Program \\#\\#\\#\\# Tools \\* Curl converter \\#\\#\\#\\# Legal \\* Terms of Service \\* Privacy Policy \\* GDPR Compliance \\* Data Processing Agreement \\* Cookie Policy \\* Acceptable Use Policy \\* Legal Notices \\#\\#\\#\\# Product \\* Features \\* Pricing \\* Status \\#\\#\\#\\# How we compare \\* Alternative to Crawlera \\* Alternative to Luminati \\* Alternative to NetNut \\* Alternative to ScraperAPI \\* Alternatives to ScrapingBee \\#\\#\\#\\# No code web scraping \\* No code web scraping \\* No code competitor monitoring \\* How to put scraped website data into Google Sheets \\* Send stock prices update to Slack \\* Scrape Amazon products' price with no code \\* Scrape Amazon products' price with no code \\* Extract job listings, details and salaries \\#\\#\\#\\# Learning Web Scraping \\* Web scraping questions \\* A guide to Web Scraping without getting blocked \\* Web Scraping Tools \\* Best Free Proxies \\* Best Mobile proxies \\* Web Scraping vs Web Crawling \\* Rotating and residential proxies \\* Web Scraping with Python \\* Web Scraping with PHP \\* Web Scraping with Java \\* Web Scraping with Ruby \\* Web Scraping with NodeJS \\* Web Scraping with R \\* Web Scraping with C\\# \\* Web Scraping with C++ \\* Web Scraping with Elixir \\* Web Scraping with Perl \\* Web Scraping with Rust \\* Web Scraping with Go \\_\\_ Copyright © 2025 Made in France \n",
      "\n",
      "Skip to content \\#\\# Navigation Menu Toggle navigation Sign in \\* Product \\* GitHub Copilot Write better code with AI \\* Security Find and fix vulnerabilities \\* Actions Automate any workflow \\* Codespaces Instant dev environments \\* Issues Plan and track work \\* Code Review Manage code changes \\* Discussions Collaborate outside of code \\* Code Search Find more, search less Explore \\* All features \\* Documentation \\* GitHub Skills \\* Blog \\* Solutions By company size \\* Enterprises \\* Small and medium teams \\* Startups \\* Nonprofits By use case \\* DevSecOps \\* DevOps \\* CI/CD \\* View all use cases By industry \\* Healthcare \\* Financial services \\* Manufacturing \\* Government \\* View all industries View all solutions \\* Resources Topics \\* AI \\* DevOps \\* Security \\* Software Development \\* View all Explore \\* Learning Pathways \\* Events & Webinars \\* Ebooks & Whitepapers \\* Customer Stories \\* Partners \\* Executive Insights \\* Open Source \\* GitHub Sponsors Fund open source developers \\* The ReadME Project GitHub community articles Repositories \\* Topics \\* Trending \\* Collections \\* Enterprise \\* Enterprise platform AI-powered developer platform Available add-ons \\* Advanced Security Enterprise-grade security features \\* Copilot for business Enterprise-grade AI features \\* Premium Support Enterprise-grade 24/7 support \\* Pricing Search or jump to... \\# Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips \\# Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback \\# Saved searches \\#\\# Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation. Cancel Create saved search Sign in Sign up Reseting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert \\{\\{ message \\}\\} psf / \\*\\*requests-html \\*\\* Public \\* Notifications You must be signed in to change notification settings \\* Fork 986 \\* Star 13.8k Pythonic HTML Parsing for Humans™ html.python-requests.org \\#\\#\\# License MIT license 13.8k stars 986 forks Branches Tags Activity Star Notifications You must be signed in to change notification settings \\* Code \\* Issues 195 \\* Pull requests 39 \\* Actions \\* Projects 0 \\* Security \\* Insights Additional navigation options \\* Code \\* Issues \\* Pull requests \\* Actions \\* Projects \\* Security \\* Insights \\# psf/requests-html master BranchesTags Go to file Code \\#\\# Folders and files Name| Name| Last commit message| Last commit date \\---|---|---|--- \\#\\# Latest commit \\#\\# History 462 Commits docs| docs| | ext| ext| | tests| tests| | .gitattributes| .gitattributes| | .gitignore| .gitignore| | .travis.yml| .travis.yml| | LICENSE| LICENSE| | Makefile| Makefile| | Pipfile| Pipfile| | Pipfile.lock| Pipfile.lock| | README.rst| README.rst| | pytest.ini| pytest.ini| | requests\\_html.py| requests\\_html.py| | setup.py| setup.py| | View all files \\#\\# Repository files navigation \\* README \\* Code of conduct \\* MIT license \\#\\# Requests-HTML: HTML Parsing for Humans™ This library intends to make parsing HTML \\(e.g. scraping the web\\) as simple and intuitive as possible. When using this library you automatically get: \\* \\*\\*Full JavaScript support\\*\\*\\! \\(Using Chromium, thanks to pyppeteer\\) \\* \\_CSS Selectors\\_ \\(a.k.a jQuery-style, thanks to PyQuery\\). \\* \\_XPath Selectors\\_ , for the faint of heart. \\* Mocked user-agent \\(like a real web browser\\). \\* Automatic following of redirects. \\* Connection–pooling and cookie persistence. \\* The Requests experience you know and love, with magical parsing abilities. \\* \\*\\*Async Support\\*\\* \\#\\# Tutorial & Usage Make a GET request to 'python.org', using Requests: >>> from requests\\_html import HTMLSession >>> session = HTMLSession\\(\\) >>> r = session.get\\('https://python.org/'\\) Try async and get some sites at the same time: >>> from requests\\_html import AsyncHTMLSession >>> asession = AsyncHTMLSession\\(\\) >>> async def get\\_pythonorg\\(\\): ... r = await asession.get\\('https://python.org/'\\) ... return r ... >>> async def get\\_reddit\\(\\): ... r = await asession.get\\('https://reddit.com/'\\) ... return r ... >>> async def get\\_google\\(\\): ... r = await asession.get\\('https://google.com/'\\) ... return r ... >>> results = asession.run\\(get\\_pythonorg, get\\_reddit, get\\_google\\) >>> results \\# check the requests all returned a 200 \\(success\\) code \\[, , \\] >>> \\# Each item in the results list is a response object and can be interacted with as such >>> for result in results: ... print\\(result.html.url\\) ... https://www.python.org/ https://www.google.com/ https://www.reddit.com/ Note that the order of the objects in the results list represents the order they were returned in, not the order that the coroutines are passed to the \\`run\\` method, which is shown in the example by the order being different. Grab a list of all links on the page, as–is \\(anchors excluded\\): >>> r.html.links \\{'//docs.python.org/3/tutorial/', '/about/apps/', 'https://github.com/python/pythondotorg/issues', '/accounts/login/', '/dev/peps/', '/about/legal/', '//docs.python.org/3/tutorial/introduction.html\\#lists', '/download/alternatives', 'http://feedproxy.google.com/~r/PythonInsider/~3/kihd2DW98YY/python-370a4-is-available-for-testing.html', '/download/other/', '/downloads/windows/', 'https://mail.python.org/mailman/listinfo/python-dev', '/doc/av', 'https://devguide.python.org/', '/about/success/\\#engineering', 'https://wiki.python.org/moin/PythonEventsCalendar\\#Submitting\\_an\\_Event', 'https://www.openstack.org', '/about/gettingstarted/', 'http://feedproxy.google.com/~r/PythonInsider/~3/AMoBel8b8Mc/python-3.html', '/success-stories/industrial-light-magic-runs-python/', 'http://docs.python.org/3/tutorial/introduction.html\\#using-python-as-a-calculator', '/', 'http://pyfound.blogspot.com/', '/events/python-events/past/', '/downloads/release/python-2714/', 'https://wiki.python.org/moin/PythonBooks', 'http://plus.google.com/+Python', 'https://wiki.python.org/moin/', 'https://status.python.org/', '/community/workshops/', '/community/lists/', 'http://buildbot.net/', '/community/awards', 'http://twitter.com/ThePSF', 'https://docs.python.org/3/license.html', '/psf/donations/', 'http://wiki.python.org/moin/Languages', '/dev/', '/events/python-user-group/', 'https://wiki.qt.io/PySide', '/community/sigs/', 'https://wiki.gnome.org/Projects/PyGObject', 'http://www.ansible.com', 'http://www.saltstack.com', 'http://planetpython.org/', '/events/python-events', '/about/help/', '/events/python-user-group/past/', '/about/success/', '/psf-landing/', '/about/apps', '/about/', 'http://www.wxpython.org/', '/events/python-user-group/665/', 'https://www.python.org/psf/codeofconduct/', '/dev/peps/peps.rss', '/downloads/source/', '/psf/sponsorship/sponsors/', 'http://bottlepy.org', 'http://roundup.sourceforge.net/', 'http://pandas.pydata.org/', 'http://brochure.getpython.info/', 'https://bugs.python.org/', '/community/merchandise/', 'http://tornadoweb.org', '/events/python-user-group/650/', 'http://flask.pocoo.org/', '/downloads/release/python-364/', '/events/python-user-group/660/', '/events/python-user-group/638/', '/psf/', '/doc/', 'http://blog.python.org', '/events/python-events/604/', '/about/success/\\#government', 'http://python.org/dev/peps/', 'https://docs.python.org', 'http://feedproxy.google.com/~r/PythonInsider/~3/zVC80sq9s00/python-364-is-now-available.html', '/users/membership/', '/about/success/\\#arts', 'https://wiki.python.org/moin/Python2orPython3', '/downloads/', '/jobs/', 'http://trac.edgewall.org/', 'http://feedproxy.google.com/~r/PythonInsider/~3/wh73\\_1A-N7Q/python-355rc1-and-python-348rc1-are-now.html', '/privacy/', 'https://pypi.python.org/', 'http://www.riverbankcomputing.co.uk/software/pyqt/intro', 'http://www.scipy.org', '/community/forums/', '/about/success/\\#scientific', '/about/success/\\#software-development', '/shell/', '/accounts/signup/', 'http://www.facebook.com/pythonlang?fref=ts', '/community/', 'https://kivy.org/', '/about/quotes/', 'http://www.web2py.com/', '/community/logos/', '/community/diversity/', '/events/calendars/', 'https://wiki.python.org/moin/BeginnersGuide', '/success-stories/', '/doc/essays/', '/dev/core-mentorship/', 'http://ipython.org', '/events/', '//docs.python.org/3/tutorial/controlflow.html', '/about/success/\\#education', '/blogs/', '/community/irc/', 'http://pycon.blogspot.com/', '//jobs.python.org', 'http://www.pylonsproject.org/', 'http://www.djangoproject.com/', '/downloads/mac-osx/', '/about/success/\\#business', 'http://feedproxy.google.com/~r/PythonInsider/~3/x\\_c9D0S-4C4/python-370b1-is-now-available-for.html', 'http://wiki.python.org/moin/TkInter', 'https://docs.python.org/faq/', '//docs.python.org/3/tutorial/controlflow.html\\#defining-functions'\\} Grab a list of all links on the page, in absolute form \\(anchors excluded\\): >>> r.html.absolute\\_links \\{'https://github.com/python/pythondotorg/issues', 'https://docs.python.org/3/tutorial/', 'https://www.python.org/about/success/', 'http://feedproxy.google.com/~r/PythonInsider/~3/kihd2DW98YY/python-370a4-is-available-for-testing.html', 'https://www.python.org/dev/peps/', 'https://mail.python.org/mailman/listinfo/python-dev', 'https://www.python.org/doc/', 'https://www.python.org/', 'https://www.python.org/about/', 'https://www.python.org/events/python-events/past/', 'https://devguide.python.org/', 'https://wiki.python.org/moin/PythonEventsCalendar\\#Submitting\\_an\\_Event', 'https://www.openstack.org', 'http://feedproxy.google.com/~r/PythonInsider/~3/AMoBel8b8Mc/python-3.html', 'https://docs.python.org/3/tutorial/introduction.html\\#lists', 'http://docs.python.org/3/tutorial/introduction.html\\#using-python-as-a-calculator', 'http://pyfound.blogspot.com/', 'https://wiki.python.org/moin/PythonBooks', 'http://plus.google.com/+Python', 'https://wiki.python.org/moin/', 'https://www.python.org/events/python-events', 'https://status.python.org/', 'https://www.python.org/about/apps', 'https://www.python.org/downloads/release/python-2714/', 'https://www.python.org/psf/donations/', 'http://buildbot.net/', 'http://twitter.com/ThePSF', 'https://docs.python.org/3/license.html', 'http://wiki.python.org/moin/Languages', 'https://docs.python.org/faq/', 'https://jobs.python.org', 'https://www.python.org/about/success/\\#software-development', 'https://www.python.org/about/success/\\#education', 'https://www.python.org/community/logos/', 'https://www.python.org/doc/av', 'https://wiki.qt.io/PySide', 'https://www.python.org/events/python-user-group/660/', 'https://wiki.gnome.org/Projects/PyGObject', 'http://www.ansible.com', 'http://www.saltstack.com', 'https://www.python.org/dev/peps/peps.rss', 'http://planetpython.org/', 'https://www.python.org/events/python-user-group/past/', 'https://docs.python.org/3/tutorial/controlflow.html\\#defining-functions', 'https://www.python.org/community/diversity/', 'https://docs.python.org/3/tutorial/controlflow.html', 'https://www.python.org/community/awards', 'https://www.python.org/events/python-user-group/638/', 'https://www.python.org/about/legal/', 'https://www.python.org/dev/', 'https://www.python.org/download/alternatives', 'https://www.python.org/downloads/', 'https://www.python.org/community/lists/', 'http://www.wxpython.org/', 'https://www.python.org/about/success/\\#government', 'https://www.python.org/psf/', 'https://www.python.org/psf/codeofconduct/', 'http://bottlepy.org', 'http://roundup.sourceforge.net/', 'http://pandas.pydata.org/', 'http://brochure.getpython.info/', 'https://www.python.org/downloads/source/', 'https://bugs.python.org/', 'https://www.python.org/downloads/mac-osx/', 'https://www.python.org/about/help/', 'http://tornadoweb.org', 'http://flask.pocoo.org/', 'https://www.python.org/users/membership/', 'http://blog.python.org', 'https://www.python.org/privacy/', 'https://www.python.org/about/gettingstarted/', 'http://python.org/dev/peps/', 'https://www.python.org/about/apps/', 'https://docs.python.org', 'https://www.python.org/success-stories/', 'https://www.python.org/community/forums/', 'http://feedproxy.google.com/~r/PythonInsider/~3/zVC80sq9s00/python-364-is-now-available.html', 'https://www.python.org/community/merchandise/', 'https://www.python.org/about/success/\\#arts', 'https://wiki.python.org/moin/Python2orPython3', 'http://trac.edgewall.org/', 'http://feedproxy.google.com/~r/PythonInsider/~3/wh73\\_1A-N7Q/python-355rc1-and-python-348rc1-are-now.html', 'https://pypi.python.org/', 'https://www.python.org/events/python-user-group/650/', 'http://www.riverbankcomputing.co.uk/software/pyqt/intro', 'https://www.python.org/about/quotes/', 'https://www.python.org/downloads/windows/', 'https://www.python.org/events/calendars/', 'http://www.scipy.org', 'https://www.python.org/community/workshops/', 'https://www.python.org/blogs/', 'https://www.python.org/accounts/signup/', 'https://www.python.org/events/', 'https://kivy.org/', 'http://www.facebook.com/pythonlang?fref=ts', 'http://www.web2py.com/', 'https://www.python.org/psf/sponsorship/sponsors/', 'https://www.python.org/community/', 'https://www.python.org/download/other/', 'https://www.python.org/psf-landing/', 'https://www.python.org/events/python-user-group/665/', 'https://wiki.python.org/moin/BeginnersGuide', 'https://www.python.org/accounts/login/', 'https://www.python.org/downloads/release/python-364/', 'https://www.python.org/dev/core-mentorship/', 'https://www.python.org/about/success/\\#business', 'https://www.python.org/community/sigs/', 'https://www.python.org/events/python-user-group/', 'http://ipython.org', 'https://www.python.org/shell/', 'https://www.python.org/community/irc/', 'https://www.python.org/about/success/\\#engineering', 'http://www.pylonsproject.org/', 'http://pycon.blogspot.com/', 'https://www.python.org/about/success/\\#scientific', 'https://www.python.org/doc/essays/', 'http://www.djangoproject.com/', 'https://www.python.org/success-stories/industrial-light-magic-runs-python/', 'http://feedproxy.google.com/~r/PythonInsider/~3/x\\_c9D0S-4C4/python-370b1-is-now-available-for.html', 'http://wiki.python.org/moin/TkInter', 'https://www.python.org/jobs/', 'https://www.python.org/events/python-events/604/'\\} Select an element with a CSS Selector: >>> about = r.html.find\\('\\#about', first=True\\) Grab an element's text contents: >>> print\\(about.text\\) About Applications Quotes Getting Started Help Python Brochure Introspect an Element's attributes: >>> about.attrs \\{'id': 'about', 'class': \\('tier-1', 'element-1'\\), 'aria-haspopup': 'true'\\} Render out an Element's HTML: >>> about.html '\n",
      "* \\nAbout\\n\n",
      "\\n\n",
      "  * Applications\n",
      "\\n\n",
      "  * Quotes\n",
      "\\n\n",
      "  * Getting Started\n",
      "\\n\n",
      "  * Help\n",
      "\\n\n",
      "  * Python Brochure\n",
      "\\n\n",
      "\\n\n",
      "' Select Elements within Elements: >>> about.find\\('a'\\) \\[, , , , , \\] Search for links within an element: >>> about.absolute\\_links \\{'http://brochure.getpython.info/', 'https://www.python.org/about/gettingstarted/', 'https://www.python.org/about/', 'https://www.python.org/about/quotes/', 'https://www.python.org/about/help/', 'https://www.python.org/about/apps/'\\} Search for text on the page: >>> r.html.search\\('Python is a \\{\\} language'\\)\\[0\\] programming More complex CSS Selector example \\(copied from Chrome dev tools\\): >>> r = session.get\\('https://github.com/'\\) >>> sel = 'body > div.application-main > div.jumbotron.jumbotron-codelines > div > div > div.col-md-7.text-center.text-md-left > p' >>> print\\(r.html.find\\(sel, first=True\\).text\\) GitHub is a development platform inspired by the way you work. From open source to business, you can host and review code, manage projects, and build software alongside millions of other developers. XPath is also supported: >>> r.html.xpath\\('/html/body/div\\[1\\]/a'\\) \\[\\] \\#\\# JavaScript Support Let's grab some text that's rendered by JavaScript. Until 2020, the Python 2.7 countdown clock \\(https://pythonclock.org\\) will serve as a good test page: >>> r = session.get\\('https://pythonclock.org'\\) Let's try and see the dynamically rendered code \\(The countdown clock\\). To do that quickly at first, we'll search between the last text we see before it \\('Python 2.7 will retire in...'\\) and the first text we see after it \\('Enable Guido Mode'\\). >>> r.html.search\\('Python 2.7 will retire in...\\{\\}Enable Guido Mode'\\)\\[0\\] '\n",
      "\\n \n",
      "\\n \n",
      "\\n \n",
      "\\n \n",
      "\\n ' Notice the clock is missing. The \\`render\\(\\)\\` method takes the response and renders the dynamic content just like a web browser would. >>> r.html.render\\(\\) >>> r.html.search\\('Python 2.7 will retire in...\\{\\}Enable Guido Mode'\\)\\[0\\] '\n",
      "\\n \n",
      "\\n \n",
      "1Year2Months28Days16Hours52Minutes46Seconds\n",
      "\\n \n",
      "\\n \n",
      "\\n ' Let's clean it up a bit. This step is not needed, it just makes it a bit easier to visualize the returned html to see what we need to target to extract our required information. >>> from pprint import pprint >>> pprint\\(r.html.search\\('Python 2.7 will retire in...\\{\\}Enable'\\)\\[0\\]\\) \\('\n",
      "\\n' ' \n",
      "\\n' ' \n",
      "1Year2Months28Days16Hours52Minutes46Seconds\n",
      "\\n' ' \n",
      "\\n' ' \n",
      "\\n' ' '\\) The rendered html has all the same methods and attributes as above. Let's extract just the data that we want out of the clock into something easy to use elsewhere and introspect like a dictionary. >>> periods = \\[element.text for element in r.html.find\\('.countdown-period'\\)\\] >>> amounts = \\[element.text for element in r.html.find\\('.countdown-amount'\\)\\] >>> countdown\\_data = dict\\(zip\\(periods, amounts\\)\\) >>> countdown\\_data \\{'Year': '1', 'Months': '2', 'Days': '5', 'Hours': '23', 'Minutes': '34', 'Seconds': '37'\\} Or you can do this async also: >>> async def get\\_pyclock\\(\\): ... r = await asession.get\\('https://pythonclock.org/'\\) ... await r.html.arender\\(\\) ... return r ... >>> results = asession.run\\(get\\_pyclock, get\\_pyclock, get\\_pyclock\\) The rest of the code operates the same way as the synchronous version except that \\`results\\` is a list containing multiple response objects however the same basic processes can be applied as above to extract the data you want. Note, the first time you ever run the \\`render\\(\\)\\` method, it will download Chromium into your home directory \\(e.g. \\`~/.pyppeteer/\\`\\). This only happens once. \\#\\# Using without Requests You can also use this library without Requests: >>> from requests\\_html import HTML >>> doc = \"\"\"\"\"\" >>> html = HTML\\(html=doc\\) >>> html.links \\{'https://httpbin.org'\\} \\#\\# Installation $ pipenv install requests-html ✨🍰✨ Only \\*\\*Python 3.6 and above\\*\\* is supported. \\#\\# About Pythonic HTML Parsing for Humans™ html.python-requests.org \\#\\#\\# Topics python html http scraping requests kennethreitz beautifulsoup lxml css-selectors pyquery \\#\\#\\# Resources Readme \\#\\#\\# License MIT license \\#\\#\\# Code of conduct Code of conduct Activity Custom properties \\#\\#\\# Stars \\*\\*13.8k\\*\\* stars \\#\\#\\# Watchers \\*\\*268\\*\\* watching \\#\\#\\# Forks \\*\\*986\\*\\* forks Report repository \\#\\# Releases 1 v0.10.0 Latest Feb 18, 2019 \\#\\# Packages 0 No packages published \\#\\# Contributors 60 \\* \\* \\* \\* \\* \\* \\* \\* \\* \\* \\* \\* \\* \\* \\\\+ 46 contributors \\#\\# Languages \\* Python 99.7% \\* Makefile 0.3% \\#\\# Footer \\(C\\) 2025 GitHub, Inc. \\#\\#\\# Footer navigation \\* Terms \\* Privacy \\* Security \\* Status \\* Docs \\* Contact \\* Manage cookies \\* Do not share my personal information You can’t perform that action at this time. \n",
      "\n",
      "\\_\\_ \\* Login \\* Sign Up \\* Pricing \\* FAQ \\* Blog \\* Other Features \\* AI Web Scraping \\* Screenshots \\* Google search API \\* Data extraction \\* JavaScript scenario \\* No code web scraping \\* Developers \\* Tutorials \\* Documentation \\* Knowledge Base \\# 10 Tips on How to make Python's Beautiful Soup faster when scraping Try ScrapingBee for Free \\*\\*Satyam Tripathi | 19 September 2024 | 19 min read\\*\\* Table of contents Beautiful Soup is super easy to use for parsing HTML and is hugely popular. However, if you're extracting a gigantic amount of data from tons of scraped pages it can slow to a crawl if not properly optimized. In this tutorial, I'll show you 10 expert-level tips and tricks for transforming Beautiful Soup into a blazing-fast data-extracting beast and how to optimize your scraping process to be as fast as lightning. \\#\\# Factors affecting the speed of Beautiful Soup Beautiful Soup's performance can vary based on several factors. Here are some key factors that influence the speed of web scraping using Beautiful Soup . 1\\. \\*\\*Parser Choice:\\*\\* The parser you choose \\(such as \\`lxml\\`, \\`html.parser\\`, or \\`html5lib\\`\\) significantly impacts Beautiful Soup's speed and performance. 2\\. \\*\\*Document Complexity:\\*\\* Parsing large or complex HTML/XML documents can be resource-intensive, which ultimately results in slower execution times. 3\\. \\*\\*CSS Selectors:\\*\\* Using complex or inefficient CSS selectors can slow down the parsing process. 4\\. \\*\\*Connection Overhead:\\*\\* Repeatedly establishing new connections for each request can significantly reduce the speed. So, using sessions or persistent connections can help a lot. 5\\. \\*\\*Concurrency:\\*\\* Using multi-threading or asynchronous processing allows you for concurrent fetching and parsing, which improve the overall speed. Many other factors affect Beautiful Soup's performance, which we'll explore further in the following sections, along with strategies to make beautiful soup faster. \\#\\# How to profile the different performance factors Profiling is an important step in identifying performance bottlenecks in any software application. Python provides several tools for profiling, with \\`cProfile\\` being one of the most commonly used for identifying slow parts of the code. By analyzing the output of a profiler, developers can pinpoint areas that require optimization and implement alternative approaches to make beautiful soup faster. \\`cProfile\\` is a built-in Python module that allows you to profile the execution time of various parts of a program. Here's a quick example of how to profile a code: import cProfile from bs4 import BeautifulSoup import requests def fetch\\_data\\(url\\): response = requests.get\\(url\\) soup = BeautifulSoup\\(response.content, \"html.parser\"\\) return soup.title.text \\# Profiling the function to analyze performance cProfile.run\\('fetch\\_data\\(\"https://news.ycombinator.com\"\\)'\\) The result is: So, we’re using \\`cProfile.run\\(\\)\\` for a quick overview of the performance. If you want more detailed and sorted profiling results, you can add the following code to sort and format the output: import cProfile import pstats from bs4 import BeautifulSoup import requests def fetch\\_data\\(url\\): response = requests.get\\(url\\) soup = BeautifulSoup\\(response.content, \"html.parser\"\\) return soup.title.text \\# Profiling the function to analyze performance cProfile.run\\('fetch\\_data\\(\"https://news.ycombinator.com\"\\)', \"my\\_profile\"\\) p = pstats.Stats\\(\"my\\_profile\"\\) p.sort\\_stats\\(\"cumulative\"\\).print\\_stats\\(\\) \\# Sort by cumulative time The result is: Here, we’re \\`pstats.Stats\\` with sorting options like \\`cumulative\\` to get more detailed and sorted output. By using tools like cProfile to identify bottlenecks and employing strategies such as choosing the right parser, optimizing CSS selectors, leveraging concurrency, and implementing caching, developers can significantly improve the efficiency of their web scraping projects. \\#\\# Tip 1: Effective use of parsers in beautiful soup One of the main factors that impact the performance of Beautiful Soup is the choice of parser. A common mistake many developers make is selecting any parser without considering its effect on speed and efficiency. Beautiful Soup supports multiple parsers, including \\`html.parser\\`, \\`lxml\\`, and \\`html5lib\\`, each with its unique features and performance characteristics. Choosing the right parser can significantly make beautiful soup faster. But how do you select the best parser for your specific needs? Let's look at the several options to help you make the correct decision. \\#\\#\\# html5lib \\`html5lib\\` is a parser that's great for handling messy HTML because it parses documents the same way a web browser would. It's especially useful when dealing with poorly structured HTML or pages that use newer HTML5 elements. However, this flexibility comes at the cost of slower performance compared to other parsers. Here's the code: import cProfile import requests from bs4 import BeautifulSoup \\# Function to parse HTML content using html5lib parser def parse\\_html\\(url\\): response = requests.get\\(url\\) soup = BeautifulSoup\\(response.content, \"html5lib\"\\) \\# Profile the parse\\_html function using cProfile cProfile.run\\('parse\\_html\\(\"https://news.ycombinator.com/\"\\)'\\) The result is: The results for the \\`html5lib\\` parser show that it made 318,133 function calls in \\*\\*1.030\\*\\* seconds. \\#\\#\\# html.parser \\`html.parser\\` is the default parser that comes with Python's standard library, and it offers better performance than \\`html5lib\\`. It's a good choice for smaller projects or when dealing with HTML that is relatively well-structured. Here's the code: import cProfile import requests from bs4 import BeautifulSoup \\# Function to parse HTML content using html.parser parser def parse\\_html\\(url\\): response = requests.get\\(url\\) soup = BeautifulSoup\\(response.content, \"html.parser\"\\) \\# Profile the parse\\_html function using cProfile cProfile.run\\('parse\\_html\\(\"https://news.ycombinator.com/\"\\)'\\) The result is: The results for \\`html.parser\\` shows that it made \\*\\*170272\\*\\* function calls in 22.626 seconds. This is significantly faster than \\`html5lib\\`. \\#\\#\\# lxml The \\`lxml\\` parser is the fastest option among the parsers supported by Beautiful Soup. It's built on the C libraries \\`libxml2\\` and \\`libxslt\\`, which offer great performance and support for advanced features like XPath and XSLT. Here's the code: import cProfile import requests from bs4 import BeautifulSoup \\# Function to parse HTML content using lxml parser def parse\\_html\\(url\\): response = requests.get\\(url\\) soup = BeautifulSoup\\(response.content, \"lxml\"\\) \\# Profile the parse\\_html function using cProfile cProfile.run\\('parse\\_html\\(\"https://news.ycombinator.com/\"\\)'\\) The result is: The \\`lxml\\` parser shows great performance, making \\*\\*141250\\*\\* function calls in just 22.625 seconds. This was the fastest result among the parsers tested. \\#\\#\\# Performance analysis We measured the time each parser took to process the same HTML document. \\* \\*\\*html5lib\\*\\* : This parser is the slowest, as it takes the most time to process the document. This is due to its comprehensive approach to handling malformed HTML. \\* \\*\\*html.parser\\*\\* : It is faster than \\`html5lib\\`, but it still falls short of \\`lxml\\` in terms of speed. It provides a good balance between speed and robustness for moderately well-formed HTML. \\* \\*\\*lxml\\*\\* : The lxml parser is the fastest and most efficient for parsing well-formed HTML and XML documents. \\#\\# Tip 2: Reducing network load with sessions Another common mistake developers make is establishing a new connection for each request, which can significantly slow down the process. To avoid this, you can use sessions to maintain persistent connections, reducing connection overhead. The Python \\`requests\\` library provides a \\`Session\\` object that can be reused for multiple requests to the same server. For example, using a session can be as simple as: import requests from bs4 import BeautifulSoup import cProfile def fetch\\_data\\(\\): \\# Create a session session = requests.Session\\(\\) \\# Use the session to make requests response = session.get\\(\"https://news.ycombinator.com/\"\\) soup = BeautifulSoup\\(response.content, \"lxml\"\\) \\# Close the session session.close\\(\\) \\# Profile the fetch\\_data function cProfile.run\\(\"fetch\\_data\\(\\)\"\\) The result is: Maintaining a session helps you avoid the time-consuming process of setting up a new connection for each request. This is especially useful when scraping multiple pages from the same domain, as it allows faster and more efficient data extraction. \\#\\# Tip 3: Using multi-threading and asynchronous processing Multi-threading and asynchronous processing are techniques that can significantly improve the performance of web scraping tasks by enabling concurrent execution of tasks. \\#\\#\\# Using multi-threading The \\`ThreadPoolExecutor\\` from the \\`concurrent.futures\\` module is a popular tool for implementing multi-threading in Python. It enables you to create a pool of threads and manage their execution efficiently. Here’s how to use \\`ThreadPoolExecutor\\` with Beautiful Soup: from concurrent.futures import ThreadPoolExecutor, as\\_completed import requests from bs4 import BeautifulSoup import cProfile def fetch\\_url\\(url\\): \"\"\" Fetches the content of a URL and parses it using BeautifulSoup. \"\"\" response = requests.get\\(url\\) print\\(f\"Processed URL: \\{url\\}\"\\) return BeautifulSoup\\(response.content, \"lxml\"\\) \\# List of URLs to be processed. urls = \\[ \"https://news.ycombinator.com/\", \"https://news.ycombinator.com/?p=2\", \"https://news.ycombinator.com/?p=3\", \\] def main\\(\\): \"\"\" Main function to fetch and process multiple URLs concurrently. Uses ThreadPoolExecutor to manage concurrent requests. \"\"\" with ThreadPoolExecutor\\(max\\_workers=5\\) as executor: \\# Dictionary to track futures and corresponding URLs. futures = \\{executor.submit\\(fetch\\_url, url\\): url for url in urls\\} for future in as\\_completed\\(futures\\): url = futures\\[future\\] try: \\# Attempt to get the result of the future. soup = future.result\\(\\) except Exception as e: print\\(f\"Error processing \\{url\\}: \\{e\\}\"\\) if \\_\\_name\\_\\_ == \"\\_\\_main\\_\\_\": \\# Profile the main function to analyze performance. cProfile.run\\(\"main\\(\\)\"\\) This approach results in 1,357,674 function calls completed in 23.274 seconds. The code creates a pool of five threads, with each thread responsible for fetching and parsing a URL. The \\`as\\_completed\\` function is then used to iterate over the completed futures, allowing you to process results as soon as it is ready. \\#\\#\\# Using asynchronous processing For asynchronous processing, the combination of the \\`asyncio\\` library with \\`aiohttp\\` provides a powerful solution for managing web requests. \\`aiohttp\\` is an asynchronous HTTP client that integrates smoothly with \\`asyncio\\`, allowing you to perform web scraping without blocking other operations. > 💡You can also check out a detailed guide on how to use asyncio for web > scraping with Python . Here’s an example of how you can use these libraries to implement asynchronous web scraping: import asyncio import aiohttp from bs4 import BeautifulSoup import cProfile urls = \\[ \"https://news.ycombinator.com/\", \"https://news.ycombinator.com/?p=2\", \"https://news.ycombinator.com/?p=3\", \\] async def fetch\\_url\\(session, url\\): \"\"\" Asynchronously fetches the content of a URL using aiohttp and parses it using BeautifulSoup. \"\"\" async with session.get\\(url\\) as response: content = await response.text\\(\\) print\\(f\"Processed URL: \\{url\\}\"\\) return BeautifulSoup\\(content, \"lxml\"\\) async def main\\(\\): \"\"\" Main function to create an aiohttp session and fetch all URLs concurrently. \"\"\" async with aiohttp.ClientSession\\(\\) as session: tasks = \\[fetch\\_url\\(session, url\\) for url in urls\\] await asyncio.gather\\(\\*tasks\\) if \\_\\_name\\_\\_ == \"\\_\\_main\\_\\_\": cProfile.run\\(\"asyncio.run\\(main\\(\\)\\)\"\\) This asynchronous approach results in 126,515 function calls completed in 22.383 seconds. In this example, \\`aiohttp.ClientSession\\` is used to manage HTTP connections, and \\`asyncio.gather\\` is used to run multiple asynchronous tasks concurrently. This approach allows you to handle a large number of requests efficiently, as the event loop manages the execution of tasks without blocking. \\#\\#\\# Combining multi-threading and asynchronous processing Multi-threading and asynchronous processing are effective individually as we can see, however, combining them can lead to even better performance gains. For example, you can use asynchronous processing to handle network requests and multi-threading to parse HTML content simultaneously, optimizing both fetching and processing times. Here’s a code of this combined approach: import aiohttp import asyncio from bs4 import BeautifulSoup from concurrent.futures import ThreadPoolExecutor import cProfile \\# List of URLs to fetch data from urls = \\[ \"https://news.ycombinator.com/\", \"https://news.ycombinator.com/?p=2\", \"https://news.ycombinator.com/?p=3\", \\] async def fetch\\(session, url\\): \"\"\" Asynchronously fetches the content from a given URL using the provided session. \"\"\" async with session.get\\(url\\) as response: content = await response.text\\(\\) print\\(f\"Fetched URL: \\{url\\}\"\\) return content def parse\\(html\\): soup = BeautifulSoup\\(html, \"lxml\"\\) async def main\\(\\): \"\"\" Main function to fetch URLs concurrently and parse their HTML content. \"\"\" async with aiohttp.ClientSession\\(\\) as session: \\# Create a list of tasks for fetching URLs tasks = \\[fetch\\(session, url\\) for url in urls\\] \\# Gather all responses concurrently htmls = await asyncio.gather\\(\\*tasks\\) \\# Use ThreadPoolExecutor to parse HTML content in parallel with ThreadPoolExecutor\\(max\\_workers=5\\) as executor: executor.map\\(parse, htmls\\) if \\_\\_name\\_\\_ == \"\\_\\_main\\_\\_\": \\# Profile the asynchronous main function to analyze performance cProfile.run\\(\"asyncio.run\\(main\\(\\)\\)\"\\) This combined approach yields 125,608 function calls completed in just 1.642 seconds. In this hybrid approach, \\`aiohttp\\` and \\`asyncio\\` are used to fetch web pages asynchronously, while \\`ThreadPoolExecutor\\` handles the HTML parsing concurrently. By combining these methods, you can use the strengths of both asynchronous processing and multi-threading to achieve maximum performance in your web scraping tasks. \\*\\*Observation?\\*\\* The combined approach delivered the best performance, with an execution time of just \\*\\*1.642\\*\\* seconds, compared to 23.274 seconds for multi-threading alone and 22.383 seconds for pure asynchronous processing. \\#\\# Tip 4: Caching techniques to speed up scraping Caching is an excellent method to improve the performance of your web scraping activities. By temporarily storing data, caching decreases the frequency of repeated requests to the server. Therefore, when you make the same request again, it retrieves the data from the local cache rather than fetching it from the server, ultimately reducing the load on external resources. One of the easiest ways to add caching to your web scraping is by using the requests-cache , a persistent cache for Python requests. First, install the \\`requests-cache\\` library using pip: pip install requests-cache You can then use \\`requests\\_cache.CachedSession\\` to make your requests. This session behaves like a standard \\`requests.Session\\`, but includes caching functionality. Here's a simple code: import requests\\_cache \\# Import the library for caching HTTP requests from bs4 import BeautifulSoup import cProfile def fetch\\_data\\(\\): \\# Create a cached session that stores responses for 24 hours \\(86400 seconds\\) session = requests\\_cache.CachedSession\\(\"temp\\_cache\", expire\\_after=86400\\) response = session.get\\(\"https://news.ycombinator.com/\"\\) \\# Check if the response is from the cache if response.from\\_cache: print\\(\"Data is coming from cache\"\\) else: print\\(\"Data is coming from server\"\\) soup = BeautifulSoup\\(response.content, \"lxml\"\\) print\\(soup.title.text\\) \\# Profile the function call cProfile.run\\(\"fetch\\_data\\(\\)\"\\) When you run the script for the first time, data is fetched from the server, as shown below: In this initial request, the operation involves approximately 602,701 function calls and takes about 2.939 seconds. Now, when you run the same script again, the data is retrieved from the cache: Here, the cached response makes about 602,477 function calls and completes in 1.333 seconds. This shows an improvement in speed and efficiency due to caching. \\#\\#\\# Customizing cache \\`requests-cache\\` allows you to customize cache behaviour according to your needs. For example, you can choose different backends \\(e.g., SQLite, Redis\\), set expiration times, and specify which HTTP methods to cache. Here’s the code import requests\\_cache import cProfile def fetch\\_data\\(\\): session = requests\\_cache.CachedSession\\( \"temp\\_cache\", backend=\"sqlite\", \\# use SQLite as the caching backend expire\\_after=3600, \\# cache expires after 1 hour allowable\\_methods=\\[\"GET\", \"POST\"\\], \\# cache both GET and POST requests \\) response = session.get\\(\"https://news.ycombinator.com/\"\\) \\# ... cProfile.run\\(\"fetch\\_data\\(\\)\"\\) You might also want to cache only specific types of responses, such as successful ones. This can be achieved using the \\`cache\\_filter\\` parameter, which allows you to define a custom filtering function: import requests\\_cache import cProfile def is\\_successful\\(response\\): \"\"\"Cache only responses with status code 200.\"\"\" return response.status\\_code == 200 def fetch\\_data\\(\\): \\# Create a cached session with a filter to cache only successful responses session = requests\\_cache.CachedSession\\( \"temp\\_cache\", expire\\_after=86400, cache\\_filter=is\\_successful \\) response = session.get\\(\"https://news.ycombinator.com/\"\\) \\# ... cProfile.run\\(\"fetch\\_data\\(\\)\"\\) \\#\\# Tip 5: Using \\`SoupStrainer\\` for partial parsing When working with large documents, sometimes you might prefer to parse just a portion rather than the entire content. Beautiful Soup makes this easier with \\`SoupStrainer\\`, which lets you filter out the unnecessary parts and focus only on what matters. Here’s the code: import cProfile import requests from bs4 import BeautifulSoup, SoupStrainer \\# Function to parse HTML content using lxml parser def parse\\_html\\(url\\): response = requests.get\\(url\\) only\\_a\\_tags = SoupStrainer\\(\"a\"\\) soup = BeautifulSoup\\(response.content, \"lxml\", parse\\_only=only\\_a\\_tags\\) for link in soup: print\\(link.get\\(\"href\"\\)\\) \\# Profile the parse\\_html function using cProfile cProfile.run\\('parse\\_html\\(\"https://news.ycombinator.com/\"\\)'\\) The result is: So, by parsing only the parts of the document you need, you can significantly reduce the time and resources required for parsing. \\#\\# Tip 6: Optimizing CSS selectors When using Beautiful Soup to extract specific elements from HTML, the efficiency of your CSS selectors plays a crucial role in performance. By narrowing the scope of your selectors, you can significantly reduce parsing time. Instead of using broad selectors that traverse a large portion of the DOM tree, focus on targeting elements directly. \\# Using efficient CSS selectors soup.select\\(\"div.classname > p\"\\) > 💡 You can also refer to a detailed guide on using CSS selectors for web > scraping . \\#\\# Tip 7: Using proxy servers for load balancing Proxy servers act as intermediaries between your scraper and the target website. They distribute requests across multiple IP addresses, which helps reduce the risk of being blocked and balances the load of requests. This is particularly useful for scraping websites with rate limits or IP-based restrictions. > 💡 You can also check out the detailed guide on getting started with > BeautifulSoup \\*\\*.\\*\\* To use proxies with Beautiful Soup, you can configure the \\`requests\\` library to route requests through a proxy server. Here’s the code: import requests from bs4 import BeautifulSoup \\# Define the proxy server proxies = \\{ \"http\": \"http://your\\_proxy\\_ip:port\", \"https\": \"http://your\\_proxy\\_ip:port\", \\} \\# Make a request through the proxy server response = requests.get\\(\"https://news.ycombinator.com/\", proxies=proxies\\) soup = BeautifulSoup\\(response.content, \"lxml\"\\) In the code, requests are routed through the proxy server specified in the \\`proxies\\` dictionary. This helps in balancing the request load and improving the performance of Beautiful Soup. \\#\\# Tip 8: Managing rate limits with sessions and proxies When web scraping, it's important to respect the rate limits set by the target server to avoid getting blocked. You can effectively manage the frequency of your requests by using sessions and proxies. Additionally, introducing a delay between requests can also help. import requests import time \\# Initialize a session for HTTP requests web\\_session = requests.Session\\(\\) \\# Proxy settings proxy\\_settings = \\{ 'http': 'http://your\\_proxy\\_ip:port', 'https': 'http://your\\_proxy\\_ip:port', \\} \\# Apply proxy settings to the session web\\_session.proxies.update\\(proxy\\_settings\\) def retrieve\\_web\\_data\\(target\\_url\\): \\# Make a request using the predefined session result = web\\_session.get\\(target\\_url\\) if result.status\\_code == 200: print\\(\"Successfully retrieved data.\"\\) else: print\\(\"Data retrieval failed.\"\\) return result \\# Target URL target\\_url = 'https://news.ycombinator.com/' \\# Execute data retrieval with pauses between requests for \\_ in range\\(5\\): retrieve\\_web\\_data\\(target\\_url\\) time.sleep\\(2\\) \\# Wait for 1 second between requests In the code, using \\`requests.Session\\` can speed up web scraping by reusing connections. Setting up proxies to rotate IP addresses helps distribute the load and avoid rate limits. Additionally, using \\`time.sleep\\(\\)\\` introduces pauses between requests, which helps you stay within server limits and reduces the risk of getting blocked. \\#\\# Tip 9: Error handling and retry logic with proxies When scraping the web, using proxies is great, but you also have to manage connection errors and timeouts. Therefore, it's important to use error handling and retry logic to ensure effective web scraping. For example, if a request fails, attempting a retry with a different proxy can significantly improve the likelihood of success. Here’s the code of how to implement retry logic with proxies. import requests from requests.exceptions import RequestException, ProxyError, Timeout import random from bs4 import BeautifulSoup proxies\\_list = \\[ \"http://your\\_proxy\\_ip:port\", \"http://your\\_proxy\\_ip:port\", \"http://your\\_proxy\\_ip:port\", \\] def fetch\\_with\\_retry\\(url, retries=3\\): session = requests.Session\\(\\) for attempt in range\\(1, retries + 1\\): try: proxy = \\{\"http\": random.choice\\(proxies\\_list\\)\\} response = session.get\\(url, proxies=proxy, timeout=10\\) if response.status\\_code == 200: print\\(f\"Attempt \\{attempt\\} succeeded.\"\\) return BeautifulSoup\\(response.text, \"lxml\"\\) print\\( f\"Attempt \\{attempt\\} failed with status code: \\{ response.status\\_code\\}\" \\) except \\(RequestException, ProxyError, Timeout\\) as e: print\\(f\"Attempt \\{attempt\\} failed with error: \\{e\\}\"\\) continue return None if \\_\\_name\\_\\_ == \"\\_\\_main\\_\\_\": url = \"https://news.ycombinator.com\" result = fetch\\_with\\_retry\\(url\\) if result: print\\(\"Fetch successful\\!\"\\) else: print\\(\"Failed to fetch the URL after several retries.\"\\) This code tries to fetch data from a URL using different proxy servers. It retries the request up to three times if it fails, picking a new proxy for each attempt. If successful, it parses the HTML content with BeautifulSoup. If not, it continues trying with different proxies until it either succeeds or exhausts all attempts. \\#\\# Tip 10: Efficient data extraction with XPath Although Beautiful Soup is great for parsing HTML and navigating the document tree, XPath expressions can provide greater precision and flexibility when it comes to complex data extraction. The \\`lxml\\` library is particularly powerful because it combines the ease of HTML parsing with the advanced querying capabilities of XPath and XSLT. Before you can start using \\`lxml\\`, you'll need to install it: pip install lxml Here's a quick example of how you can use \\`lxml\\` with XPath: import requests from lxml import html import cProfile def fetch\\_data\\(\\): url = 'https://news.ycombinator.com/' response = requests.get\\(url\\) if response.status\\_code == 200: \\# Parse the HTML content into a tree structure tree = html.fromstring\\(response.content\\) \\# Use XPath to extract data titles = tree.xpath\\( '//tr\\[@class=\"athing\"\\]//span\\[@class=\"titleline\"\\]/a/text\\(\\)'\\) else: print\\(\"Failed to fetch the page. Status code:\", response.status\\_code\\) cProfile.run\\('fetch\\_data\\(\\)'\\) The result shows that we successfully extracted all the news titles from the URL in just \\*\\*1.2\\*\\* seconds which is quick and efficient. XPath expressions can be more efficient and concise than traditional CSS selectors, especially when dealing with complex document structures. This method can significantly speed up the data extraction process. \\#\\# Tips for efficient HTML parsing Here are some tips on how to parse HTML more efficiently. \\*\\*1\\\\. Navigating the DOM tree:\\*\\* Understanding the Document Object Model \\(DOM\\) is crucial for efficient HTML parsing. The DOM represents the structure of an HTML document as a tree of objects. \\*\\*2\\\\. Traversing the DOM:\\*\\* \\* Use \\`.parent\\` to access the parent of the tag and \\`.children\\` to iterate over a children of the tag. This hierarchical navigation is useful for extracting nested data. \\* Use \\`.next\\_sibling\\` and \\`.previous\\_sibling\\` to move horizontally across the DOM tree. This is useful for extracting data from elements that are on the same level. \\*\\*3\\\\. Searching the DOM Tree:\\*\\* Efficient searching within the DOM tree is vital for extracting specific data points. BeautifulSoup offers several methods to facilitate this: \\* Use methods such as \\`find\\(\\)\\` and \\`find\\_all\\(\\)\\` to locate elements by tag name or attributes. \\* The \\`select\\(\\)\\` method allows for more complex queries using CSS selectors. \\*\\*4\\\\. Handling large documents:\\*\\* For large HTML documents, performance can be a concern. So, you can follow these quick tips: \\* Consider using the \\`lxml\\` Parser, which is faster and more efficient for parsing large documents, as discussed earlier in detail. \\* Install cchardet as this library speeds up encoding detection, which can be a bottleneck when parsing large files. \\* You can also use \\`SoupStrainer\\` to limit parsing to only the necessary parts of the document, as discussed earlier in detail. \\*\\*5\\\\. Modifying the parse tree:\\*\\* BeautifulSoup allows you the modification of the parse tree, like the addition, deletion, or alteration of HTML elements. This is particularly useful for cleaning up scraped data or preparing it for further analysis. \\*\\*6\\\\. Error handling and logging:\\*\\* It's important to implement error handling in your code. BeautifulSoup can encounter a lot of issues with malformed HTML or missing tags, which leads to exceptions. Therefore, using \\`try-except\\` blocks and logging errors can help in debugging and improving the stability of your scraper. \\*\\*7\\\\. Integrating with other tools:\\*\\* For JavaScript-heavy sites, integrating BeautifulSoup with tools like Selenium , Playwright , or Puppeteer can be very effective. BeautifulSoup can handle static HTML well, but these tools can interact with the browser to scrape dynamic content that JavaScript generates. \\#\\# Use a Scraping API instead Data extraction is not easy nowadays, as it involves many challenges due to the anti-bot measures put in place. Bypassing anti-bot mechanisms can be challenging and take up a lot of time and resources. That’s where web scraping APIs like \\*\\*ScrapingBee\\*\\* come in\\! ScrapingBee simplifies the scraping process by handling the hard parts like rotating proxies and rendering JavaScript, so you don’t have to worry about getting blocked. You can focus on extracting valuable data without the need to invest time and resources in optimizing BeautifulSoup for performance. To start, sign up for a free ScrapingBee trial no credit card is needed, and you'll receive 1000 credits to begin. Each request costs approximately 25 credits. Next, install the ScrapingBee Python client : pip install scrapingbee You can use the below Python code to begin web scraping: from scrapingbee import ScrapingBeeClient client = ScrapingBeeClient\\(api\\_key=\"YOUR\\_API\\_KEY\"\\) response = client.get\\( \"https://www.g2.com/products/anaconda/reviews\", params=\\{ \"stealth\\_proxy\": True, \\# Use stealth proxies for tougher sites \"block\\_resources\": True, \\# Block images and CSS to speed up loading \"wait\": \"1500\", \\# Milliseconds to wait before capturing data \\# \"device\": \"desktop\", \\# Set the device type \\# \"country\\_code\": \"gb\", \\# Specify the country code \\# Optional screenshot settings: \\# \"screenshot\": True, \\# \"screenshot\\_full\\_page\": True, \\}, \\) print\\(\"Response HTTP Status Code: \", response.status\\_code\\) print\\(\"Response HTTP Response Body: \", response.text\\) The status code \\*\\*200\\*\\* indicates that the G2 anti-bot has been bypassed. Using a web scraping API like ScrapingBee saves you from dealing with various anti-scraping measures, making your data collection efficient and less prone to blocks. \\#\\# Wrapping up In this tutorial we've shown you 10 expert tips and tricks for speeding up your scraping with Beautiful Soup, some of these concepts can be applied to different scraping libraries as well. Scaling your scraping operation can be a mammoth technical challenge to get right in a cost-effective and efficient way, but with these pointers, you should be on the way to becoming a scraping master. If you want to skip all of the technical challenges of scraping, we've done the hard work for you with our web scraping API which will allow you to easily scrape any page with one API call, give it a spin with 1,000 free credits, just sign up and start scraping. \\*\\*Satyam Tripathi\\*\\* Satyam is a senior technical writer who is passionate about web scraping, automation, and data engineering. He has delivered over 130 blog posts since 2021\\. \\_\\_ \\#\\#\\# You might also like: \\#\\#\\#\\# How to bypass error 1005 'access denied, you have been banned' when scraping \\*\\*Satyam Tripathi\\*\\* 8 min read Learn how to bypass Cloudflare Error 1005 when scraping websites. Discover techniques like residential proxies, headless browsers, and more to avoid IP bans. \\#\\#\\#\\# Mapping the Funniest US States on Reddit using AI \\*\\*Karthik Devan\\*\\* 3 min read Discover which US states joke the most on Reddit using AI. Explore our detailed analysis ranking states by humorous comments and uncover surprising patterns\\! \\#\\#\\#\\# Puppeteer Stealth Tutorial; How to Set Up & Use \\(+ Working Alternatives\\) \\*\\*Satyam Tripathi\\*\\* 13 min read Learn how to use Puppeteer Stealth to bypass anti-scraping measures and avoid detection. This guide offers advanced techniques and alternatives for seamless web scraping. \\#\\# Tired of getting blocked while scraping the web? ScrapingBee API handles headless browsers and rotates proxies for you. Get access to 1,000 free API credits, no credit card required\\! Try ScrapingBee for Free ScrapingBee API handles headless browsers and rotates proxies for you. \\* \\_\\_ \\* \\_\\_ \\#\\#\\#\\# Company \\* Team \\* Company's journey \\* Blog \\* Rebranding \\* Affiliate Program \\#\\#\\#\\# Tools \\* Curl converter \\#\\#\\#\\# Legal \\* Terms of Service \\* Privacy Policy \\* GDPR Compliance \\* Data Processing Agreement \\* Cookie Policy \\* Acceptable Use Policy \\* Legal Notices \\#\\#\\#\\# Product \\* Features \\* Pricing \\* Status \\#\\#\\#\\# How we compare \\* Alternative to Crawlera \\* Alternative to Luminati \\* Alternative to NetNut \\* Alternative to ScraperAPI \\* Alternatives to ScrapingBee \\#\\#\\#\\# No code web scraping \\* No code web scraping \\* No code competitor monitoring \\* How to put scraped website data into Google Sheets \\* Send stock prices update to Slack \\* Scrape Amazon products' price with no code \\* Scrape Amazon products' price with no code \\* Extract job listings, details and salaries \\#\\#\\#\\# Learning Web Scraping \\* Web scraping questions \\* A guide to Web Scraping without getting blocked \\* Web Scraping Tools \\* Best Free Proxies \\* Best Mobile proxies \\* Web Scraping vs Web Crawling \\* Rotating and residential proxies \\* Web Scraping with Python \\* Web Scraping with PHP \\* Web Scraping with Java \\* Web Scraping with Ruby \\* Web Scraping with NodeJS \\* Web Scraping with R \\* Web Scraping with C\\# \\* Web Scraping with C++ \\* Web Scraping with Elixir \\* Web Scraping with Perl \\* Web Scraping with Rust \\* Web Scraping with Go \\_\\_ Copyright © 2025 Made in France \n",
      "\n",
      "Theme Auto Light Dark \\#\\#\\#\\# Previous topic Networking and Interprocess Communication \\#\\#\\#\\# Next topic Runners \\#\\#\\# This Page \\* Report a Bug \\* Show Source \\#\\#\\# Navigation \\* index \\* modules | \\* next | \\* previous | \\* \\* Python » \\* \\* \\* 3.13.2 Documentation » \\* The Python Standard Library » \\* Networking and Interprocess Communication » \\* \\`asyncio\\` — Asynchronous I/O \\* | \\* Theme Auto Light Dark | \\# \\`asyncio\\` — Asynchronous I/O¶ \\* \\* \\* Hello World\\! import asyncio async def main\\(\\): print\\('Hello ...'\\) await asyncio.sleep\\(1\\) print\\('... World\\!'\\) asyncio.run\\(main\\(\\)\\) asyncio is a library to write \\*\\*concurrent\\*\\* code using the \\*\\*async/await\\*\\* syntax. asyncio is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc. asyncio is often a perfect fit for IO-bound and high-level \\*\\*structured\\*\\* network code. asyncio provides a set of \\*\\*high-level\\*\\* APIs to: \\* run Python coroutines concurrently and have full control over their execution; \\* perform network IO and IPC; \\* control subprocesses; \\* distribute tasks via queues; \\* synchronize concurrent code; Additionally, there are \\*\\*low-level\\*\\* APIs for \\_library and framework developers\\_ to: \\* create and manage event loops, which provide asynchronous APIs for networking, running subprocesses, handling OS signals, etc; \\* implement efficient protocols using transports; \\* bridge callback-based libraries and code with async/await syntax. Availability: not WASI. This module does not work or is not available on WebAssembly. See WebAssembly platforms for more information. asyncio REPL You can experiment with an \\`asyncio\\` concurrent context in the REPL: $ python -m asyncio asyncio REPL ... Use \"await\" directly instead of \"asyncio.run\\(\\)\". Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import asyncio >>> await asyncio.sleep\\(10, result='hello'\\) 'hello' Raises an auditing event \\`cpython.run\\_stdin\\` with no arguments. Changed in version 3.12.5: \\(also 3.11.10, 3.10.15, 3.9.20, and 3.8.20\\) Emits audit events. Changed in version 3.13: Uses PyREPL if possible, in which case \\`PYTHONSTARTUP\\` is also executed. Emits audit events. Reference High-level APIs \\* Runners \\* Coroutines and Tasks \\* Streams \\* Synchronization Primitives \\* Subprocesses \\* Queues \\* Exceptions Low-level APIs \\* Event Loop \\* Futures \\* Transports and Protocols \\* Policies \\* Platform Support \\* Extending Guides and Tutorials \\* High-level API Index \\* Low-level API Index \\* Developing with asyncio Note The source code for asyncio can be found in Lib/asyncio/. \\#\\#\\#\\# Previous topic Networking and Interprocess Communication \\#\\#\\#\\# Next topic Runners \\#\\#\\# This Page \\* Report a Bug \\* Show Source « \\#\\#\\# Navigation \\* index \\* modules | \\* next | \\* previous | \\* \\* Python » \\* \\* \\* 3.13.2 Documentation » \\* The Python Standard Library » \\* Networking and Interprocess Communication » \\* \\`asyncio\\` — Asynchronous I/O \\* | \\* Theme Auto Light Dark | \\(C\\) Copyright 2001-2025, Python Software Foundation. This page is licensed under the Python Software Foundation License Version 2. Examples, recipes, and other code in the documentation are additionally licensed under the Zero Clause BSD License. See History and License for more information. The Python Software Foundation is a non-profit corporation. Please donate. Last updated on Mar 09, 2025 \\(07:27 UTC\\). Found a bug? Created using Sphinx 8.2.3. \n",
      "\n",
      "\\* Start Here \\* Learn Python Python Tutorials → In-depth articles and video courses Learning Paths → Guided study plans for accelerated learning Quizzes → Check your learning progress Browse Topics → Focus on a specific area or skill level Community Chat → Learn with other Pythonistas Office Hours → Live Q&A; calls with Python experts Podcast → Hear what’s new in the world of Python Books → Round out your knowledge and learn offline Reference → Concise definitions for common Python terms Code Mentor →Beta Personalized code assistance & learning tools Unlock All Content → \\* More Learner Stories Python Newsletter Python Job Board Meet the Team Become a Tutorial Writer Become a Video Instructor \\* Search / \\* Join \\* Sign‑In — FREE Email Series — 🐍 Python Tricks 💌 Get Python Tricks » 🔒 No spam. Unsubscribe any time. Browse Topics Guided Learning Paths Basics Intermediate Advanced \\* \\* \\* api best-practices career community databases data-science data-structures data-viz devops django docker editors flask front-end gamedev gui machine- learning numpy projects python testing tools web-dev web-scraping Table of Contents \\* Choose the Right XML Parsing Model \\* Document Object Model \\(DOM\\) \\* Simple API for XML \\(SAX\\) \\* Streaming API for XML \\(StAX\\) \\* Learn About XML Parsers in Python's Standard Library \\* xml.dom.minidom: Minimal DOM Implementation \\* xml.sax: The SAX Interface for Python \\* xml.dom.pulldom: Streaming Pull Parser \\* xml.etree.ElementTree: A Lightweight, Pythonic Alternative \\* Explore Third-Party XML Parser Libraries \\* untangle: Convert XML to a Python Object \\* xmltodict: Convert XML to a Python Dictionary \\* lxml: Use ElementTree on Steroids \\* BeautifulSoup: Deal With Malformed XML \\* Bind XML Data to Python Objects \\* Define Models With XPath Expressions \\* Generate Models From an XML Schema \\* Defuse the XML Bomb With Secure Parsers \\* Conclusion Mark as Completed Share \\# A Roadmap to XML Parsers in Python by Bartosz Zaczyński intermediate Mark as Completed Share Table of Contents \\* Choose the Right XML Parsing Model \\* Document Object Model \\(DOM\\) \\* Simple API for XML \\(SAX\\) \\* Streaming API for XML \\(StAX\\) \\* Learn About XML Parsers in Python's Standard Library \\* xml.dom.minidom: Minimal DOM Implementation \\* xml.sax: The SAX Interface for Python \\* xml.dom.pulldom: Streaming Pull Parser \\* xml.etree.ElementTree: A Lightweight, Pythonic Alternative \\* Explore Third-Party XML Parser Libraries \\* untangle: Convert XML to a Python Object \\* xmltodict: Convert XML to a Python Dictionary \\* lxml: Use ElementTree on Steroids \\* BeautifulSoup: Deal With Malformed XML \\* Bind XML Data to Python Objects \\* Define Models With XPath Expressions \\* Generate Models From an XML Schema \\* Defuse the XML Bomb With Secure Parsers \\* Conclusion Remove ads If you've ever tried to parse an \\*\\*XML document\\*\\* in Python before, then you know how surprisingly difficult such a task can be. On the one hand, the Zen of Python promises only one obvious way to achieve your goal. At the same time, the standard library follows the batteries included motto by letting you choose from not one but several XML parsers. Luckily, the Python community solved this surplus problem by creating even more XML parsing libraries. Jokes aside, all XML parsers have their place in a world full of smaller or bigger challenges. It's worthwhile to familiarize yourself with the available tools. \\*\\*In this tutorial, you 'll learn how to:\\*\\* \\* Choose the right XML \\*\\*parsing model\\*\\* \\* Use the XML parsers in the \\*\\*standard library\\*\\* \\* Use major XML parsing \\*\\*libraries\\*\\* \\* Parse XML documents declaratively using \\*\\*data binding\\*\\* \\* Use safe XML parsers to eliminate \\*\\*security vulnerabilities\\*\\* You can use this tutorial as a \\*\\*roadmap\\*\\* to guide you through the confusing world of XML parsers in Python. By the end of it, you'll be able to pick the right XML parser for a given problem. To get the most out of this tutorial, you should already be familiar with XML and its building blocks, as well as how to work with files in Python. \\*\\*Free Bonus:\\*\\* 5 Thoughts On Python Mastery, a free course for Python developers that shows you the roadmap and the mindset you'll need to take your Python skills to the next level. \\#\\# Choose the Right XML Parsing Model It turns out that you can process XML documents using a few language-agnostic strategies. Each demonstrates different memory and speed trade-offs, which can partially justify the wide range of XML parsers available in Python. In the following section, you'll find out their differences and strengths. Remove ads \\#\\#\\# Document Object Model \\(DOM\\) Historically, the first and the most widespread model for parsing XML has been the DOM, or the Document Object Model, originally defined by the World Wide Web Consortium \\(W3C\\). You might have already heard about the DOM because web browsers expose a DOM interface through JavaScript to let you manipulate the HTML code of your websites. Both XML and HTML belong to the same family of markup languages, which makes parsing XML with the DOM possible. The DOM is arguably the most straightforward and versatile model to use. It defines a handful of \\*\\*standard operations\\*\\* for traversing and modifying document elements arranged in a hierarchy of objects. An abstract representation of the entire document tree is stored in memory, giving you \\*\\*random access\\*\\* to the individual elements. While the DOM tree allows for fast and \\*\\*omnidirectional navigation\\*\\* , building its abstract representation in the first place can be time-consuming. Moreover, the XML gets \\*\\*parsed at once\\*\\* , as a whole, so it has to be reasonably small to fit the available memory. This renders the DOM suitable only for moderately large configuration files rather than multi-gigabyte XML databases. Use a DOM parser when convenience is more important than processing time and when memory is not an issue. Some typical use cases are when you need to parse a relatively small document or when you only need to do the parsing infrequently. \\#\\#\\# Simple API for XML \\(SAX\\) To address the shortcomings of the DOM, the Java community came up with a library through a collaborative effort, which then became an alternative model for parsing XML in other languages. There was no formal specification, only organic discussions on a mailing list. The end result was an \\*\\*event-based streaming API\\*\\* that operates sequentially on individual elements rather than the whole tree. Elements are processed from top to bottom in the same order they appear in the document. The parser triggers user-defined callbacks to handle specific XML nodes as it finds them in the document. This approach is known as \\*\\*\" push\" parsing\\*\\* because elements are pushed to your functions by the parser. SAX also lets you discard elements if you're not interested in them. This means it has a much lower memory footprint than DOM and can deal with arbitrarily large files, which is great for \\*\\*single-pass processing\\*\\* such as indexing, conversion to other formats, and so on. However, finding or modifying random tree nodes is cumbersome because it usually requires multiple passes on the document and tracking the visited nodes. SAX is also inconvenient for handling deeply nested elements. Finally, the SAX model just allows for \\*\\*read-only\\*\\* parsing. In short, SAX is cheap in terms of space and time but more difficult to use than DOM in most cases. It works well for parsing very large documents or parsing incoming XML data in real time. \\#\\#\\# Streaming API for XML \\(StAX\\) Although somewhat less popular in Python, this third approach to parsing XML builds on top of SAX. It extends the idea of \\*\\*streaming\\*\\* but uses a \\*\\*\" pull\" parsing\\*\\* model instead, which gives you more control. You can think of StAX as an iterator advancing a \\*\\*cursor object\\*\\* through an XML document, where custom handlers call the parser on demand and not the other way around. \\*\\*Note:\\*\\* It's possible to combine more than one XML parsing model. For example, you can use SAX or StAX to quickly find an interesting piece of data in the document and then build a DOM representation of only that particular branch in memory. Using StAX gives you more control over the parsing process and allows for more convenient \\*\\*state management\\*\\*. The events in the stream are only consumed when requested, enabling lazy evaluation. Other than that, its performance should be on par with SAX, depending on the parser implementation. \\#\\# Learn About XML Parsers in Python's Standard Library In this section, you'll take a look at Python's built-in XML parsers, which are available to you in nearly every Python distribution. You're going to compare those parsers against a sample Scalable Vector Graphics \\(SVG\\) image, which is an XML-based format. By processing the same document with different parsers, you'll be able to choose the one that suits you best. The sample image, which you're about to save in a local file for reference, depicts a smiley face. It consists of the following XML content: XML  \\]>  Some value &custom;\\_entity; <svg>\\! Copied\\! It starts with an \\*\\*XML declaration\\*\\* , followed by a Document Type Definition \\(DTD\\) and the \\`\\` \\*\\*root element\\*\\*. The DTD is optional, but it can help validate your document structure if you decide to use an XML validator. The root element specifies the \\*\\*default namespace\\*\\* \\`xmlns\\` as well as a \\*\\*prefixed namespace\\*\\* \\`xmlns:inkscape\\` for editor-specific elements and attributes. The document also contains: \\* Nested elements \\* Attributes \\* Comments \\* Character data \\(\\`CDATA\\`\\) \\* Predefined and custom entities Go ahead, save the XML in a file named \\_smiley.svg\\_ , and open it using a modern web browser, which will run the JavaScript snippet present at the end: The code adds an interactive component to the image. When you hover the mouse over the smiley face, it blinks its eyes. If you want to edit the smiley face using a convenient graphical user interface \\(GUI\\), then you can open the file using a vector graphics editor such as Adobe Illustrator or Inkscape. \\*\\*Note:\\*\\* Unlike JSON or YAML, some features of XML can be exploited by hackers. The standard XML parsers available in the \\`xml\\` package in Python are insecure and vulnerable to an array of attacks. To safely parse XML documents from an untrusted source, prefer secure alternatives. You can jump to the last section in this tutorial for more details. It's worth noting that Python's standard library defines \\*\\*abstract interfaces\\*\\* for parsing XML documents while letting you supply concrete parser implementation. In practice, you rarely do that because Python bundles a binding for the Expat library, which is a widely used open-source XML parser written in C. All of the following Python modules in the standard library use Expat under the hood by default. Unfortunately, while the Expat parser can tell you if your document is \\*\\*well- formed\\*\\* , it can't \\*\\*validate\\*\\* the structure of your documents against an XML Schema Definition \\(XSD\\) or a Document Type Definition \\(DTD\\). For that, you'll have to use one of the third-party libraries discussed later. Remove ads \\#\\#\\# \\`xml.dom.minidom\\`: Minimal DOM Implementation Considering that parsing XML documents using the DOM is arguably the most straightforward, you won't be that surprised to find a DOM parser in the Python standard library. What is surprising, though, is that there are actually two DOM parsers. The \\`xml.dom\\` package houses two modules to work with DOM in Python: 1\\. \\`xml.dom.minidom\\` 2\\. \\`xml.dom.pulldom\\` The first is a stripped-down implementation of the DOM interface conforming to a relatively old version of the W3C specification. It provides common objects defined by the DOM API such as \\`Document\\`, \\`Element\\`, and \\`Attr\\`. This module is poorly documented and has quite limited usefulness, as you're about to find out. The second module has a slightly misleading name because it defines a \\*\\*streaming pull parser\\*\\* , which can \\_optionally\\_ produce a DOM representation of the current node in the document tree. You'll find more information about the \\`pulldom\\` parser later. There are two functions in \\`minidom\\` that let you parse XML data from various data sources. One accepts either a filename or a file object, while another one expects a Python string: Python >>> from xml.dom.minidom import parse, parseString >>> \\# Parse XML from a filename >>> document = parse\\(\"smiley.svg\"\\) >>> \\# Parse XML from a file object >>> with open\\(\"smiley.svg\"\\) as file: ... document = parse\\(file\\) ... >>> \\# Parse XML from a Python string >>> document = parseString\\(\"\"\"\\ ...  ...  ...  ... \"\"\"\\) Copied\\! The triple-quoted string helps embed a multiline string literal without using the continuation character \\(\\`\\\\\\`\\) at the end of each line. In any case, you'll end up with a \\`Document\\` instance, which exhibits the familiar DOM interface, letting you traverse the tree. Apart from that, you'll be able to access the XML declaration, DTD, and the root element: Python >>> document = parse\\(\"smiley.svg\"\\) >>> \\# XML Declaration >>> document.version, document.encoding, document.standalone \\('1.0', 'UTF-8', False\\) >>> \\# Document Type Definition \\(DTD\\) >>> dtd = document.doctype >>> dtd.entities\\[\"custom\\_entity\"\\].childNodes \\[\\] >>> \\# Document Root >>> document.documentElement  Copied\\! As you can see, even though the default XML parser in Python can't validate documents, it still lets you inspect \\`.doctype\\`, the DTD, if it's present. Note that the XML declaration and DTD are optional. If the XML declaration or a given XML attribute is missing, then the corresponding Python attributes will be \\`None\\`. To find an element by ID, you must use the \\`Document\\` instance rather than a specific parent \\`Element\\`. The sample SVG image has two nodes with an \\`id\\` attribute, but you can't find either of them: Python >>> document.getElementById\\(\"skin\"\\) is None True >>> document.getElementById\\(\"smiley\"\\) is None True Copied\\! That may be surprising for someone who has only worked with HTML and JavaScript but hasn't worked with XML before. While HTML defines the semantics for certain elements and attributes such as \\`\\` or \\`id\\`, XML doesn't attach any meaning to its building blocks. You need to mark an attribute as an ID explicitly using DTD or by calling \\`.setIdAttribute\\(\\)\\` in Python, for example: Definition Style | Implementation \\---|--- DTD | \\`\\` Python | \\`linearGradient.setIdAttribute\\(\"id\"\\)\\` However, using a DTD isn't enough to fix the problem if your document has a default namespace, which is the case for the sample SVG image. To address this, you can visit all elements recursively in Python, check whether they have the \\`id\\` attribute, and indicate it as their ID in one go: Python >>> from xml.dom.minidom import parse, Node >>> def set\\_id\\_attribute\\(parent, attribute\\_name=\"id\"\\): ... if parent.nodeType == Node.ELEMENT\\_NODE: ... if parent.hasAttribute\\(attribute\\_name\\): ... parent.setIdAttribute\\(attribute\\_name\\) ... for child in parent.childNodes: ... set\\_id\\_attribute\\(child, attribute\\_name\\) ... >>> document = parse\\(\"smiley.svg\"\\) >>> set\\_id\\_attribute\\(document\\) Copied\\! Your custom \\`set\\_id\\_attribute\\(\\)\\` function takes a parent element and an optional name for the identity attribute, which defaults to \\`\"id\"\\`. When you call that function on your SVG document, then all children elements that have an \\`id\\` attribute will become accessible through the DOM API: Python >>> document.getElementById\\(\"skin\"\\)  >>> document.getElementById\\(\"smiley\"\\)  Copied\\! Now, you're getting the expected XML element corresponding to the \\`id\\` attribute's value. Using an ID allows for finding at most one unique element, but you can also find a collection of similar elements by their \\*\\*tag name\\*\\*. Unlike the \\`.getElementById\\(\\)\\` method, you can call \\`.getElementsByTagName\\(\\)\\` on the document or a particular parent element to reduce the search scope: Python >>> document.getElementsByTagName\\(\"ellipse\"\\) \\[ ,  \\] >>> root = document.documentElement >>> root.getElementsByTagName\\(\"ellipse\"\\) \\[ ,  \\] Copied\\! Notice that \\`.getElementsByTagName\\(\\)\\` always returns a list of elements instead of a single element or \\`None\\`. Forgetting about it when you switch between both methods is a common source of errors. Unfortunately, elements like \\`\\` that are \\*\\*prefixed\\*\\* with a namespace identifier won't be included. They must be searched using \\`.getElementsByTagNameNS\\(\\)\\`, which expects different arguments: Python >>> document.getElementsByTagNameNS\\( ... \"http://www.inkscape.org/namespaces/inkscape\", ... \"custom\" ... \\) ... \\[\\] >>> document.getElementsByTagNameNS\\(\"\\*\", \"custom\"\\) \\[\\] Copied\\! The first argument must be the XML namespace, which typically has the form of a domain name, while the second argument is the tag name. Notice that the namespace prefix is irrelevant\\! To search all namespaces, you can provide a wildcard character \\(\\`\\*\\`\\). \\*\\*Note:\\*\\* To find the namespaces declared in your XML document, you can check out the root element's attributes. In theory, they could be declared on any element, but the top-level one is where you'd usually find them. Once you locate the element you're interested in, you may use it to walk over the tree. However, another jarring quirk with \\`minidom\\` is how it handles \\*\\*whitespace characters\\*\\* between elements: Python >>> element = document.getElementById\\(\"smiley\"\\) >>> element.parentNode  >>> element.firstChild  >>> element.lastChild  >>> element.nextSibling  >>> element.previousSibling  Copied\\! The newline characters and leading indentation are captured as separate tree elements, which is what the specification requires. Some parsers let you ignore these, but not the Python one. What you can do, however, is collapse whitespace in such nodes manually: Python >>> def remove\\_whitespace\\(node\\): ... if node.nodeType == Node.TEXT\\_NODE: ... if node.nodeValue.strip\\(\\) == \"\": ... node.nodeValue = \"\" ... for child in node.childNodes: ... remove\\_whitespace\\(child\\) ... >>> document = parse\\(\"smiley.svg\"\\) >>> set\\_id\\_attribute\\(document\\) >>> remove\\_whitespace\\(document\\) >>> document.normalize\\(\\) Copied\\! Note that you also have to \\`.normalize\\(\\)\\` the document to combine adjacent text nodes. Otherwise, you could end up with a bunch of redundant XML elements with just whitespace. Again, recursion is the only way to visit tree elements since you can't iterate over the document and its elements with a loop. Finally, this should give you the expected result: Python >>> element = document.getElementById\\(\"smiley\"\\) >>> element.parentNode  >>> element.firstChild  >>> element.lastChild  >>> element.nextSibling  >>> element.previousSibling  >>> element.childNodes \\[ , , , , , ,  \\] Copied\\! Elements expose a few helpful methods and properties to let you query their details: Python >>> element = document.getElementsByTagNameNS\\(\"\\*\", \"custom\"\\)\\[0\\] >>> element.prefix 'inkscape' >>> element.tagName 'inkscape:custom' >>> element.attributes  >>> dict\\(element.attributes.items\\(\\)\\) \\{'x': '42', 'inkscape:z': '555'\\} >>> element.hasChildNodes\\(\\) True >>> element.hasAttributes\\(\\) True >>> element.hasAttribute\\(\"x\"\\) True >>> element.getAttribute\\(\"x\"\\) '42' >>> element.getAttributeNode\\(\"x\"\\)  >>> element.getAttribute\\(\"missing-attribute\"\\) '' Copied\\! For instance, you can check an element's namespace, tag name, or attributes. If you ask for a missing attribute, then you'll get an empty string \\(\\`''\\`\\). Dealing with namespaced attributes isn't much different. You just have to remember to prefix the attribute name accordingly or provide the domain name: Python >>> element.hasAttribute\\(\"z\"\\) False >>> element.hasAttribute\\(\"inkscape:z\"\\) True >>> element.hasAttributeNS\\( ... \"http://www.inkscape.org/namespaces/inkscape\", ... \"z\" ... \\) ... True >>> element.hasAttributeNS\\(\"\\*\", \"z\"\\) False Copied\\! Strangely enough, the wildcard character \\(\\`\\*\\`\\) doesn't work here as it did with the \\`.getElementsByTagNameNS\\(\\)\\` method before. Since this tutorial is only about XML parsing, you'll need to check the \\`minidom\\` documentation for methods that modify the DOM tree. They mostly follow the W3C specification. As you can see, the \\`minidom\\` module isn't terribly convenient. Its main advantage comes from being part of the standard library, which means you don't have to install any external dependencies in your project to work with the DOM. Remove ads \\#\\#\\# \\`xml.sax\\`: The SAX Interface for Python To start working with SAX in Python, you can use the same \\`parse\\(\\)\\` and \\`parseString\\(\\)\\` convenience functions as before, but from the \\`xml.sax\\` package instead. You also have to provide at least one more required argument, which must be a \\*\\*content handler\\*\\* instance. In the spirit of Java, you provide one by subclassing a specific base class: Python from xml.sax import parse from xml.sax.handler import ContentHandler class SVGHandler\\(ContentHandler\\): pass parse\\(\"smiley.svg\", SVGHandler\\(\\)\\) Copied\\! The content handler receives a \\*\\*stream of events\\*\\* corresponding to elements in your document as it's being parsed. Running this code won't do anything useful yet because your handler class is empty. To make it work, you'll need to overload one or more callback methods from the superclass. Fire up your favorite editor, type the following code, and save it in a file named \\`svg\\_handler.py\\`: Python \\# svg\\_handler.py from xml.sax.handler import ContentHandler class SVGHandler\\(ContentHandler\\): def startElement\\(self, name, attrs\\): print\\(f\"BEGIN: <\\{name\\}>, \\{attrs.keys\\(\\)\\}\"\\) def endElement\\(self, name\\): print\\(f\"END: \"\\) def characters\\(self, content\\): if content.strip\\(\\) \\!= \"\": print\\(\"CONTENT:\", repr\\(content\\)\\) Copied\\! This modified content handler prints out a few events onto the standard output. The SAX parser will call these three methods for you in response to finding the start tag, end tag, and some text between them. When you open an interactive session of the Python interpreter, import your content handler and give it a test drive. It should produce the following output: Python >>> from xml.sax import parse >>> from svg\\_handler import SVGHandler >>> parse\\(\"smiley.svg\", SVGHandler\\(\\)\\) BEGIN: , \\['xmlns', 'xmlns:inkscape', 'viewBox', 'width', 'height'\\] BEGIN: , \\['x', 'inkscape:z'\\] CONTENT: 'Some value' END:  BEGIN: , \\[\\] BEGIN: , \\['id', 'x1', 'x2', 'y1', 'y2'\\] BEGIN: , \\['offset', 'stop-color', 'stop-opacity'\\] END:  ⋮ Copied\\! That's essentially the observer design pattern, which lets you translate XML into another hierarchical format incrementally. Say you wanted to convert that SVG file into a simplified JSON representation. First, you'll want to store your content handler object in a separate variable to extract information from it later: Python >>> from xml.sax import parse >>> from svg\\_handler import SVGHandler >>> handler = SVGHandler\\(\\) >>> parse\\(\"smiley.svg\", handler\\) Copied\\! Since the SAX parser emits events without providing any context about the element it's found, you need to keep track of where you are in the tree. Therefore, it makes sense to push and pop the current element onto a stack, which you can simulate through a regular Python list. You may also define a helper property \\`.current\\_element\\` that will return the last element placed on the top of the stack: Python \\# svg\\_handler.py \\# ... class SVGHandler\\(ContentHandler\\): def \\_\\_init\\_\\_\\(self\\): super\\(\\).\\_\\_init\\_\\_\\(\\) self.element\\_stack = \\[\\] @property def current\\_element\\(self\\): return self.element\\_stack\\[-1\\] \\# ... Copied\\! When the SAX parser finds a new element, you can immediately capture its tag name and attributes while making placeholders for children elements and the value, both of which are optional. For now, you can store every element as a \\`dict\\` object. Replace your existing \\`.startElement\\(\\)\\` method with a new implementation: Python \\# svg\\_handler.py \\# ... class SVGHandler\\(ContentHandler\\): \\# ... def startElement\\(self, name, attrs\\): self.element\\_stack.append\\(\\{ \"name\": name, \"attributes\": dict\\(attrs\\), \"children\": \\[\\], \"value\": \"\" \\}\\) Copied\\! The SAX parser gives you attributes as a mapping that you can convert to a plain Python dictionary with a call to the \\`dict\\(\\)\\` function. The element value is often spread over multiple pieces that you can concatenate using the plus operator \\(\\`+\\`\\) or a corresponding augmented assignment statement: Python \\# svg\\_handler.py \\# ... class SVGHandler\\(ContentHandler\\): \\# ... def characters\\(self, content\\): self.current\\_element\\[\"value\"\\] += content Copied\\! Aggregating text in such a way will ensure that multiline content ends up in the current element. For example, the \\` Copied\\! The client connects to a local server listening on port 8000. Once you save the HTML code in a file, you'll be able to open it with your favorite web browser. But before that, you'll need to implement the server. Python doesn't come with WebSocket support, but you can install the \\`websockets\\` library into your active virtual environment. You're also going to need \\`lxml\\` later, so it's a good moment to install both dependencies in one go: Shell $ python -m pip install websockets lxml Copied\\! Finally, you can scaffold a minimal asynchronous web server: Python \\# server.py import asyncio import websockets async def handle\\_connection\\(websocket, path\\): async for message in websocket: print\\(message\\) if \\_\\_name\\_\\_ == \"\\_\\_main\\_\\_\": future = websockets.serve\\(handle\\_connection, \"localhost\", 8000\\) asyncio.get\\_event\\_loop\\(\\).run\\_until\\_complete\\(future\\) asyncio.get\\_event\\_loop\\(\\).run\\_forever\\(\\) Copied\\! When you start the server and open the saved HTML file in a web browser, you should see XML messages appear in the standard output in response to your mouse moves and key presses. You can open the client in multiple tabs or even multiple browsers simultaneously\\! Remove ads \\#\\#\\# Define Models With XPath Expressions Right now, your messages arrive in plain string format. It's not very convenient to work with the messages in this format. Fortunately, you can turn them into compound Python objects with a single line of code using the \\`lxml.objectify\\` module: Python \\# server.py import asyncio import websockets import lxml.objectify async def handle\\_connection\\(websocket, path\\): async for message in websocket: try: xml = lxml.objectify.fromstring\\(message\\) except SyntaxError: print\\(\"Malformed XML message:\", repr\\(message\\)\\) else: if xml.tag == \"KeyboardEvent\": if xml.Type == \"keyup\": print\\(\"Key:\", xml.Key.Unicode\\) elif xml.tag == \"MouseEvent\": screen = xml.Cursor.Screen print\\(\"Mouse:\", screen.get\\(\"x\"\\), screen.get\\(\"y\"\\)\\) else: print\\(\"Unrecognized event type\"\\) \\# ... Copied\\! As long as the XML parsing is successful, you can inspect the root element's usual properties, such as the tag name, attributes, inner text, and so on. You'll be able to use the dot operator to navigate deep into the element tree. In most cases, the library will recognize a suitable Python data type and convert the value for you. After saving those changes and restarting the server, you'll need to reload the page in your web browser to make a new WebSocket connection. Here's a sample output of the modified program: Shell $ python server.py Mouse: 820 121 Mouse: 820 122 Mouse: 820 123 Mouse: 820 124 Mouse: 820 125 Key: a Mouse: 820 125 Mouse: 820 125 Key: a Key: A Key: Shift Mouse: 821 125 Mouse: 821 125 Mouse: 820 123 ⋮ Copied\\! Sometimes, XML may contain tag names that aren't valid Python identifiers, or you might want to adapt the message structure to fit your data model. In such a case, an interesting option would be defining custom \\*\\*model classes\\*\\* with descriptors that declare how to look up information using XPath expressions. That's the part that starts to resemble Django models or Pydantic schema definitions. You're going to use a custom \\`XPath\\` descriptor and an accompanying \\`Model\\` class, which provide reusable properties for your data models. The descriptor expects an XPath expression for element lookup in the received message. The underlying implementation is a bit advanced, so feel free to copy the code from the collapsible section below. XPath Descriptor and the Model ClassShow/Hide Python import lxml.objectify class XPath: def \\_\\_init\\_\\_\\(self, expression, /, default=None, multiple=False\\): self.expression = expression self.default = default self.multiple = multiple def \\_\\_set\\_name\\_\\_\\(self, owner, name\\): self.attribute\\_name = name self.annotation = owner.\\_\\_annotations\\_\\_.get\\(name\\) def \\_\\_get\\_\\_\\(self, instance, owner\\): value = self.extract\\(instance.xml\\) instance.\\_\\_dict\\_\\_\\[self.attribute\\_name\\] = value return value def extract\\(self, xml\\): elements = xml.xpath\\(self.expression\\) if elements: if self.multiple: if self.annotation: return \\[self.annotation\\(x\\) for x in elements\\] else: return elements else: first = elements\\[0\\] if self.annotation: return self.annotation\\(first\\) else: return first else: return self.default class Model: \"\"\"Abstract base class for your models.\"\"\" def \\_\\_init\\_\\_\\(self, data\\): if isinstance\\(data, str\\): self.xml = lxml.objectify.fromstring\\(data\\) elif isinstance\\(data, lxml.objectify.ObjectifiedElement\\): self.xml = data else: raise TypeError\\(\"Unsupported data type:\", type\\(data\\)\\) Copied\\! Assuming you already have the desired \\`XPath\\` descriptor and the \\`Model\\` abstract base class in your module, you might use them to define \\`KeyboardEvent\\` and \\`MouseEvent\\` message types along with reusable building blocks to avoid repetition. There are infinite ways to do so, but here's one example: Python \\# ... class Event\\(Model\\): \"\"\"Base class for event messages with common elements.\"\"\" type\\_: str = XPath\\(\"./Type\"\\) timestamp: float = XPath\\(\"./Timestamp\"\\) class Modifiers\\(Model\\): alt: bool = XPath\\(\"./Alt\"\\) ctrl: bool = XPath\\(\"./Ctrl\"\\) shift: bool = XPath\\(\"./Shift\"\\) meta: bool = XPath\\(\"./Meta\"\\) class KeyboardEvent\\(Event\\): key: str = XPath\\(\"./Key/Code\"\\) modifiers: Modifiers = XPath\\(\"./Modifiers\"\\) class MouseEvent\\(Event\\): x: int = XPath\\(\"./Cursor/Screen/@x\"\\) y: int = XPath\\(\"./Cursor/Screen/@y\"\\) modifiers: Modifiers = XPath\\(\"./Modifiers\"\\) Copied\\! The \\`XPath\\` descriptor allows for \\*\\*lazy evaluation\\*\\* so that elements of the XML messages are looked up only when requested. More specifically, they're only looked up when you access a property on the event object. Moreover, the results are \\*\\*cached\\*\\* to avoid running the same XPath query more than once. The descriptor also respects type annotations and converts deserialized data to the right Python type automatically. Using those event objects isn't much different from the ones auto-generated by \\`lxml.objectify\\` before: Python if xml.tag == \"KeyboardEvent\": event = KeyboardEvent\\(xml\\) if event.type\\_ == \"keyup\": print\\(\"Key:\", event.key\\) elif xml.tag == \"MouseEvent\": event = MouseEvent\\(xml\\) print\\(\"Mouse:\", event.x, event.y\\) else: print\\(\"Unrecognized event type\"\\) Copied\\! There's an additional step of creating new objects of the specific event type. But other than that, it gives you more flexibility in terms of structuring your model independently of the XML protocol. Additionally, it's possible to derive new model attributes based on the ones in the received messages and add more methods on top of that. \\#\\#\\# Generate Models From an XML Schema Implementing model classes is a tedious and error-prone task. However, as long as your model mirrors the XML messages, you can take advantage of an automated tool to generate the necessary code for you based on XML Schema. The downside of such code is that it's usually less readable than if written by hand. One of the oldest third-party modules to allow that was PyXB, which mimics Java's popular JAXB library. Unfortunately, it was last released several years ago and was targeting legacy Python versions. You can look into a similar yet actively maintained \\`generateDS\\` alternative, which generates data structures from XML Schema. Let's say you have this \\`models.xsd\\` schema file describing your \\`KeyboardEvent\\` message: XML  Copied\\! A schema tells the XML parser what elements to expect, their order, and their level in the tree. It also restricts the allowed values for the XML attributes. Any discrepancies between these declarations and an actual XML document should render it invalid and make the parser reject the document. Additionally, some tools can leverage this information to produce a piece of code that hides the details of XML parsing from you. After installing the library, you should be able to run the \\`generateDS\\` command in your active virtual environment: Shell $ generateDS -o models.py models.xsd Copied\\! It will create a new file named \\`models.py\\` in the same directory with the generated Python source code. You can then import that module and use it to parse the incoming messages: Python >>> from models import parseString >>> event = parseString\\(\"\"\"\\ ...  ... keydown ... 253459.17999999982 ...  ... `Digit2` ... @ ...  ...  ... false ... false ... true ... false ...  ... \"\"\", silence=True\\) >>> event.Type, event.Key.Code \\('keydown', 'Digit2'\\) Copied\\! It looks similar to the \\`lxml.objectify\\` example shown earlier. The difference is that using data binding enforces compliance with the schema, whereas \\`lxml.objectify\\` produces objects dynamically no matter if they're semantically correct. Remove ads \\#\\# Defuse the XML Bomb With Secure Parsers The XML parsers in Python's standard library are vulnerable to a host of security threats that can lead to denial-of-service \\(DoS\\) or data loss, at best. That isn't their fault, to be fair. They just follow the specification of the XML standard, which is more complicated and powerful than most people know. \\*\\*Note:\\*\\* Please be advised that you should use the information you're about to see wisely. You don't want to wind up being the attacker, exposing yourself to legal consequences, or facing lifetime banishment from using a particular service. One of the most common attacks is the \\*\\*XML Bomb\\*\\* , also known as the billion laughs attack. The attack exploits \\*\\*entity expansion\\*\\* in DTD to blow up the memory and occupy the CPU for as long as possible. All you need to stop an unprotected web server from receiving new traffic are these few lines of XML code: Python import xml.etree.ElementTree as ET ET.fromstring\\(\"\"\"\\  \\]> &lol9;\"\"\"\\) Copied\\! A naïve parser will try to resolve the custom entity \\`&lol9;\\` placed in the document root by inspecting the DTD. However, that entity itself refers to another entity several times, which refers to yet another entity, and so forth. When you run the script above, you'll notice something disturbing about your memory and the processing unit: Look how the main memory and the swap partition are exhausted in just a matter of seconds while one of the CPUs works at 100% of its capacity. The recording stops abruptly when the system memory becomes full and then resumes after the Python process gets killed. Another popular type of attack known as XXE takes advantage of \\*\\*general external entities\\*\\* to read local files and make network requests. Nevertheless, starting from Python 3.7.1, this feature has been disabled by default to increase security. If you trust your data, then you can tell the SAX parser to process external entities anyway: Python >>> from xml.sax import make\\_parser >>> from xml.sax.handler import feature\\_external\\_ges >>> parser = make\\_parser\\(\\) >>> parser.setFeature\\(feature\\_external\\_ges, True\\) Copied\\! This parser will be able to read local files on your computer. It may pull usernames on a Unix-like operating system, for example: Python >>> from xml.dom.minidom import parseString >>> xml = \"\"\"\\ ...  ...  ... \\]> ... &usernames;\"\"\" >>> document = parseString\\(xml, parser\\) >>> print\\(document.documentElement.toxml\\(\\)\\) root:x:0:0:root:/root:/bin/bash daemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin bin:x:2:2:bin:/bin:/usr/sbin/nologin ⋮ realpython:x:1001:1001:Real Python,,,:/home/realpython:/bin/bash  Copied\\! It's perfectly feasible to send that data over the network to a remote server\\! Now, how can you protect yourself from such attacks? The Python official documentation prominently warns you about the risks of using the built-in XML parsers and recommends switching to an external package in mission-critical applications. While not distributed with Python, \\`defusedxml\\` is a \\*\\*drop-in replacement\\*\\* for all the parsers in the standard library. The library imposes strict limits and disables a lot of the dangerous XML features. It should stop most of the well-known attacks, including the two just described. To use it, grab the library from PyPI and replace your import statements accordingly: Python >>> import defusedxml.ElementTree as ET >>> ET.parse\\(\"bomb.xml\"\\) Traceback \\(most recent call last\\): ... raise EntitiesForbidden\\(name, value, base, sysid, pubid, notation\\_name\\) defusedxml.common.EntitiesForbidden: EntitiesForbidden\\(name='lol', system\\_id=None, public\\_id=None\\) Copied\\! That's it\\! Forbidden features won't make it through anymore. \\#\\# Conclusion The XML data format is a mature and surprisingly powerful standard that is still in use today, especially in the enterprise setting. Choosing the right XML parser is crucial in finding the \\*\\*sweet spot\\*\\* between performance, security, compliance, and convenience. This tutorial puts a detailed \\*\\*roadmap\\*\\* in your hand to navigate the confusing maze of XML parsers in Python. You know where to take the shortcuts and how to avoid dead ends, saving you lots of time. \\*\\*In this tutorial, you learned how to:\\*\\* \\* Choose the right XML \\*\\*parsing model\\*\\* \\* Use the XML parsers in the \\*\\*standard library\\*\\* \\* Use major \\*\\*XML parsing libraries\\*\\* \\* Parse XML documents declaratively using \\*\\*data binding\\*\\* \\* Use safe XML parsers to eliminate \\*\\*security vulnerabilities\\*\\* Now, you understand the different strategies for parsing XML documents as well as their strengths and weaknesses. With this knowledge, you're able to pick the most suitable XML parser for your specific use case and even \\*\\*combine\\*\\* more than one to read multi-gigabyte XML files faster. Mark as Completed Share 🐍 Python Tricks 💌 Get a short & sweet \\*\\*Python Trick\\*\\* delivered to your inbox every couple of days. No spam ever. Unsubscribe any time. Curated by the Real Python team. Send Me Python Tricks » About \\*\\*Bartosz Zaczyński\\*\\* Bartosz is a bootcamp instructor, author, and polyglot programmer in love with Python. He helps his students get into software engineering by sharing over a decade of commercial experience in the IT industry. » More about Bartosz \\* \\* \\* \\_Each tutorial at Real Python is created by a team of developers so that it meets our high quality standards. The team members who worked on this tutorial are:\\_ Aldren David Geir Arne Sadie Master \\_Real-World Python Skills\\_ With Unlimited Access to Real Python \\*\\*Join us and get access to thousands of tutorials, hands-on video courses, and a community of expert Pythonistas:\\*\\* Level Up Your Python Skills » Master \\_Real-World Python Skills\\_ With Unlimited Access to Real Python \\*\\*Join us and get access to thousands of tutorials, hands-on video courses, and a community of expert Pythonistas:\\*\\* Level Up Your Python Skills » What Do You Think? \\*\\*Rate this article:\\*\\* LinkedIn Twitter Bluesky Facebook Email What’s your \\#1 takeaway or favorite thing you learned? How are you going to put your newfound skills to use? Leave a comment below and let us know. \\*\\*Commenting Tips:\\*\\* The most useful comments are those written with the goal of learning from or helping out other students. Get tips for asking good questions and get answers to common questions in our support portal. \\* \\* \\* Looking for a real-time conversation? Visit the Real Python Community Chat or join the next \"Office Hours\" Live Q&A; Session. Happy Pythoning\\! Keep Learning Related Topics: intermediate Related Tutorials: \\* Reading and Writing CSV Files in Python \\* Unicode & Character Encodings in Python: A Painless Guide \\* Python Exceptions: An Introduction \\* Working With JSON Data in Python \\* Serialize Your Data With Python \\#\\# Keep reading Real Python by creating a free account or signing in: \\_Continue » Already have an account? Sign-In Almost there\\! Complete this form and click the button below to gain instant access: × 5 Thoughts On Python Mastery Start the Class » 🔒 No spam. We take your privacy seriously. Remove ads © 2012–2025 Real Python ⋅ Newsletter ⋅ Podcast ⋅ YouTube ⋅ Twitter ⋅ Facebook ⋅ Instagram ⋅ Python Tutorials ⋅ Search ⋅ Privacy Policy ⋅ Energy Policy ⋅ Advertise ⋅ Contact Happy Pythoning\\! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from storage import StorageHandler \n",
    "from r2r import R2RAsyncClient\n",
    "client = R2RAsyncClient()\n",
    "s_handler = StorageHandler(client)\n",
    "s_handler.ingest_chunks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13609ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__\n",
      "\n",
      "  * Login\n",
      "  * Sign Up\n",
      "\n",
      "  * Pricing\n",
      "  * FAQ\n",
      "  * Blog\n",
      "  * Other Features\n",
      "    * AI Web Scraping\n",
      "    * Screenshots\n",
      "    * Google search API\n",
      "    * Data extraction\n",
      "    * JavaScript scenario\n",
      "    * No code web scraping\n",
      "  * Developers\n",
      "    * Tutorials\n",
      "    * Documentation\n",
      "    * Knowledge Base\n",
      "\n",
      "# 10 Tips on How to make Python's Beautiful Soup faster when scraping\n",
      "\n",
      "Try ScrapingBee for Free\n",
      "\n",
      "**Satyam Tripathi | 19 September 2024 | 19 min read**\n",
      "\n",
      "Table of contents\n",
      "\n",
      "Beautiful Soup is super easy to use for parsing HTML and is hugely popular.\n",
      "However, if you're extracting a gigantic amount of data from tons of scraped\n",
      "pages it can slow to a crawl if not properly optimized.\n",
      "\n",
      "In this tutorial, I'll show you 10 expert-level tips and tricks for\n",
      "transforming Beautiful Soup into a blazing-fast data-extracting beast and how\n",
      "to optimize your scraping process to be as fast as lightning.\n",
      "\n",
      "## Factors affecting the speed of Beautiful Soup\n",
      "\n",
      "Beautiful Soup's performance can vary based on several factors. Here are some\n",
      "key factors that influence the speed of web scraping using Beautiful Soup .\n",
      "\n",
      "  1. **Parser Choice:** The parser you choose (such as `lxml`, `html.parser`, or `html5lib`) significantly impacts Beautiful Soup's speed and performance.\n",
      "  2. **Document Complexity:** Parsing large or complex HTML/XML documents can be resource-intensive, which ultimately results in slower execution times.\n",
      "  3. **CSS Selectors:** Using complex or inefficient CSS selectors can slow down the parsing process.\n",
      "  4. **Connection Overhead:** Repeatedly establishing new connections for each request can significantly reduce the speed. So, using sessions or persistent connections can help a lot.\n",
      "  5. **Concurrency:** Using multi-threading or asynchronous processing allows you for concurrent fetching and parsing, which improve the overall speed.\n",
      "\n",
      "Many other factors affect Beautiful Soup's performance, which we'll explore\n",
      "further in the following sections, along with strategies to make beautiful\n",
      "soup faster.\n",
      "\n",
      "## How to profile the different performance factors\n",
      "\n",
      "Profiling is an important step in identifying performance bottlenecks in any\n",
      "software application. Python provides several tools for profiling, with\n",
      "`cProfile` being one of the most commonly used for identifying slow parts of\n",
      "the code. By analyzing the output of a profiler, developers can pinpoint areas\n",
      "that require optimization and implement alternative approaches to make\n",
      "beautiful soup faster.\n",
      "\n",
      "`cProfile` is a built-in Python module that allows you to profile the\n",
      "execution time of various parts of a program.\n",
      "\n",
      "Here's a quick example of how to profile a code:\n",
      "\n",
      "    \n",
      "    \n",
      "    import cProfile\n",
      "    from bs4 import BeautifulSoup\n",
      "    import requests\n",
      "    \n",
      "    def fetch_data(url):\n",
      "        response = requests.get(url)\n",
      "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
      "        return soup.title.text\n",
      "    \n",
      "    # Profiling the function to analyze performance\n",
      "    cProfile.run('fetch_data(\"https://news.ycombinator.com\")')\n",
      "    \n",
      "\n",
      "The result is:\n",
      "\n",
      "So, we’re using `cProfile.run()` for a quick overview of the performance.\n",
      "\n",
      "If you want more detailed and sorted profiling results, you can add the\n",
      "following code to sort and format the output:\n",
      "\n",
      "    \n",
      "    \n",
      "    import cProfile\n",
      "    import pstats\n",
      "    from bs4 import BeautifulSoup\n",
      "    import requests\n",
      "    \n",
      "    def fetch_data(url):\n",
      "        response = requests.get(url)\n",
      "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
      "        return soup.title.text\n",
      "    \n",
      "    # Profiling the function to analyze performance\n",
      "    cProfile.run('fetch_data(\"https://news.ycombinator.com\")', \"my_profile\")\n",
      "    \n",
      "    p = pstats.Stats(\"my_profile\")\n",
      "    p.sort_stats(\"cumulative\").print_stats()  # Sort by cumulative time\n",
      "    \n",
      "\n",
      "The result is:\n",
      "\n",
      "Here, we’re `pstats.Stats` with sorting options like `cumulative` to get more\n",
      "detailed and sorted output.\n",
      "\n",
      "By using tools like cProfile to identify bottlenecks and employing strategies\n",
      "such as choosing the right parser, optimizing CSS selectors, leveraging\n",
      "concurrency, and implementing caching, developers can significantly improve\n",
      "the efficiency of their web scraping projects.\n",
      "\n",
      "## Tip 1: Effective use of parsers in beautiful soup\n",
      "\n",
      "One of the main factors that impact the performance of Beautiful Soup is the\n",
      "choice of parser. A common mistake many developers make is selecting any\n",
      "parser without considering its effect on speed and efficiency. Beautiful Soup\n",
      "supports multiple parsers, including `html.parser`, `lxml`, and `html5lib`,\n",
      "each with its unique features and performance characteristics.\n",
      "\n",
      "Choosing the right parser can significantly make beautiful soup faster. But\n",
      "how do you select the best parser for your specific needs? Let's look at the\n",
      "several options to help you make the correct decision.\n",
      "\n",
      "### html5lib\n",
      "\n",
      "`html5lib` is a parser that's great for handling messy HTML because it parses\n",
      "documents the same way a web browser would. It's especially useful when\n",
      "dealing with poorly structured HTML or pages that use newer HTML5 elements.\n",
      "However, this flexibility comes at the cost of slower performance compared to\n",
      "other parsers.\n",
      "\n",
      "Here's the code:\n",
      "\n",
      "    \n",
      "    \n",
      "    import cProfile\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    \n",
      "    # Function to parse HTML content using html5lib parser\n",
      "    def parse_html(url):\n",
      "        response = requests.get(url)\n",
      "        soup = BeautifulSoup(response.content, \"html5lib\")\n",
      "    \n",
      "    # Profile the parse_html function using cProfile\n",
      "    cProfile.run('parse_html(\"https://news.ycombinator.com/\")')\n",
      "    \n",
      "\n",
      "The result is:\n",
      "\n",
      "The results for the `html5lib` parser show that it made 318,133 function calls\n",
      "in **1.030** seconds.\n",
      "\n",
      "### html.parser\n",
      "\n",
      "`html.parser` is the default parser that comes with Python's standard library,\n",
      "and it offers better performance than `html5lib`. It's a good choice for\n",
      "smaller projects or when dealing with HTML that is relatively well-structured.\n",
      "\n",
      "Here's the code:\n",
      "\n",
      "    \n",
      "    \n",
      "    import cProfile\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    \n",
      "    # Function to parse HTML content using html.parser parser\n",
      "    def parse_html(url):\n",
      "        response = requests.get(url)\n",
      "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
      "    \n",
      "    # Profile the parse_html function using cProfile\n",
      "    cProfile.run('parse_html(\"https://news.ycombinator.com/\")')\n",
      "    \n",
      "\n",
      "The result is:\n",
      "\n",
      "The results for `html.parser` shows that it made **170272** function calls in\n",
      "22.626 seconds. This is significantly faster than `html5lib`.\n",
      "\n",
      "### lxml\n",
      "\n",
      "The `lxml` parser is the fastest option among the parsers supported by\n",
      "Beautiful Soup. It's built on the C libraries `libxml2` and `libxslt`, which\n",
      "offer great performance and support for advanced features like XPath and XSLT.\n",
      "\n",
      "Here's the code:\n",
      "\n",
      "    \n",
      "    \n",
      "    import cProfile\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    \n",
      "    # Function to parse HTML content using lxml parser\n",
      "    def parse_html(url):\n",
      "        response = requests.get(url)\n",
      "        soup = BeautifulSoup(response.content, \"lxml\")\n",
      "    \n",
      "    # Profile the parse_html function using cProfile\n",
      "    cProfile.run('parse_html(\"https://news.ycombinator.com/\")')\n",
      "    \n",
      "\n",
      "The result is:\n",
      "\n",
      "The `lxml` parser shows great performance, making **141250** function calls in\n",
      "just 22.625 seconds. This was the fastest result among the parsers tested.\n",
      "\n",
      "### Performance analysis\n",
      "\n",
      "We measured the time each parser took to process the same HTML document.\n",
      "\n",
      "  * **html5lib** : This parser is the slowest, as it takes the most time to process the document. This is due to its comprehensive approach to handling malformed HTML.\n",
      "  * **html.parser** : It is faster than `html5lib`, but it still falls short of `lxml` in terms of speed. It provides a good balance between speed and robustness for moderately well-formed HTML.\n",
      "  * **lxml** : The lxml parser is the fastest and most efficient for parsing well-formed HTML and XML documents.\n",
      "\n",
      "## Tip 2: Reducing network load with sessions\n",
      "\n",
      "Another common mistake developers make is establishing a new connection for\n",
      "each request, which can significantly slow down the process. To avoid this,\n",
      "you can use sessions to maintain persistent connections, reducing connection\n",
      "overhead. The Python `requests` library provides a `Session` object that can\n",
      "be reused for multiple requests to the same server.\n",
      "\n",
      "For example, using a session can be as simple as:\n",
      "\n",
      "    \n",
      "    \n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    import cProfile\n",
      "    \n",
      "    def fetch_data():\n",
      "        # Create a session\n",
      "        session = requests.Session()\n",
      "    \n",
      "        # Use the session to make requests\n",
      "        response = session.get(\"https://news.ycombinator.com/\")\n",
      "        soup = BeautifulSoup(response.content, \"lxml\")\n",
      "    \n",
      "        # Close the session\n",
      "        session.close()\n",
      "    \n",
      "    # Profile the fetch_data function\n",
      "    cProfile.run(\"fetch_data()\")\n",
      "    \n",
      "\n",
      "The result is:\n",
      "\n",
      "Maintaining a session helps you avoid the time-consuming process of setting up\n",
      "a new connection for each request. This is especially useful when scraping\n",
      "multiple pages from the same domain, as it allows faster and more efficient\n",
      "data extraction.\n",
      "\n",
      "## Tip 3: Using multi-threading and asynchronous processing\n",
      "\n",
      "Multi-threading and asynchronous processing are techniques that can\n",
      "significantly improve the performance of web scraping tasks by enabling\n",
      "concurrent execution of tasks.\n",
      "\n",
      "### Using multi-threading\n",
      "\n",
      "The `ThreadPoolExecutor` from the `concurrent.futures` module is a popular\n",
      "tool for implementing multi-threading in Python. It enables you to create a\n",
      "pool of threads and manage their execution efficiently.\n",
      "\n",
      "Here’s how to use `ThreadPoolExecutor` with Beautiful Soup:\n",
      "\n",
      "    \n",
      "    \n",
      "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    import cProfile\n",
      "    \n",
      "    def fetch_url(url):\n",
      "        \"\"\"\n",
      "        Fetches the content of a URL and parses it using BeautifulSoup.\n",
      "        \"\"\"\n",
      "        response = requests.get(url)\n",
      "        print(f\"Processed URL: {url}\")\n",
      "        return BeautifulSoup(response.content, \"lxml\")\n",
      "    \n",
      "    # List of URLs to be processed.\n",
      "    urls = [\n",
      "        \"https://news.ycombinator.com/\",\n",
      "        \"https://news.ycombinator.com/?p=2\",\n",
      "        \"https://news.ycombinator.com/?p=3\",\n",
      "    ]\n",
      "    \n",
      "    def main():\n",
      "        \"\"\"\n",
      "        Main function to fetch and process multiple URLs concurrently.\n",
      "        Uses ThreadPoolExecutor to manage concurrent requests.\n",
      "        \"\"\"\n",
      "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
      "            # Dictionary to track futures and corresponding URLs.\n",
      "            futures = {executor.submit(fetch_url, url): url for url in urls}\n",
      "            for future in as_completed(futures):\n",
      "                url = futures[future]\n",
      "                try:\n",
      "                    # Attempt to get the result of the future.\n",
      "                    soup = future.result()\n",
      "                except Exception as e:\n",
      "                    print(f\"Error processing {url}: {e}\")\n",
      "    \n",
      "    if __name__ == \"__main__\":\n",
      "        # Profile the main function to analyze performance.\n",
      "        cProfile.run(\"main()\")\n",
      "    \n",
      "\n",
      "This approach results in 1,357,674 function calls completed in 23.274 seconds.\n",
      "\n",
      "The code creates a pool of five threads, with each thread responsible for\n",
      "fetching and parsing a URL. The `as_completed` function is then used to\n",
      "iterate over the completed futures, allowing you to process results as soon as\n",
      "it is ready.\n",
      "\n",
      "### Using asynchronous processing\n",
      "\n",
      "For asynchronous processing, the combination of the `asyncio` library with\n",
      "`aiohttp` provides a powerful solution for managing web requests. `aiohttp` is\n",
      "an asynchronous HTTP client that integrates smoothly with `asyncio`, allowing\n",
      "you to perform web scraping without blocking other operations.\n",
      "\n",
      "> 💡You can also check out a detailed guide on how to use asyncio for web\n",
      "> scraping with Python .\n",
      "\n",
      "Here’s an example of how you can use these libraries to implement asynchronous\n",
      "web scraping:\n",
      "\n",
      "    \n",
      "    \n",
      "    import asyncio\n",
      "    import aiohttp\n",
      "    from bs4 import BeautifulSoup\n",
      "    import cProfile\n",
      "    \n",
      "    urls = [\n",
      "        \"https://news.ycombinator.com/\",\n",
      "        \"https://news.ycombinator.com/?p=2\",\n",
      "        \"https://news.ycombinator.com/?p=3\",\n",
      "    ]\n",
      "    \n",
      "    async def fetch_url(session, url):\n",
      "        \"\"\"\n",
      "        Asynchronously fetches the content of a URL using aiohttp and parses it using BeautifulSoup.\n",
      "        \"\"\"\n",
      "        async with session.get(url) as response:\n",
      "            content = await response.text()\n",
      "            print(f\"Processed URL: {url}\")\n",
      "            return BeautifulSoup(content, \"lxml\")\n",
      "    \n",
      "    async def main():\n",
      "        \"\"\"\n",
      "        Main function to create an aiohttp session and fetch all URLs concurrently.\n",
      "        \"\"\"\n",
      "        async with aiohttp.ClientSession() as session:\n",
      "            tasks = [fetch_url(session, url) for url in urls]\n",
      "            await asyncio.gather(*tasks)\n",
      "    \n",
      "    if __name__ == \"__main__\":\n",
      "        cProfile.run(\"asyncio.run(main())\")\n",
      "    \n",
      "\n",
      "This asynchronous approach results in 126,515 function calls completed in\n",
      "22.383 seconds.\n",
      "\n",
      "In this example, `aiohttp.ClientSession` is used to manage HTTP connections,\n",
      "and `asyncio.gather` is used to run multiple asynchronous tasks concurrently.\n",
      "This approach allows you to handle a large number of requests efficiently, as\n",
      "the event loop manages the execution of tasks without blocking.\n",
      "\n",
      "### Combining multi-threading and asynchronous processing\n",
      "\n",
      "Multi-threading and asynchronous processing are effective individually as we\n",
      "can see, however, combining them can lead to even better performance gains.\n",
      "For example, you can use asynchronous processing to handle network requests\n",
      "and multi-threading to parse HTML content simultaneously, optimizing both\n",
      "fetching and processing times.\n",
      "\n",
      "Here’s a code of this combined approach:\n",
      "\n",
      "    \n",
      "    \n",
      "    import aiohttp\n",
      "    import asyncio\n",
      "    from bs4 import BeautifulSoup\n",
      "    from concurrent.futures import ThreadPoolExecutor\n",
      "    import cProfile\n",
      "    \n",
      "    # List of URLs to fetch data from\n",
      "    urls = [\n",
      "        \"https://news.ycombinator.com/\",\n",
      "        \"https://news.ycombinator.com/?p=2\",\n",
      "        \"https://news.ycombinator.com/?p=3\",\n",
      "    ]\n",
      "    \n",
      "    async def fetch(session, url):\n",
      "        \"\"\"\n",
      "        Asynchronously fetches the content from a given URL using the provided session.\n",
      "        \"\"\"\n",
      "        async with session.get(url) as response:\n",
      "            content = await response.text()\n",
      "            print(f\"Fetched URL: {url}\")\n",
      "            return content\n",
      "    \n",
      "    def parse(html):\n",
      "        soup = BeautifulSoup(html, \"lxml\")\n",
      "    \n",
      "    async def main():\n",
      "        \"\"\"\n",
      "        Main function to fetch URLs concurrently and parse their HTML content.\n",
      "        \"\"\"\n",
      "        async with aiohttp.ClientSession() as session:\n",
      "            # Create a list of tasks for fetching URLs\n",
      "            tasks = [fetch(session, url) for url in urls]\n",
      "            # Gather all responses concurrently\n",
      "            htmls = await asyncio.gather(*tasks)\n",
      "    \n",
      "            # Use ThreadPoolExecutor to parse HTML content in parallel\n",
      "            with ThreadPoolExecutor(max_workers=5) as executor:\n",
      "                executor.map(parse, htmls)\n",
      "    \n",
      "    if __name__ == \"__main__\":\n",
      "        # Profile the asynchronous main function to analyze performance\n",
      "        cProfile.run(\"asyncio.run(main())\")\n",
      "    \n",
      "\n",
      "This combined approach yields 125,608 function calls completed in just 1.642\n",
      "seconds.\n",
      "\n",
      "In this hybrid approach, `aiohttp` and `asyncio` are used to fetch web pages\n",
      "asynchronously, while `ThreadPoolExecutor` handles the HTML parsing\n",
      "concurrently. By combining these methods, you can use the strengths of both\n",
      "asynchronous processing and multi-threading to achieve maximum performance in\n",
      "your web scraping tasks.\n",
      "\n",
      "**Observation?** The combined approach delivered the best performance, with an\n",
      "execution time of just **1.642** seconds, compared to 23.274 seconds for\n",
      "multi-threading alone and 22.383 seconds for pure asynchronous processing.\n",
      "\n",
      "## Tip 4: Caching techniques to speed up scraping\n",
      "\n",
      "Caching is an excellent method to improve the performance of your web scraping\n",
      "activities. By temporarily storing data, caching decreases the frequency of\n",
      "repeated requests to the server. Therefore, when you make the same request\n",
      "again, it retrieves the data from the local cache rather than fetching it from\n",
      "the server, ultimately reducing the load on external resources.\n",
      "\n",
      "One of the easiest ways to add caching to your web scraping is by using the\n",
      "requests-cache , a persistent cache for Python requests.\n",
      "\n",
      "First, install the `requests-cache` library using pip:\n",
      "\n",
      "    \n",
      "    \n",
      "    pip install requests-cache\n",
      "    \n",
      "\n",
      "You can then use `requests_cache.CachedSession` to make your requests. This\n",
      "session behaves like a standard `requests.Session`, but includes caching\n",
      "functionality.\n",
      "\n",
      "Here's a simple code:\n",
      "\n",
      "    \n",
      "    \n",
      "    import requests_cache  # Import the library for caching HTTP requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    import cProfile\n",
      "    \n",
      "    def fetch_data():\n",
      "        # Create a cached session that stores responses for 24 hours (86400 seconds)\n",
      "        session = requests_cache.CachedSession(\"temp_cache\", expire_after=86400)\n",
      "        response = session.get(\"https://news.ycombinator.com/\")\n",
      "    \n",
      "        # Check if the response is from the cache\n",
      "        if response.from_cache:\n",
      "            print(\"Data is coming from cache\")\n",
      "        else:\n",
      "            print(\"Data is coming from server\")\n",
      "        soup = BeautifulSoup(response.content, \"lxml\")\n",
      "        print(soup.title.text)\n",
      "    \n",
      "    # Profile the function call\n",
      "    cProfile.run(\"fetch_data()\")\n",
      "    \n",
      "\n",
      "When you run the script for the first time, data is fetched from the server,\n",
      "as shown below:\n",
      "\n",
      "In this initial request, the operation involves approximately 602,701 function\n",
      "calls and takes about 2.939 seconds.\n",
      "\n",
      "Now, when you run the same script again, the data is retrieved from the cache:\n",
      "\n",
      "Here, the cached response makes about 602,477 function calls and completes in\n",
      "1.333 seconds. This shows an improvement in speed and efficiency due to\n",
      "caching.\n",
      "\n",
      "### Customizing cache\n",
      "\n",
      "`requests-cache` allows you to customize cache behaviour according to your\n",
      "needs. For example, you can choose different backends (e.g., SQLite, Redis),\n",
      "set expiration times, and specify which HTTP methods to cache.\n",
      "\n",
      "Here’s the code\n",
      "\n",
      "    \n",
      "    \n",
      "    import requests_cache\n",
      "    import cProfile\n",
      "    \n",
      "    def fetch_data():\n",
      "        session = requests_cache.CachedSession(\n",
      "            \"temp_cache\",\n",
      "            backend=\"sqlite\",  # use SQLite as the caching backend\n",
      "            expire_after=3600,  # cache expires after 1 hour\n",
      "            allowable_methods=[\"GET\", \"POST\"],  # cache both GET and POST requests\n",
      "        )\n",
      "    \n",
      "        response = session.get(\"https://news.ycombinator.com/\")\n",
      "    \n",
      "        # ...\n",
      "    \n",
      "    cProfile.run(\"fetch_data()\")\n",
      "    \n",
      "\n",
      "You might also want to cache only specific types of responses, such as\n",
      "successful ones. This can be achieved using the `cache_filter` parameter,\n",
      "which allows you to define a custom filtering function:\n",
      "\n",
      "    \n",
      "    \n",
      "    import requests_cache\n",
      "    import cProfile\n",
      "    \n",
      "    def is_successful(response):\n",
      "        \"\"\"Cache only responses with status code 200.\"\"\"\n",
      "        return response.status_code == 200\n",
      "    \n",
      "    def fetch_data():\n",
      "        # Create a cached session with a filter to cache only successful responses\n",
      "        session = requests_cache.CachedSession(\n",
      "            \"temp_cache\", expire_after=86400, cache_filter=is_successful\n",
      "        )\n",
      "    \n",
      "        response = session.get(\"https://news.ycombinator.com/\")\n",
      "    \n",
      "        # ...\n",
      "    \n",
      "    cProfile.run(\"fetch_data()\")\n",
      "    \n",
      "\n",
      "## Tip 5: Using `SoupStrainer` for partial parsing\n",
      "\n",
      "When working with large documents, sometimes you might prefer to parse just a\n",
      "portion rather than the entire content. Beautiful Soup makes this easier with\n",
      "`SoupStrainer`, which lets you filter out the unnecessary parts and focus only\n",
      "on what matters.\n",
      "\n",
      "Here’s the code:\n",
      "\n",
      "    \n",
      "    \n",
      "    import cProfile\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup, SoupStrainer\n",
      "    \n",
      "    # Function to parse HTML content using lxml parser\n",
      "    def parse_html(url):\n",
      "        response = requests.get(url)\n",
      "        only_a_tags = SoupStrainer(\"a\")\n",
      "        soup = BeautifulSoup(response.content, \"lxml\", parse_only=only_a_tags)\n",
      "        for link in soup:\n",
      "            print(link.get(\"href\"))\n",
      "    \n",
      "    # Profile the parse_html function using cProfile\n",
      "    cProfile.run('parse_html(\"https://news.ycombinator.com/\")')\n",
      "    \n",
      "\n",
      "The result is:\n",
      "\n",
      "So, by parsing only the parts of the document you need, you can significantly\n",
      "reduce the time and resources required for parsing.\n",
      "\n",
      "## Tip 6: Optimizing CSS selectors\n",
      "\n",
      "When using Beautiful Soup to extract specific elements from HTML, the\n",
      "efficiency of your CSS selectors plays a crucial role in performance. By\n",
      "narrowing the scope of your selectors, you can significantly reduce parsing\n",
      "time. Instead of using broad selectors that traverse a large portion of the\n",
      "DOM tree, focus on targeting elements directly.\n",
      "\n",
      "    \n",
      "    \n",
      "    # Using efficient CSS selectors\n",
      "    soup.select(\"div.classname > p\")\n",
      "    \n",
      "\n",
      "> 💡 You can also refer to a detailed guide on using CSS selectors for web\n",
      "> scraping .\n",
      "\n",
      "## Tip 7: Using proxy servers for load balancing\n",
      "\n",
      "Proxy servers act as intermediaries between your scraper and the target\n",
      "website. They distribute requests across multiple IP addresses, which helps\n",
      "reduce the risk of being blocked and balances the load of requests. This is\n",
      "particularly useful for scraping websites with rate limits or IP-based\n",
      "restrictions.\n",
      "\n",
      "> 💡 You can also check out the detailed guide on getting started with\n",
      "> BeautifulSoup **.**\n",
      "\n",
      "To use proxies with Beautiful Soup, you can configure the `requests` library\n",
      "to route requests through a proxy server.\n",
      "\n",
      "Here’s the code:\n",
      "\n",
      "    \n",
      "    \n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    \n",
      "    # Define the proxy server\n",
      "    proxies = {\n",
      "        \"http\": \"http://your_proxy_ip:port\",\n",
      "        \"https\": \"http://your_proxy_ip:port\",\n",
      "    }\n",
      "    \n",
      "    # Make a request through the proxy server\n",
      "    response = requests.get(\"https://news.ycombinator.com/\", proxies=proxies)\n",
      "    soup = BeautifulSoup(response.content, \"lxml\")\n",
      "    \n",
      "\n",
      "In the code, requests are routed through the proxy server specified in the\n",
      "`proxies` dictionary. This helps in balancing the request load and improving\n",
      "the performance of Beautiful Soup.\n",
      "\n",
      "## Tip 8: Managing rate limits with sessions and proxies\n",
      "\n",
      "When web scraping, it's important to respect the rate limits set by the target\n",
      "server to avoid getting blocked. You can effectively manage the frequency of\n",
      "your requests by using sessions and proxies. Additionally, introducing a delay\n",
      "between requests can also help.\n",
      "\n",
      "    \n",
      "    \n",
      "    import requests\n",
      "    import time\n",
      "    \n",
      "    # Initialize a session for HTTP requests\n",
      "    web_session = requests.Session()\n",
      "    \n",
      "    # Proxy settings\n",
      "    proxy_settings = {\n",
      "        'http': 'http://your_proxy_ip:port',\n",
      "        'https': 'http://your_proxy_ip:port',\n",
      "    }\n",
      "    \n",
      "    # Apply proxy settings to the session\n",
      "    web_session.proxies.update(proxy_settings)\n",
      "    \n",
      "    def retrieve_web_data(target_url):\n",
      "        # Make a request using the predefined session\n",
      "        result = web_session.get(target_url)\n",
      "    \n",
      "        if result.status_code == 200:\n",
      "            print(\"Successfully retrieved data.\")\n",
      "        else:\n",
      "            print(\"Data retrieval failed.\")\n",
      "    \n",
      "        return result\n",
      "    \n",
      "    # Target URL\n",
      "    target_url = 'https://news.ycombinator.com/'\n",
      "    \n",
      "    # Execute data retrieval with pauses between requests\n",
      "    for _ in range(5):\n",
      "        retrieve_web_data(target_url)\n",
      "        time.sleep(2) # Wait for 1 second between requests\n",
      "    \n",
      "\n",
      "In the code, using `requests.Session` can speed up web scraping by reusing\n",
      "connections. Setting up proxies to rotate IP addresses helps distribute the\n",
      "load and avoid rate limits. Additionally, using `time.sleep()` introduces\n",
      "pauses between requests, which helps you stay within server limits and reduces\n",
      "the risk of getting blocked.\n",
      "\n",
      "## Tip 9: Error handling and retry logic with proxies\n",
      "\n",
      "When scraping the web, using proxies is great, but you also have to manage\n",
      "connection errors and timeouts. Therefore, it's important to use error\n",
      "handling and retry logic to ensure effective web scraping. For example, if a\n",
      "request fails, attempting a retry with a different proxy can significantly\n",
      "improve the likelihood of success.\n",
      "\n",
      "Here’s the code of how to implement retry logic with proxies.\n",
      "\n",
      "    \n",
      "    \n",
      "    import requests\n",
      "    from requests.exceptions import RequestException, ProxyError, Timeout\n",
      "    import random\n",
      "    from bs4 import BeautifulSoup\n",
      "    \n",
      "    proxies_list = [\n",
      "        \"http://your_proxy_ip:port\",\n",
      "        \"http://your_proxy_ip:port\",\n",
      "        \"http://your_proxy_ip:port\",\n",
      "    ]\n",
      "    \n",
      "    def fetch_with_retry(url, retries=3):\n",
      "        session = requests.Session()\n",
      "        for attempt in range(1, retries + 1):\n",
      "            try:\n",
      "                proxy = {\"http\": random.choice(proxies_list)}\n",
      "                response = session.get(url, proxies=proxy, timeout=10)\n",
      "                if response.status_code == 200:\n",
      "                    print(f\"Attempt {attempt} succeeded.\")\n",
      "                    return BeautifulSoup(response.text, \"lxml\")\n",
      "                print(\n",
      "                    f\"Attempt {attempt} failed with status code: {\n",
      "                      response.status_code}\"\n",
      "                )\n",
      "            except (RequestException, ProxyError, Timeout) as e:\n",
      "                print(f\"Attempt {attempt} failed with error: {e}\")\n",
      "                continue\n",
      "        return None\n",
      "    \n",
      "    if __name__ == \"__main__\":\n",
      "        url = \"https://news.ycombinator.com\"\n",
      "        result = fetch_with_retry(url)\n",
      "        if result:\n",
      "            print(\"Fetch successful!\")\n",
      "        else:\n",
      "            print(\"Failed to fetch the URL after several retries.\")\n",
      "    \n",
      "\n",
      "This code tries to fetch data from a URL using different proxy servers. It\n",
      "retries the request up to three times if it fails, picking a new proxy for\n",
      "each attempt. If successful, it parses the HTML content with BeautifulSoup. If\n",
      "not, it continues trying with different proxies until it either succeeds or\n",
      "exhausts all attempts.\n",
      "\n",
      "## Tip 10: Efficient data extraction with XPath\n",
      "\n",
      "Although Beautiful Soup is great for parsing HTML and navigating the document\n",
      "tree, XPath expressions can provide greater precision and flexibility when it\n",
      "comes to complex data extraction. The `lxml` library is particularly powerful\n",
      "because it combines the ease of HTML parsing with the advanced querying\n",
      "capabilities of XPath and XSLT.\n",
      "\n",
      "Before you can start using `lxml`, you'll need to install it:\n",
      "\n",
      "    \n",
      "    \n",
      "    pip install lxml\n",
      "    \n",
      "\n",
      "Here's a quick example of how you can use `lxml` with XPath:\n",
      "\n",
      "    \n",
      "    \n",
      "    import requests\n",
      "    from lxml import html\n",
      "    import cProfile\n",
      "    \n",
      "    def fetch_data():\n",
      "        url = 'https://news.ycombinator.com/'\n",
      "        response = requests.get(url)\n",
      "    \n",
      "        if response.status_code == 200:\n",
      "            # Parse the HTML content into a tree structure\n",
      "            tree = html.fromstring(response.content)\n",
      "            # Use XPath to extract data\n",
      "            titles = tree.xpath(\n",
      "                '//tr[@class=\"athing\"]//span[@class=\"titleline\"]/a/text()')\n",
      "        else:\n",
      "            print(\"Failed to fetch the page. Status code:\", response.status_code)\n",
      "    \n",
      "    cProfile.run('fetch_data()')\n",
      "    \n",
      "\n",
      "The result shows that we successfully extracted all the news titles from the\n",
      "URL in just **1.2** seconds which is quick and efficient.\n",
      "\n",
      "XPath expressions can be more efficient and concise than traditional CSS\n",
      "selectors, especially when dealing with complex document structures. This\n",
      "method can significantly speed up the data extraction process.\n",
      "\n",
      "## Tips for efficient HTML parsing\n",
      "\n",
      "Here are some tips on how to parse HTML more efficiently.\n",
      "\n",
      "**1\\. Navigating the DOM tree:** Understanding the Document Object Model (DOM)\n",
      "is crucial for efficient HTML parsing. The DOM represents the structure of an\n",
      "HTML document as a tree of objects.\n",
      "\n",
      "**2\\. Traversing the DOM:**\n",
      "\n",
      "  * Use `.parent` to access the parent of the tag and `.children` to iterate over a children of the tag. This hierarchical navigation is useful for extracting nested data.\n",
      "  * Use `.next_sibling` and `.previous_sibling` to move horizontally across the DOM tree. This is useful for extracting data from elements that are on the same level.\n",
      "\n",
      "**3\\. Searching the DOM Tree:**\n",
      "\n",
      "Efficient searching within the DOM tree is vital for extracting specific data\n",
      "points. BeautifulSoup offers several methods to facilitate this:\n",
      "\n",
      "  * Use methods such as `find()` and `find_all()` to locate elements by tag name or attributes.\n",
      "  * The `select()` method allows for more complex queries using CSS selectors.\n",
      "\n",
      "**4\\. Handling large documents:**\n",
      "\n",
      "For large HTML documents, performance can be a concern. So, you can follow\n",
      "these quick tips:\n",
      "\n",
      "  * Consider using the `lxml` Parser, which is faster and more efficient for parsing large documents, as discussed earlier in detail.\n",
      "  * Install cchardet as this library speeds up encoding detection, which can be a bottleneck when parsing large files.\n",
      "  * You can also use `SoupStrainer` to limit parsing to only the necessary parts of the document, as discussed earlier in detail.\n",
      "\n",
      "**5\\. Modifying the parse tree:** BeautifulSoup allows you the modification of\n",
      "the parse tree, like the addition, deletion, or alteration of HTML elements.\n",
      "This is particularly useful for cleaning up scraped data or preparing it for\n",
      "further analysis.\n",
      "\n",
      "**6\\. Error handling and logging:** It's important to implement error handling\n",
      "in your code. BeautifulSoup can encounter a lot of issues with malformed HTML\n",
      "or missing tags, which leads to exceptions. Therefore, using `try-except`\n",
      "blocks and logging errors can help in debugging and improving the stability of\n",
      "your scraper.\n",
      "\n",
      "**7\\. Integrating with other tools:** For JavaScript-heavy sites, integrating\n",
      "BeautifulSoup with tools like Selenium , Playwright , or Puppeteer can be very\n",
      "effective. BeautifulSoup can handle static HTML well, but these tools can\n",
      "interact with the browser to scrape dynamic content that JavaScript generates.\n",
      "\n",
      "## Use a Scraping API instead\n",
      "\n",
      "Data extraction is not easy nowadays, as it involves many challenges due to\n",
      "the anti-bot measures put in place. Bypassing anti-bot mechanisms can be\n",
      "challenging and take up a lot of time and resources. That’s where web scraping\n",
      "APIs like **ScrapingBee** come in!\n",
      "\n",
      "ScrapingBee simplifies the scraping process by handling the hard parts like\n",
      "rotating proxies and rendering JavaScript, so you don’t have to worry about\n",
      "getting blocked. You can focus on extracting valuable data without the need to\n",
      "invest time and resources in optimizing BeautifulSoup for performance.\n",
      "\n",
      "To start, sign up for a free ScrapingBee trial no credit card is needed, and\n",
      "you'll receive 1000 credits to begin. Each request costs approximately 25\n",
      "credits.\n",
      "\n",
      "Next, install the ScrapingBee Python client :\n",
      "\n",
      "    \n",
      "    \n",
      "    pip install scrapingbee\n",
      "    \n",
      "\n",
      "You can use the below Python code to begin web scraping:\n",
      "\n",
      "    \n",
      "    \n",
      "    from scrapingbee import ScrapingBeeClient\n",
      "    \n",
      "    client = ScrapingBeeClient(api_key=\"YOUR_API_KEY\")\n",
      "    \n",
      "    response = client.get(\n",
      "        \"https://www.g2.com/products/anaconda/reviews\",\n",
      "        params={\n",
      "            \"stealth_proxy\": True,  # Use stealth proxies for tougher sites\n",
      "            \"block_resources\": True,  # Block images and CSS to speed up loading\n",
      "            \"wait\": \"1500\",  # Milliseconds to wait before capturing data\n",
      "    \n",
      "            # \"device\": \"desktop\",  # Set the device type\n",
      "            # \"country_code\": \"gb\",  # Specify the country code\n",
      "            # Optional screenshot settings:\n",
      "            # \"screenshot\": True,\n",
      "            # \"screenshot_full_page\": True,\n",
      "        },\n",
      "    )\n",
      "    \n",
      "    print(\"Response HTTP Status Code: \", response.status_code)\n",
      "    print(\"Response HTTP Response Body: \", response.text)\n",
      "    \n",
      "\n",
      "The status code **200** indicates that the G2 anti-bot has been bypassed.\n",
      "\n",
      "Using a web scraping API like ScrapingBee saves you from dealing with various\n",
      "anti-scraping measures, making your data collection efficient and less prone\n",
      "to blocks.\n",
      "\n",
      "## Wrapping up\n",
      "\n",
      "In this tutorial we've shown you 10 expert tips and tricks for speeding up\n",
      "your scraping with Beautiful Soup, some of these concepts can be applied to\n",
      "different scraping libraries as well. Scaling your scraping operation can be a\n",
      "mammoth technical challenge to get right in a cost-effective and efficient\n",
      "way, but with these pointers, you should be on the way to becoming a scraping\n",
      "master. If you want to skip all of the technical challenges of scraping, we've\n",
      "done the hard work for you with our web scraping API which will allow you to\n",
      "easily scrape any page with one API call, give it a spin with 1,000 free\n",
      "credits, just sign up and start scraping.\n",
      "\n",
      "**Satyam Tripathi**\n",
      "\n",
      "Satyam is a senior technical writer who is passionate about web scraping,\n",
      "automation, and data engineering. He has delivered over 130 blog posts since\n",
      "2021.  __\n",
      "\n",
      "### You might also like:\n",
      "\n",
      "#### How to bypass error 1005 'access denied, you have been banned' when\n",
      "scraping\n",
      "\n",
      "**Satyam Tripathi**\n",
      "\n",
      "8 min read\n",
      "\n",
      "Learn how to bypass Cloudflare Error 1005 when scraping websites. Discover\n",
      "techniques like residential proxies, headless browsers, and more to avoid IP\n",
      "bans.\n",
      "\n",
      "#### Mapping the Funniest US States on Reddit using AI\n",
      "\n",
      "**Karthik Devan**\n",
      "\n",
      "3 min read\n",
      "\n",
      "Discover which US states joke the most on Reddit using AI. Explore our\n",
      "detailed analysis ranking states by humorous comments and uncover surprising\n",
      "patterns!\n",
      "\n",
      "#### Puppeteer Stealth Tutorial; How to Set Up & Use (+ Working Alternatives)\n",
      "\n",
      "**Satyam Tripathi**\n",
      "\n",
      "13 min read\n",
      "\n",
      "Learn how to use Puppeteer Stealth to bypass anti-scraping measures and avoid\n",
      "detection. This guide offers advanced techniques and alternatives for seamless\n",
      "web scraping.\n",
      "\n",
      "## Tired of getting blocked while scraping the web?\n",
      "\n",
      "ScrapingBee API handles headless browsers and rotates proxies for you.\n",
      "\n",
      "Get access to 1,000 free API credits, no credit card required!\n",
      "\n",
      "Try ScrapingBee for Free\n",
      "\n",
      "ScrapingBee API handles headless browsers and rotates proxies for you.\n",
      "\n",
      "  * __\n",
      "  * __\n",
      "\n",
      "#### Company\n",
      "\n",
      "  * Team\n",
      "  * Company's journey\n",
      "  * Blog\n",
      "  * Rebranding\n",
      "  * Affiliate Program\n",
      "\n",
      "#### Tools\n",
      "\n",
      "  * Curl converter\n",
      "\n",
      "#### Legal\n",
      "\n",
      "  * Terms of Service\n",
      "  * Privacy Policy\n",
      "  * GDPR Compliance\n",
      "  * Data Processing Agreement\n",
      "  * Cookie Policy\n",
      "  * Acceptable Use Policy\n",
      "  * Legal Notices\n",
      "\n",
      "#### Product\n",
      "\n",
      "  * Features\n",
      "  * Pricing\n",
      "  * Status\n",
      "\n",
      "#### How we compare\n",
      "\n",
      "  * Alternative to Crawlera\n",
      "  * Alternative to Luminati\n",
      "  * Alternative to NetNut\n",
      "  * Alternative to ScraperAPI\n",
      "  * Alternatives to ScrapingBee\n",
      "\n",
      "#### No code web scraping\n",
      "\n",
      "  * No code web scraping\n",
      "  * No code competitor monitoring\n",
      "  * How to put scraped website data into Google Sheets\n",
      "  * Send stock prices update to Slack\n",
      "  * Scrape Amazon products' price with no code\n",
      "  * Scrape Amazon products' price with no code\n",
      "  * Extract job listings, details and salaries\n",
      "\n",
      "#### Learning Web Scraping\n",
      "\n",
      "  * Web scraping questions\n",
      "  * A guide to Web Scraping without getting blocked\n",
      "  * Web Scraping Tools\n",
      "  * Best Free Proxies\n",
      "  * Best Mobile proxies\n",
      "  * Web Scraping vs Web Crawling\n",
      "  * Rotating and residential proxies\n",
      "  * Web Scraping with Python\n",
      "  * Web Scraping with PHP\n",
      "  * Web Scraping with Java\n",
      "  * Web Scraping with Ruby\n",
      "  * Web Scraping with NodeJS\n",
      "  * Web Scraping with R\n",
      "  * Web Scraping with C#\n",
      "  * Web Scraping with C++\n",
      "  * Web Scraping with Elixir\n",
      "  * Web Scraping with Perl\n",
      "  * Web Scraping with Rust\n",
      "  * Web Scraping with Go\n",
      "\n",
      " __\n",
      "\n",
      "Copyright © 2025\n",
      "\n",
      "Made in France\n",
      "\n",
      "\n",
      "Theme  Auto Light Dark\n",
      "\n",
      "#### Previous topic\n",
      "\n",
      "Networking and Interprocess Communication\n",
      "\n",
      "#### Next topic\n",
      "\n",
      "Runners\n",
      "\n",
      "### This Page\n",
      "\n",
      "  * Report a Bug\n",
      "  * Show Source \n",
      "\n",
      "### Navigation\n",
      "\n",
      "  * index\n",
      "  * modules |\n",
      "  * next |\n",
      "  * previous |\n",
      "  *   * Python »\n",
      "  *   *   * 3.13.2 Documentation »\n",
      "  * The Python Standard Library »\n",
      "  * Networking and Interprocess Communication »\n",
      "  * `asyncio` — Asynchronous I/O\n",
      "  * | \n",
      "  * Theme  Auto Light Dark |\n",
      "\n",
      "# `asyncio` — Asynchronous I/O¶\n",
      "\n",
      "* * *\n",
      "\n",
      "Hello World!\n",
      "\n",
      "    \n",
      "    \n",
      "    import asyncio\n",
      "    \n",
      "    async def main():\n",
      "        print('Hello ...')\n",
      "        await asyncio.sleep(1)\n",
      "        print('... World!')\n",
      "    \n",
      "    asyncio.run(main())\n",
      "    \n",
      "\n",
      "asyncio is a library to write **concurrent** code using the **async/await**\n",
      "syntax.\n",
      "\n",
      "asyncio is used as a foundation for multiple Python asynchronous frameworks\n",
      "that provide high-performance network and web-servers, database connection\n",
      "libraries, distributed task queues, etc.\n",
      "\n",
      "asyncio is often a perfect fit for IO-bound and high-level **structured**\n",
      "network code.\n",
      "\n",
      "asyncio provides a set of **high-level** APIs to:\n",
      "\n",
      "  * run Python coroutines concurrently and have full control over their execution;\n",
      "\n",
      "  * perform network IO and IPC;\n",
      "\n",
      "  * control subprocesses;\n",
      "\n",
      "  * distribute tasks via queues;\n",
      "\n",
      "  * synchronize concurrent code;\n",
      "\n",
      "Additionally, there are **low-level** APIs for _library and framework\n",
      "developers_ to:\n",
      "\n",
      "  * create and manage event loops, which provide asynchronous APIs for networking, running subprocesses, handling OS signals, etc;\n",
      "\n",
      "  * implement efficient protocols using transports;\n",
      "\n",
      "  * bridge callback-based libraries and code with async/await syntax.\n",
      "\n",
      "Availability: not WASI.\n",
      "\n",
      "This module does not work or is not available on WebAssembly. See WebAssembly\n",
      "platforms for more information.\n",
      "\n",
      "asyncio REPL\n",
      "\n",
      "You can experiment with an `asyncio` concurrent context in the REPL:\n",
      "\n",
      "    \n",
      "    \n",
      "    $ python -m asyncio\n",
      "    asyncio REPL ...\n",
      "    Use \"await\" directly instead of \"asyncio.run()\".\n",
      "    Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
      "    >>> import asyncio\n",
      "    >>> await asyncio.sleep(10, result='hello')\n",
      "    'hello'\n",
      "    \n",
      "\n",
      "Raises an auditing event `cpython.run_stdin` with no arguments.\n",
      "\n",
      "Changed in version 3.12.5: (also 3.11.10, 3.10.15, 3.9.20, and 3.8.20) Emits\n",
      "audit events.\n",
      "\n",
      "Changed in version 3.13: Uses PyREPL if possible, in which case\n",
      "`PYTHONSTARTUP` is also executed. Emits audit events.\n",
      "\n",
      "Reference\n",
      "\n",
      "High-level APIs\n",
      "\n",
      "  * Runners\n",
      "  * Coroutines and Tasks\n",
      "  * Streams\n",
      "  * Synchronization Primitives\n",
      "  * Subprocesses\n",
      "  * Queues\n",
      "  * Exceptions\n",
      "\n",
      "Low-level APIs\n",
      "\n",
      "  * Event Loop\n",
      "  * Futures\n",
      "  * Transports and Protocols\n",
      "  * Policies\n",
      "  * Platform Support\n",
      "  * Extending\n",
      "\n",
      "Guides and Tutorials\n",
      "\n",
      "  * High-level API Index\n",
      "  * Low-level API Index\n",
      "  * Developing with asyncio\n",
      "\n",
      "Note\n",
      "\n",
      "The source code for asyncio can be found in Lib/asyncio/.\n",
      "\n",
      "#### Previous topic\n",
      "\n",
      "Networking and Interprocess Communication\n",
      "\n",
      "#### Next topic\n",
      "\n",
      "Runners\n",
      "\n",
      "### This Page\n",
      "\n",
      "  * Report a Bug\n",
      "  * Show Source \n",
      "\n",
      "«\n",
      "\n",
      "### Navigation\n",
      "\n",
      "  * index\n",
      "  * modules |\n",
      "  * next |\n",
      "  * previous |\n",
      "  *   * Python »\n",
      "  *   *   * 3.13.2 Documentation »\n",
      "  * The Python Standard Library »\n",
      "  * Networking and Interprocess Communication »\n",
      "  * `asyncio` — Asynchronous I/O\n",
      "  * | \n",
      "  * Theme  Auto Light Dark |\n",
      "\n",
      "(C) Copyright  2001-2025, Python Software Foundation.  \n",
      "This page is licensed under the Python Software Foundation License Version 2.  \n",
      "Examples, recipes, and other code in the documentation are additionally\n",
      "licensed under the Zero Clause BSD License.  \n",
      "See History and License for more information.  \n",
      "  \n",
      "The Python Software Foundation is a non-profit corporation. Please donate.  \n",
      "  \n",
      "Last updated on Mar 09, 2025 (07:27 UTC). Found a bug?  \n",
      "Created using Sphinx 8.2.3.\n",
      "\n",
      "\n",
      "__\n",
      "\n",
      "  * Login\n",
      "  * Sign Up\n",
      "\n",
      "  * Pricing\n",
      "  * FAQ\n",
      "  * Blog\n",
      "  * Other Features\n",
      "    * AI Web Scraping\n",
      "    * Screenshots\n",
      "    * Google search API\n",
      "    * Data extraction\n",
      "    * JavaScript scenario\n",
      "    * No code web scraping\n",
      "  * Developers\n",
      "    * Tutorials\n",
      "    * Documentation\n",
      "    * Knowledge Base\n",
      "\n",
      "# How to use asyncio to scrape websites with Python\n",
      "\n",
      "Try ScrapingBee for Free\n",
      "\n",
      "**Alexander M | 01 July 2024 | 10 min read**\n",
      "\n",
      "Table of contents\n",
      "\n",
      "In this article, we'll take a look at how you can use Python and its\n",
      "coroutines, with their `async`/`await` syntax, to efficiently scrape websites,\n",
      "without having to go all-in on threads 🧵 and semaphores 🚦. For this purpose,\n",
      "we'll check out asyncio , along with the asynchronous HTTP library aiohttp .\n",
      "\n",
      "## What is asyncio?\n",
      "\n",
      "asyncio is part of Python's standard library (yay, no additional dependency to\n",
      "manage 🥳) which enables the implementation of concurrency using the same\n",
      "asynchronous patterns you may already know from JavaScript and other\n",
      "languages: `async` and `await`\n",
      "\n",
      "Asynchronous programming is a convenient alternative to Python threads , as it\n",
      "allows you to run tasks in parallel without the need to fully dive into multi-\n",
      "threading, with all the complexities this might involve.\n",
      "\n",
      "When using the asynchronous approach, you program your code in a seemingly\n",
      "good-old synchronous/blocking fashion and just sprinkle mentioned keywords at\n",
      "the relevant spots in your code and the Python runtime will automatically take\n",
      "care of your code being executed concurrently.\n",
      "\n",
      "### Asynchronous Python basics\n",
      "\n",
      "asyncio uses the following three key concepts to provide asynchronicity:\n",
      "\n",
      "  * Coroutines\n",
      "  * Tasks\n",
      "  * Futures\n",
      "\n",
      "**Coroutines** are the basic building blocks and allow you to declare\n",
      "asynchronous functions, which are executed concurrently by asyncio's event\n",
      "loop and provide a _Future_ as response (more on that in a second). A\n",
      "coroutine is declared by prefixing the function declaration with `async` (i.e.\n",
      "`async def my_function():`) and it typically uses `await` itself, to invoke\n",
      "other asynchronous functions.\n",
      "\n",
      "**Tasks** are the components used for scheduling and the actual concurrent\n",
      "execution of coroutines in an asyncio context. They are instantiated with\n",
      "`asyncio.create_task()` and automatically handled by the event loop.\n",
      "\n",
      "**Futures** are the return value of a coroutine and represent the _future_\n",
      "value computed by the coroutine.\n",
      "\n",
      "You can find a full list of technical details at\n",
      "https://docs.python.org/3/library/asyncio-task.html#awaitables .\n",
      "\n",
      "### How does `async`/`await` work?\n",
      "\n",
      "If you happen to be already familiar with `async`/`await` in JavaScript,\n",
      "you'll feel right at home as the underlying concept is the same. While\n",
      "asynchronous programming per se is not something new, this was usually\n",
      "achieved with callbacks and the eventual pyramid of doom with all the nested\n",
      "and chained callbacks. This was pretty unmanageable in both, JavaScript and\n",
      "Python. `async`/`await` came to the rescue here.\n",
      "\n",
      "When you have a task which takes longer to compute (typical example when to\n",
      "use multi-threading), you can mark the function with `async` and turn it into\n",
      "a coroutine. Let's take the following code as quick example.\n",
      "\n",
      "    \n",
      "    \n",
      "    import asyncio\n",
      "    \n",
      "    async def wait_and_print(str):\n",
      "      await asyncio.sleep(1)\n",
      "      print(str)\n",
      "    \n",
      "    async def main():\n",
      "      tasks = []\n",
      "    \n",
      "      for i in range(1, 10):\n",
      "        tasks.append(asyncio.create_task(wait_and_print(i)))\n",
      "    \n",
      "      for task in tasks:\n",
      "        await task\n",
      "    \n",
      "    asyncio.run(main())\n",
      "    \n",
      "\n",
      "Here, we define a function which prints a value and we call the function ten\n",
      "times. Pretty simple, but the catch is the function waits a second before it\n",
      "prints its text. If we called this in a regular, sequential fashion, the whole\n",
      "execution would take ten times one second. However, with coroutines, we use\n",
      "`create_task` to set up a list of tasks, which we subsequently execute all at\n",
      "the same time using the `await` statement. Each function still pauses for one\n",
      "second, but given they all run at the same time, the whole script will have\n",
      "completed after a second. Yet, the code does not utilise any (obvious) multi-\n",
      "threading and looks mostly like traditional single-threaded code.\n",
      "\n",
      "One thing to note is, that we need to encapsulate our top-level code in its\n",
      "own `async` function `main()`, which we then call with `run()`. This starts\n",
      "the event loop and provides us with all the asynchronous goodies. Let's take a\n",
      "quick look at what asyncio provides out-of-the-box!\n",
      "\n",
      "### Asyncio feature/function overview\n",
      "\n",
      "The following table provides a quick overview of the core features and\n",
      "functions of asyncio, which you'll mostly come across when programming\n",
      "asynchronously with asyncio.\n",
      "\n",
      "Feature| Description  \n",
      "---|---  \n",
      "as_completed()| Executes a list of coroutines and returns an iterator object\n",
      "for their results  \n",
      "create_task()| Executes the given coroutine concurrently in the context of a\n",
      "task  \n",
      "ensure_future()| Accepts a list of different parameters types and verifies\n",
      "they are all Future-like objects  \n",
      "gather()| Concurrently executes the passed awaitables using tasks and returns\n",
      "their combined results  \n",
      "get_event_loop()| Provides access to the currently active event loop instance  \n",
      "run()| Executes the given coroutine - typically used for the main function  \n",
      "sleep()| Suspends the current task for the indicated number of seconds - akin\n",
      "to the standard `time.sleep()` function  \n",
      "wait()| Execute a list of awaitables and waits until the condition specified\n",
      "in the `when` parameter is met  \n",
      "wait_for()| Similar to `wait`, but cancels the future when a timeout occurs  \n",
      "Lock()| Provides access to a mutex object  \n",
      "Semaphore()| Provides access to a semaphore object  \n",
      "Task()| The task object, as returned by `create_task()`  \n",
      "  \n",
      "## Scrape Wikipedia asynchronously with Python and asyncio\n",
      "\n",
      "Now, that we have a basic understanding of how asynchronous calls work in\n",
      "Python and the features asyncio provides, let's put our knowledge to use with\n",
      "a real-world scraping example, shall we?\n",
      "\n",
      "The idea of the following example is to compile a list of creators of\n",
      "programming languages. For this purpose, we first crawl Wikipedia for the\n",
      "articles it has on programming languages.\n",
      "\n",
      "With that list, we then scrape these pages in the second step and extract the\n",
      "respective information from the pages' infoboxes. Voilà, that should then get\n",
      "us a list of programming languages with their respective creators. Let's get\n",
      "coding! 👨🏻‍💻\n",
      "\n",
      "### Installing dependencies\n",
      "\n",
      "For starters, let's install the necessary library dependencies using pip:\n",
      "\n",
      "    \n",
      "    \n",
      "    pip install aiohttp\n",
      "    pip install BeautifulSoup\n",
      "    \n",
      "\n",
      "Splendid! We have the basic libraries installed and can continue with getting\n",
      "the links to scrape.\n",
      "\n",
      "### Crawling\n",
      "\n",
      "Create a new file `scraper.py` and save the following code:\n",
      "\n",
      "    \n",
      "    \n",
      "    import asyncio\n",
      "    import aiohttp as aiohttp\n",
      "    from bs4 import BeautifulSoup\n",
      "    from urllib.parse import urljoin\n",
      "    import pprint\n",
      "    \n",
      "    \n",
      "    BASE_URL = 'https://en.wikipedia.org'\n",
      "    \n",
      "    \n",
      "    async def fetch(url):\n",
      "      async with aiohttp.ClientSession() as session:\n",
      "        async with session.get(url) as resp:\n",
      "          return await resp.text()\n",
      "    \n",
      "    \n",
      "    async def crawl():\n",
      "      pages = []\n",
      "    \n",
      "      content = await fetch(urljoin(BASE_URL, '/wiki/List_of_programming_languages'))\n",
      "      soup = BeautifulSoup(content, 'html.parser')\n",
      "      for link in soup.select('div.div-col a'):\n",
      "        pages.append(urljoin(BASE_URL, link['href']))\n",
      "    \n",
      "      return pages\n",
      "    \n",
      "    \n",
      "    async def main():\n",
      "      links = await crawl()\n",
      "    \n",
      "      pp = pprint.PrettyPrinter()\n",
      "    \n",
      "      pp.pprint(links)\n",
      "    \n",
      "    \n",
      "    asyncio.run(main())\n",
      "    \n",
      "\n",
      "Lovely! That code is ready to run, but before we do that, let's take a look at\n",
      "the individual parts we are performing here:\n",
      "\n",
      "  1. The usual imports\n",
      "  2. Then, we declare `BASE_URL` for our base URL\n",
      "  3. Next, we define a basic `fetch()` function handling the asynchronous aiohttp calls and returning the URL's content\n",
      "  4. We also define our central `crawl()` function, which does the crawling and uses the CSS selector `div.div-col a` to get a list of all relevant language pages\n",
      "  5. Lastly, we call our asynchronous `main()` function, using `asyncio.run()`, to run `crawl()` and print the list of the links we found\n",
      "\n",
      "> 💡 **Using CSS selectors with Python**\n",
      ">\n",
      "> If you want to learn more about how to use CSS selectors specifically in\n",
      "> Python, please check out How to use CSS Selectors in Python? .\n",
      "\n",
      "Great so far, but just theory. Let's run it and check if we actually get the\n",
      "links we are after ....\n",
      "\n",
      "    \n",
      "    \n",
      "    $ python3 scraper.py\n",
      "    ['https://en.wikipedia.org/wiki/A_Sharp_(.NET)',\n",
      "     'https://en.wikipedia.org/wiki/A-0_System',\n",
      "     'https://en.wikipedia.org/wiki/A%2B_(programming_language)',\n",
      "     'https://en.wikipedia.org/wiki/ABAP',\n",
      "     'https://en.wikipedia.org/wiki/ABC_(programming_language)',\n",
      "     'https://en.wikipedia.org/wiki/ABC_ALGOL',\n",
      "     'https://en.wikipedia.org/wiki/ACC_(programming_language)',\n",
      "     'https://en.wikipedia.org/wiki/Accent_(programming_language)',\n",
      "    \n",
      "     LOTS MORE\n",
      "    \n",
      "\n",
      "Well, we did, and it was easier than one might think - not lots of boilerplate\n",
      "code (`public static void main(String[] args)`, anyone? ☕).\n",
      "\n",
      "But crawling was only the first step on our journey. It's certainly crucial,\n",
      "because without a list of URLs we won't be able to scrape them, but what we\n",
      "are really interested in is the information of each individual language. So\n",
      "let's continue to the scraping bit of our project!\n",
      "\n",
      "### Scraping\n",
      "\n",
      "All right, so far we managed to get the list of URLs we want to scrape and now\n",
      "we are going to implement the code which will perform the actual scraping.\n",
      "\n",
      "For this, let's start with the core function that implement the scraping logic\n",
      "and add the following asynchronous function to our `scraper.py` file.\n",
      "\n",
      "    \n",
      "    \n",
      "    async def scrape(link):\n",
      "      content = await fetch(link)\n",
      "      soup = BeautifulSoup(content, 'html.parser')\n",
      "    \n",
      "      # Select name\n",
      "      name = soup.select_one('caption.infobox-title')\n",
      "    \n",
      "      if name is not None:\n",
      "        name = name.text\n",
      "    \n",
      "        creator = soup.select_one('table.infobox tr:has(th a:-soup-contains(\"Developer\", \"Designed by\")) td')\n",
      "        if creator is not None:\n",
      "          creator = creator.text\n",
      "    \n",
      "        return [name, creator]\n",
      "    \n",
      "      return []\n",
      "    \n",
      "\n",
      "Once again, quite some manageable piece of code, isn't it? What do we do here\n",
      "in detail, though?\n",
      "\n",
      "  1. First, `scrape()` takes one argument: `link`, the URL to scrape\n",
      "  2. Next, we call our `fetch()` function to get the content of that URL and save it into `content`\n",
      "  3. Now, we instantiate an instance of Beautiful Soup and use it to parse `content` into `soup`\n",
      "  4. We quickly use the CSS selector `caption.infobox-title` to get language name\n",
      "  5. As last step, we use the `:-soup-contains` pseudo-selector to precisely select the table entry with the name of the language author and return everything as array - or an empty array if we did not find the information\n",
      "\n",
      "Now that we have this in place, we just need to pass the links obtained with\n",
      "our crawler to `scrape()` and our scraper is almost ready!\n",
      "\n",
      "Let's quickly adjust the `main()` function as follows:\n",
      "\n",
      "    \n",
      "    \n",
      "    async def main():\n",
      "      links = await crawl()\n",
      "    \n",
      "      tasks = []\n",
      "      for link in links:\n",
      "        tasks.append(scrape(link))\n",
      "      authors = await asyncio.gather(*tasks)\n",
      "    \n",
      "      pp = pprint.PrettyPrinter()\n",
      "      pp.pprint(authors)\n",
      "    \n",
      "\n",
      "We still call `crawl()`, but instead of printing the links, we schedule a\n",
      "`scrape()` call for each link as an asyncio task and add it to the `tasks`\n",
      "queue. The real magic then happens when we pass the queue to `asyncio.gather`\n",
      ", which runs all the tasks asynchronously and in parallel.\n",
      "\n",
      "And here is the full code:\n",
      "\n",
      "    \n",
      "    \n",
      "    import asyncio\n",
      "    import aiohttp as aiohttp\n",
      "    from bs4 import BeautifulSoup\n",
      "    from urllib.parse import urljoin\n",
      "    import pprint\n",
      "    \n",
      "    \n",
      "    BASE_URL = 'https://en.wikipedia.org'\n",
      "    \n",
      "    \n",
      "    async def fetch(url):\n",
      "      async with aiohttp.ClientSession() as session:\n",
      "        async with session.get(url) as resp:\n",
      "          return await resp.text()\n",
      "    \n",
      "    \n",
      "    async def crawl():\n",
      "      pages = []\n",
      "    \n",
      "      content = await fetch(urljoin(BASE_URL, '/wiki/List_of_programming_languages'))\n",
      "      soup = BeautifulSoup(content, 'html.parser')\n",
      "      for link in soup.select('div.div-col a'):\n",
      "        pages.append(urljoin(BASE_URL, link['href']))\n",
      "    \n",
      "      return pages\n",
      "    \n",
      "    \n",
      "    async def scrape(link):\n",
      "      content = await fetch(link)\n",
      "      soup = BeautifulSoup(content, 'html.parser')\n",
      "    \n",
      "      # Select name\n",
      "      name = soup.select_one('caption.infobox-title')\n",
      "    \n",
      "      if name is not None:\n",
      "        name = name.text\n",
      "    \n",
      "        creator = soup.select_one('table.infobox tr:has(th a:-soup-contains(\"Developer\", \"Designed by\")) td')\n",
      "        if creator is not None:\n",
      "          creator = creator.text\n",
      "    \n",
      "        return [name, creator]\n",
      "    \n",
      "      return []\n",
      "    \n",
      "    \n",
      "    async def main():\n",
      "      links = await crawl()\n",
      "    \n",
      "      tasks = []\n",
      "      for link in links:\n",
      "        tasks.append(scrape(link))\n",
      "      authors = await asyncio.gather(*tasks)\n",
      "    \n",
      "      pp = pprint.PrettyPrinter()\n",
      "      pp.pprint(authors)\n",
      "    \n",
      "    \n",
      "    asyncio.run(main())\n",
      "    \n",
      "\n",
      "> 💡 Love web scraping in Python? Check out our expert list of the Best Python\n",
      "> web scraping libraries.\n",
      "\n",
      "## Summary\n",
      "\n",
      "What we learned in this article is that Python provides an excellent\n",
      "environment for running concurrent tasks without the need to implement full\n",
      "multi-threading. Its asynchronous `async`/`await` syntax enables you to\n",
      "implement your scraping logic in a straightforward, blocking fashion and,\n",
      "nonetheless, run an efficient scraping pipeline and fully utilise the\n",
      "available CPU cores.\n",
      "\n",
      "Our examples provide a good initial overview on how to approach async\n",
      "programming in Python, but there are still a few factors to take into account\n",
      "to make sure your scraper is successful\n",
      "\n",
      "  * the user-agent - you can simply pass a `headers` dictionary as argument to `session.get` and indicate the desired user-agent\n",
      "  * request throttling - make sure you are not overwhelming the server and send your requests with reasonable delays\n",
      "  * IP addresses - some sites may be limited to certain geographical regions or import restrictions on concurrent or total requests from one, single IP address\n",
      "  * JavaScript - some sites (especially SPAs) make heavy use of JavaScript and you need a proper JavaScript engine to support your scraping\n",
      "\n",
      "If you wish to find out more on these issues, please drop by our other article\n",
      "on this very subject: Web Scraping without getting blocked\n",
      "\n",
      "If you don't feel like having to deal with all the scraping bureaucracy of IP\n",
      "address rotation, user-agents, geo-fencing, browser management for JavaScript\n",
      "and rather want to focus on the data extraction and analysis, then please feel\n",
      "free to take a look at our specialised web scraping API . The platform handles\n",
      "all these issues on its own and comes with proxy support, a full-fledged\n",
      "JavaScript environment, and straightforward scraping rules using CSS selectors\n",
      "and XPath expressions . Registering an account is absolutely free and comes\n",
      "with the first 1,000 scraping requests on the house - plenty of room to\n",
      "discover how ScrapingBee can help you with your projects.\n",
      "\n",
      "Happy asynchronous scraping with Python!\n",
      "\n",
      "**Alexander M**\n",
      "\n",
      "Alexander is a software engineer and technical writer with a passion for\n",
      "everything network related.\n",
      "\n",
      "### You might also like:\n",
      "\n",
      "#### Web Scraping Tutorial Using Selenium & Python (+ examples)\n",
      "\n",
      "**Ilya Krukowski**\n",
      "\n",
      "31 min read\n",
      "\n",
      "Lean how to scrape the web with Selenium and Python with this step by step\n",
      "tutorial. We will use Selenium to automate Hacker News login.\n",
      "\n",
      "#### How to Use a Proxy with Python Requests?\n",
      "\n",
      "**Maxine Meurer**\n",
      "\n",
      "9 min read\n",
      "\n",
      "In this tutorial we will see how to use a proxy with the Requests package. We\n",
      "will also discuss on how to choose the right proxy provider.\n",
      "\n",
      "#### Study of Amazon’s Best Selling & Most Read Book Charts Since 2017\n",
      "\n",
      "**Karthik Devan**\n",
      "\n",
      "31 min read\n",
      "\n",
      "Discover the most popular books on Amazon! (since 2017) Our in-depth analysis\n",
      "reveals the top-selling and most-read titles, scraped directly from Amazon\n",
      "data.\n",
      "\n",
      "## Tired of getting blocked while scraping the web?\n",
      "\n",
      "ScrapingBee API handles headless browsers and rotates proxies for you.\n",
      "\n",
      "Get access to 1,000 free API credits, no credit card required!\n",
      "\n",
      "Try ScrapingBee for Free\n",
      "\n",
      "ScrapingBee API handles headless browsers and rotates proxies for you.\n",
      "\n",
      "  * __\n",
      "  * __\n",
      "\n",
      "#### Company\n",
      "\n",
      "  * Team\n",
      "  * Company's journey\n",
      "  * Blog\n",
      "  * Rebranding\n",
      "  * Affiliate Program\n",
      "\n",
      "#### Tools\n",
      "\n",
      "  * Curl converter\n",
      "\n",
      "#### Legal\n",
      "\n",
      "  * Terms of Service\n",
      "  * Privacy Policy\n",
      "  * GDPR Compliance\n",
      "  * Data Processing Agreement\n",
      "  * Cookie Policy\n",
      "  * Acceptable Use Policy\n",
      "  * Legal Notices\n",
      "\n",
      "#### Product\n",
      "\n",
      "  * Features\n",
      "  * Pricing\n",
      "  * Status\n",
      "\n",
      "#### How we compare\n",
      "\n",
      "  * Alternative to Crawlera\n",
      "  * Alternative to Luminati\n",
      "  * Alternative to NetNut\n",
      "  * Alternative to ScraperAPI\n",
      "  * Alternatives to ScrapingBee\n",
      "\n",
      "#### No code web scraping\n",
      "\n",
      "  * No code web scraping\n",
      "  * No code competitor monitoring\n",
      "  * How to put scraped website data into Google Sheets\n",
      "  * Send stock prices update to Slack\n",
      "  * Scrape Amazon products' price with no code\n",
      "  * Scrape Amazon products' price with no code\n",
      "  * Extract job listings, details and salaries\n",
      "\n",
      "#### Learning Web Scraping\n",
      "\n",
      "  * Web scraping questions\n",
      "  * A guide to Web Scraping without getting blocked\n",
      "  * Web Scraping Tools\n",
      "  * Best Free Proxies\n",
      "  * Best Mobile proxies\n",
      "  * Web Scraping vs Web Crawling\n",
      "  * Rotating and residential proxies\n",
      "  * Web Scraping with Python\n",
      "  * Web Scraping with PHP\n",
      "  * Web Scraping with Java\n",
      "  * Web Scraping with Ruby\n",
      "  * Web Scraping with NodeJS\n",
      "  * Web Scraping with R\n",
      "  * Web Scraping with C#\n",
      "  * Web Scraping with C++\n",
      "  * Web Scraping with Elixir\n",
      "  * Web Scraping with Perl\n",
      "  * Web Scraping with Rust\n",
      "  * Web Scraping with Go\n",
      "\n",
      " __\n",
      "\n",
      "Copyright © 2025\n",
      "\n",
      "Made in France\n",
      "\n",
      "\n",
      "Skip to content\n",
      "\n",
      "## Navigation Menu\n",
      "\n",
      "Toggle navigation\n",
      "\n",
      "Sign in\n",
      "\n",
      "  * Product \n",
      "\n",
      "    * GitHub Copilot\n",
      "\n",
      "Write better code with AI\n",
      "\n",
      "    * Security\n",
      "\n",
      "Find and fix vulnerabilities\n",
      "\n",
      "    * Actions\n",
      "\n",
      "Automate any workflow\n",
      "\n",
      "    * Codespaces\n",
      "\n",
      "Instant dev environments\n",
      "\n",
      "    * Issues\n",
      "\n",
      "Plan and track work\n",
      "\n",
      "    * Code Review\n",
      "\n",
      "Manage code changes\n",
      "\n",
      "    * Discussions\n",
      "\n",
      "Collaborate outside of code\n",
      "\n",
      "    * Code Search\n",
      "\n",
      "Find more, search less\n",
      "\n",
      "Explore\n",
      "\n",
      "    * All features \n",
      "    * Documentation \n",
      "    * GitHub Skills \n",
      "    * Blog \n",
      "\n",
      "  * Solutions \n",
      "\n",
      "By company size\n",
      "\n",
      "    * Enterprises \n",
      "    * Small and medium teams \n",
      "    * Startups \n",
      "    * Nonprofits \n",
      "\n",
      "By use case\n",
      "\n",
      "    * DevSecOps \n",
      "    * DevOps \n",
      "    * CI/CD \n",
      "    * View all use cases \n",
      "\n",
      "By industry\n",
      "\n",
      "    * Healthcare \n",
      "    * Financial services \n",
      "    * Manufacturing \n",
      "    * Government \n",
      "    * View all industries \n",
      "\n",
      "View all solutions\n",
      "\n",
      "  * Resources \n",
      "\n",
      "Topics\n",
      "\n",
      "    * AI \n",
      "    * DevOps \n",
      "    * Security \n",
      "    * Software Development \n",
      "    * View all \n",
      "\n",
      "Explore\n",
      "\n",
      "    * Learning Pathways \n",
      "    * Events & Webinars \n",
      "    * Ebooks & Whitepapers \n",
      "    * Customer Stories \n",
      "    * Partners \n",
      "    * Executive Insights \n",
      "\n",
      "  * Open Source \n",
      "\n",
      "    * GitHub Sponsors\n",
      "\n",
      "Fund open source developers\n",
      "\n",
      "    * The ReadME Project\n",
      "\n",
      "GitHub community articles\n",
      "\n",
      "Repositories\n",
      "\n",
      "    * Topics \n",
      "    * Trending \n",
      "    * Collections \n",
      "\n",
      "  * Enterprise \n",
      "\n",
      "    * Enterprise platform\n",
      "\n",
      "AI-powered developer platform\n",
      "\n",
      "Available add-ons\n",
      "\n",
      "    * Advanced Security\n",
      "\n",
      "Enterprise-grade security features\n",
      "\n",
      "    * Copilot for business\n",
      "\n",
      "Enterprise-grade AI features\n",
      "\n",
      "    * Premium Support\n",
      "\n",
      "Enterprise-grade 24/7 support\n",
      "\n",
      "  * Pricing\n",
      "\n",
      "Search or jump to...\n",
      "\n",
      "# Search code, repositories, users, issues, pull requests...\n",
      "\n",
      "Search\n",
      "\n",
      "Clear\n",
      "\n",
      "Search syntax tips\n",
      "\n",
      "#  Provide feedback\n",
      "\n",
      "We read every piece of feedback, and take your input very seriously.\n",
      "\n",
      "Include my email address so I can be contacted\n",
      "\n",
      "Cancel  Submit feedback\n",
      "\n",
      "#  Saved searches\n",
      "\n",
      "## Use saved searches to filter your results more quickly\n",
      "\n",
      "Name\n",
      "\n",
      "Query\n",
      "\n",
      "To see all available qualifiers, see our documentation.\n",
      "\n",
      "Cancel  Create saved search\n",
      "\n",
      "Sign in\n",
      "\n",
      "Sign up  Reseting focus\n",
      "\n",
      "You signed in with another tab or window. Reload to refresh your session. You\n",
      "signed out in another tab or window. Reload to refresh your session. You\n",
      "switched accounts on another tab or window. Reload to refresh your session.\n",
      "Dismiss alert\n",
      "\n",
      "{{ message }}\n",
      "\n",
      "psf  / **requests-html ** Public\n",
      "\n",
      "  * Notifications  You must be signed in to change notification settings\n",
      "  * Fork 986\n",
      "  * Star  13.8k\n",
      "\n",
      "Pythonic HTML Parsing for Humans™\n",
      "\n",
      "html.python-requests.org\n",
      "\n",
      "### License\n",
      "\n",
      "MIT license\n",
      "\n",
      "13.8k stars  986 forks  Branches Tags Activity\n",
      "\n",
      "Star\n",
      "\n",
      "Notifications  You must be signed in to change notification settings\n",
      "\n",
      "  * Code\n",
      "  * Issues 195\n",
      "  * Pull requests 39\n",
      "  * Actions\n",
      "  * Projects 0\n",
      "  * Security\n",
      "  * Insights\n",
      "\n",
      "Additional navigation options\n",
      "\n",
      "  * Code \n",
      "  * Issues \n",
      "  * Pull requests \n",
      "  * Actions \n",
      "  * Projects \n",
      "  * Security \n",
      "  * Insights \n",
      "\n",
      "# psf/requests-html\n",
      "\n",
      "master\n",
      "\n",
      "BranchesTags\n",
      "\n",
      "Go to file\n",
      "\n",
      "Code\n",
      "\n",
      "## Folders and files\n",
      "\n",
      "Name| Name| Last commit message| Last commit date  \n",
      "---|---|---|---  \n",
      "  \n",
      "## Latest commit\n",
      "\n",
      "## History\n",
      "\n",
      "462 Commits  \n",
      "docs| docs|  |   \n",
      "ext| ext|  |   \n",
      "tests| tests|  |   \n",
      ".gitattributes| .gitattributes|  |   \n",
      ".gitignore| .gitignore|  |   \n",
      ".travis.yml| .travis.yml|  |   \n",
      "LICENSE| LICENSE|  |   \n",
      "Makefile| Makefile|  |   \n",
      "Pipfile| Pipfile|  |   \n",
      "Pipfile.lock| Pipfile.lock|  |   \n",
      "README.rst| README.rst|  |   \n",
      "pytest.ini| pytest.ini|  |   \n",
      "requests_html.py| requests_html.py|  |   \n",
      "setup.py| setup.py|  |   \n",
      "View all files  \n",
      "  \n",
      "## Repository files navigation\n",
      "\n",
      "  * README\n",
      "  * Code of conduct\n",
      "  * MIT license\n",
      "\n",
      "## Requests-HTML: HTML Parsing for Humans™\n",
      "\n",
      "This library intends to make parsing HTML (e.g. scraping the web) as simple\n",
      "and intuitive as possible.\n",
      "\n",
      "When using this library you automatically get:\n",
      "\n",
      "  * **Full JavaScript support**! (Using Chromium, thanks to pyppeteer)\n",
      "  * _CSS Selectors_ (a.k.a jQuery-style, thanks to PyQuery).\n",
      "  * _XPath Selectors_ , for the faint of heart.\n",
      "  * Mocked user-agent (like a real web browser).\n",
      "  * Automatic following of redirects.\n",
      "  * Connection–pooling and cookie persistence.\n",
      "  * The Requests experience you know and love, with magical parsing abilities.\n",
      "  * **Async Support**\n",
      "\n",
      "## Tutorial & Usage\n",
      "\n",
      "Make a GET request to 'python.org', using Requests:\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> from requests_html import HTMLSession\n",
      "    >>> session = HTMLSession()\n",
      "    >>> r = session.get('https://python.org/')\n",
      "\n",
      "Try async and get some sites at the same time:\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> from requests_html import AsyncHTMLSession\n",
      "    >>> asession = AsyncHTMLSession()\n",
      "    >>> async def get_pythonorg():\n",
      "    ...     r = await asession.get('https://python.org/')\n",
      "    ...     return r\n",
      "    ...\n",
      "    >>> async def get_reddit():\n",
      "    ...    r = await asession.get('https://reddit.com/')\n",
      "    ...    return r\n",
      "    ...\n",
      "    >>> async def get_google():\n",
      "    ...    r = await asession.get('https://google.com/')\n",
      "    ...    return r\n",
      "    ...\n",
      "    >>> results = asession.run(get_pythonorg, get_reddit, get_google)\n",
      "    >>> results # check the requests all returned a 200 (success) code\n",
      "    [<Response [200]>, <Response [200]>, <Response [200]>]\n",
      "    >>> # Each item in the results list is a response object and can be interacted with as such\n",
      "    >>> for result in results:\n",
      "    ...     print(result.html.url)\n",
      "    ...\n",
      "    https://www.python.org/\n",
      "    https://www.google.com/\n",
      "    https://www.reddit.com/\n",
      "\n",
      "Note that the order of the objects in the results list represents the order\n",
      "they were returned in, not the order that the coroutines are passed to the\n",
      "`run` method, which is shown in the example by the order being different.\n",
      "\n",
      "Grab a list of all links on the page, as–is (anchors excluded):\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> r.html.links\n",
      "    {'//docs.python.org/3/tutorial/', '/about/apps/', 'https://github.com/python/pythondotorg/issues', '/accounts/login/', '/dev/peps/', '/about/legal/', '//docs.python.org/3/tutorial/introduction.html#lists', '/download/alternatives', 'http://feedproxy.google.com/~r/PythonInsider/~3/kihd2DW98YY/python-370a4-is-available-for-testing.html', '/download/other/', '/downloads/windows/', 'https://mail.python.org/mailman/listinfo/python-dev', '/doc/av', 'https://devguide.python.org/', '/about/success/#engineering', 'https://wiki.python.org/moin/PythonEventsCalendar#Submitting_an_Event', 'https://www.openstack.org', '/about/gettingstarted/', 'http://feedproxy.google.com/~r/PythonInsider/~3/AMoBel8b8Mc/python-3.html', '/success-stories/industrial-light-magic-runs-python/', 'http://docs.python.org/3/tutorial/introduction.html#using-python-as-a-calculator', '/', 'http://pyfound.blogspot.com/', '/events/python-events/past/', '/downloads/release/python-2714/', 'https://wiki.python.org/moin/PythonBooks', 'http://plus.google.com/+Python', 'https://wiki.python.org/moin/', 'https://status.python.org/', '/community/workshops/', '/community/lists/', 'http://buildbot.net/', '/community/awards', 'http://twitter.com/ThePSF', 'https://docs.python.org/3/license.html', '/psf/donations/', 'http://wiki.python.org/moin/Languages', '/dev/', '/events/python-user-group/', 'https://wiki.qt.io/PySide', '/community/sigs/', 'https://wiki.gnome.org/Projects/PyGObject', 'http://www.ansible.com', 'http://www.saltstack.com', 'http://planetpython.org/', '/events/python-events', '/about/help/', '/events/python-user-group/past/', '/about/success/', '/psf-landing/', '/about/apps', '/about/', 'http://www.wxpython.org/', '/events/python-user-group/665/', 'https://www.python.org/psf/codeofconduct/', '/dev/peps/peps.rss', '/downloads/source/', '/psf/sponsorship/sponsors/', 'http://bottlepy.org', 'http://roundup.sourceforge.net/', 'http://pandas.pydata.org/', 'http://brochure.getpython.info/', 'https://bugs.python.org/', '/community/merchandise/', 'http://tornadoweb.org', '/events/python-user-group/650/', 'http://flask.pocoo.org/', '/downloads/release/python-364/', '/events/python-user-group/660/', '/events/python-user-group/638/', '/psf/', '/doc/', 'http://blog.python.org', '/events/python-events/604/', '/about/success/#government', 'http://python.org/dev/peps/', 'https://docs.python.org', 'http://feedproxy.google.com/~r/PythonInsider/~3/zVC80sq9s00/python-364-is-now-available.html', '/users/membership/', '/about/success/#arts', 'https://wiki.python.org/moin/Python2orPython3', '/downloads/', '/jobs/', 'http://trac.edgewall.org/', 'http://feedproxy.google.com/~r/PythonInsider/~3/wh73_1A-N7Q/python-355rc1-and-python-348rc1-are-now.html', '/privacy/', 'https://pypi.python.org/', 'http://www.riverbankcomputing.co.uk/software/pyqt/intro', 'http://www.scipy.org', '/community/forums/', '/about/success/#scientific', '/about/success/#software-development', '/shell/', '/accounts/signup/', 'http://www.facebook.com/pythonlang?fref=ts', '/community/', 'https://kivy.org/', '/about/quotes/', 'http://www.web2py.com/', '/community/logos/', '/community/diversity/', '/events/calendars/', 'https://wiki.python.org/moin/BeginnersGuide', '/success-stories/', '/doc/essays/', '/dev/core-mentorship/', 'http://ipython.org', '/events/', '//docs.python.org/3/tutorial/controlflow.html', '/about/success/#education', '/blogs/', '/community/irc/', 'http://pycon.blogspot.com/', '//jobs.python.org', 'http://www.pylonsproject.org/', 'http://www.djangoproject.com/', '/downloads/mac-osx/', '/about/success/#business', 'http://feedproxy.google.com/~r/PythonInsider/~3/x_c9D0S-4C4/python-370b1-is-now-available-for.html', 'http://wiki.python.org/moin/TkInter', 'https://docs.python.org/faq/', '//docs.python.org/3/tutorial/controlflow.html#defining-functions'}\n",
      "\n",
      "Grab a list of all links on the page, in absolute form (anchors excluded):\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> r.html.absolute_links\n",
      "    {'https://github.com/python/pythondotorg/issues', 'https://docs.python.org/3/tutorial/', 'https://www.python.org/about/success/', 'http://feedproxy.google.com/~r/PythonInsider/~3/kihd2DW98YY/python-370a4-is-available-for-testing.html', 'https://www.python.org/dev/peps/', 'https://mail.python.org/mailman/listinfo/python-dev', 'https://www.python.org/doc/', 'https://www.python.org/', 'https://www.python.org/about/', 'https://www.python.org/events/python-events/past/', 'https://devguide.python.org/', 'https://wiki.python.org/moin/PythonEventsCalendar#Submitting_an_Event', 'https://www.openstack.org', 'http://feedproxy.google.com/~r/PythonInsider/~3/AMoBel8b8Mc/python-3.html', 'https://docs.python.org/3/tutorial/introduction.html#lists', 'http://docs.python.org/3/tutorial/introduction.html#using-python-as-a-calculator', 'http://pyfound.blogspot.com/', 'https://wiki.python.org/moin/PythonBooks', 'http://plus.google.com/+Python', 'https://wiki.python.org/moin/', 'https://www.python.org/events/python-events', 'https://status.python.org/', 'https://www.python.org/about/apps', 'https://www.python.org/downloads/release/python-2714/', 'https://www.python.org/psf/donations/', 'http://buildbot.net/', 'http://twitter.com/ThePSF', 'https://docs.python.org/3/license.html', 'http://wiki.python.org/moin/Languages', 'https://docs.python.org/faq/', 'https://jobs.python.org', 'https://www.python.org/about/success/#software-development', 'https://www.python.org/about/success/#education', 'https://www.python.org/community/logos/', 'https://www.python.org/doc/av', 'https://wiki.qt.io/PySide', 'https://www.python.org/events/python-user-group/660/', 'https://wiki.gnome.org/Projects/PyGObject', 'http://www.ansible.com', 'http://www.saltstack.com', 'https://www.python.org/dev/peps/peps.rss', 'http://planetpython.org/', 'https://www.python.org/events/python-user-group/past/', 'https://docs.python.org/3/tutorial/controlflow.html#defining-functions', 'https://www.python.org/community/diversity/', 'https://docs.python.org/3/tutorial/controlflow.html', 'https://www.python.org/community/awards', 'https://www.python.org/events/python-user-group/638/', 'https://www.python.org/about/legal/', 'https://www.python.org/dev/', 'https://www.python.org/download/alternatives', 'https://www.python.org/downloads/', 'https://www.python.org/community/lists/', 'http://www.wxpython.org/', 'https://www.python.org/about/success/#government', 'https://www.python.org/psf/', 'https://www.python.org/psf/codeofconduct/', 'http://bottlepy.org', 'http://roundup.sourceforge.net/', 'http://pandas.pydata.org/', 'http://brochure.getpython.info/', 'https://www.python.org/downloads/source/', 'https://bugs.python.org/', 'https://www.python.org/downloads/mac-osx/', 'https://www.python.org/about/help/', 'http://tornadoweb.org', 'http://flask.pocoo.org/', 'https://www.python.org/users/membership/', 'http://blog.python.org', 'https://www.python.org/privacy/', 'https://www.python.org/about/gettingstarted/', 'http://python.org/dev/peps/', 'https://www.python.org/about/apps/', 'https://docs.python.org', 'https://www.python.org/success-stories/', 'https://www.python.org/community/forums/', 'http://feedproxy.google.com/~r/PythonInsider/~3/zVC80sq9s00/python-364-is-now-available.html', 'https://www.python.org/community/merchandise/', 'https://www.python.org/about/success/#arts', 'https://wiki.python.org/moin/Python2orPython3', 'http://trac.edgewall.org/', 'http://feedproxy.google.com/~r/PythonInsider/~3/wh73_1A-N7Q/python-355rc1-and-python-348rc1-are-now.html', 'https://pypi.python.org/', 'https://www.python.org/events/python-user-group/650/', 'http://www.riverbankcomputing.co.uk/software/pyqt/intro', 'https://www.python.org/about/quotes/', 'https://www.python.org/downloads/windows/', 'https://www.python.org/events/calendars/', 'http://www.scipy.org', 'https://www.python.org/community/workshops/', 'https://www.python.org/blogs/', 'https://www.python.org/accounts/signup/', 'https://www.python.org/events/', 'https://kivy.org/', 'http://www.facebook.com/pythonlang?fref=ts', 'http://www.web2py.com/', 'https://www.python.org/psf/sponsorship/sponsors/', 'https://www.python.org/community/', 'https://www.python.org/download/other/', 'https://www.python.org/psf-landing/', 'https://www.python.org/events/python-user-group/665/', 'https://wiki.python.org/moin/BeginnersGuide', 'https://www.python.org/accounts/login/', 'https://www.python.org/downloads/release/python-364/', 'https://www.python.org/dev/core-mentorship/', 'https://www.python.org/about/success/#business', 'https://www.python.org/community/sigs/', 'https://www.python.org/events/python-user-group/', 'http://ipython.org', 'https://www.python.org/shell/', 'https://www.python.org/community/irc/', 'https://www.python.org/about/success/#engineering', 'http://www.pylonsproject.org/', 'http://pycon.blogspot.com/', 'https://www.python.org/about/success/#scientific', 'https://www.python.org/doc/essays/', 'http://www.djangoproject.com/', 'https://www.python.org/success-stories/industrial-light-magic-runs-python/', 'http://feedproxy.google.com/~r/PythonInsider/~3/x_c9D0S-4C4/python-370b1-is-now-available-for.html', 'http://wiki.python.org/moin/TkInter', 'https://www.python.org/jobs/', 'https://www.python.org/events/python-events/604/'}\n",
      "\n",
      "Select an element with a CSS Selector:\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> about = r.html.find('#about', first=True)\n",
      "\n",
      "Grab an element's text contents:\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> print(about.text)\n",
      "    About\n",
      "    Applications\n",
      "    Quotes\n",
      "    Getting Started\n",
      "    Help\n",
      "    Python Brochure\n",
      "\n",
      "Introspect an Element's attributes:\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> about.attrs\n",
      "    {'id': 'about', 'class': ('tier-1', 'element-1'), 'aria-haspopup': 'true'}\n",
      "\n",
      "Render out an Element's HTML:\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> about.html\n",
      "    '<li aria-haspopup=\"true\" class=\"tier-1 element-1 \" id=\"about\">\\n<a class=\"\" href=\"/about/\" title=\"\">About</a>\\n<ul aria-hidden=\"true\" class=\"subnav menu\" role=\"menu\">\\n<li class=\"tier-2 element-1\" role=\"treeitem\"><a href=\"/about/apps/\" title=\"\">Applications</a></li>\\n<li class=\"tier-2 element-2\" role=\"treeitem\"><a href=\"/about/quotes/\" title=\"\">Quotes</a></li>\\n<li class=\"tier-2 element-3\" role=\"treeitem\"><a href=\"/about/gettingstarted/\" title=\"\">Getting Started</a></li>\\n<li class=\"tier-2 element-4\" role=\"treeitem\"><a href=\"/about/help/\" title=\"\">Help</a></li>\\n<li class=\"tier-2 element-5\" role=\"treeitem\"><a href=\"http://brochure.getpython.info/\" title=\"\">Python Brochure</a></li>\\n</ul>\\n</li>'\n",
      "\n",
      "Select Elements within Elements:\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> about.find('a')\n",
      "    [<Element 'a' href='/about/' title='' class=''>, <Element 'a' href='/about/apps/' title=''>, <Element 'a' href='/about/quotes/' title=''>, <Element 'a' href='/about/gettingstarted/' title=''>, <Element 'a' href='/about/help/' title=''>, <Element 'a' href='http://brochure.getpython.info/' title=''>]\n",
      "\n",
      "Search for links within an element:\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> about.absolute_links\n",
      "    {'http://brochure.getpython.info/', 'https://www.python.org/about/gettingstarted/', 'https://www.python.org/about/', 'https://www.python.org/about/quotes/', 'https://www.python.org/about/help/', 'https://www.python.org/about/apps/'}\n",
      "\n",
      "Search for text on the page:\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> r.html.search('Python is a {} language')[0]\n",
      "    programming\n",
      "\n",
      "More complex CSS Selector example (copied from Chrome dev tools):\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> r = session.get('https://github.com/')\n",
      "    >>> sel = 'body > div.application-main > div.jumbotron.jumbotron-codelines > div > div > div.col-md-7.text-center.text-md-left > p'\n",
      "    >>> print(r.html.find(sel, first=True).text)\n",
      "    GitHub is a development platform inspired by the way you work. From open source to business, you can host and review code, manage projects, and build software alongside millions of other developers.\n",
      "\n",
      "XPath is also supported:\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> r.html.xpath('/html/body/div[1]/a')\n",
      "    [<Element 'a' class=('px-2', 'py-4', 'show-on-focus', 'js-skip-to-content') href='#start-of-content' tabindex='1'>]\n",
      "\n",
      "## JavaScript Support\n",
      "\n",
      "Let's grab some text that's rendered by JavaScript. Until 2020, the Python 2.7\n",
      "countdown clock (https://pythonclock.org) will serve as a good test page:\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> r = session.get('https://pythonclock.org')\n",
      "\n",
      "Let's try and see the dynamically rendered code (The countdown clock). To do\n",
      "that quickly at first, we'll search between the last text we see before it\n",
      "('Python 2.7 will retire in...') and the first text we see after it ('Enable\n",
      "Guido Mode').\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> r.html.search('Python 2.7 will retire in...{}Enable Guido Mode')[0]\n",
      "    '</h1>\\n        </div>\\n        <div class=\"python-27-clock\"></div>\\n        <div class=\"center\">\\n            <div class=\"guido-button-block\">\\n                <button class=\"js-guido-mode guido-button\">'\n",
      "\n",
      "Notice the clock is missing. The `render()` method takes the response and\n",
      "renders the dynamic content just like a web browser would.\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> r.html.render()\n",
      "    >>> r.html.search('Python 2.7 will retire in...{}Enable Guido Mode')[0]\n",
      "    '</h1>\\n        </div>\\n        <div class=\"python-27-clock is-countdown\"><span class=\"countdown-row countdown-show6\"><span class=\"countdown-section\"><span class=\"countdown-amount\">1</span><span class=\"countdown-period\">Year</span></span><span class=\"countdown-section\"><span class=\"countdown-amount\">2</span><span class=\"countdown-period\">Months</span></span><span class=\"countdown-section\"><span class=\"countdown-amount\">28</span><span class=\"countdown-period\">Days</span></span><span class=\"countdown-section\"><span class=\"countdown-amount\">16</span><span class=\"countdown-period\">Hours</span></span><span class=\"countdown-section\"><span class=\"countdown-amount\">52</span><span class=\"countdown-period\">Minutes</span></span><span class=\"countdown-section\"><span class=\"countdown-amount\">46</span><span class=\"countdown-period\">Seconds</span></span></span></div>\\n        <div class=\"center\">\\n            <div class=\"guido-button-block\">\\n                <button class=\"js-guido-mode guido-button\">'\n",
      "\n",
      "Let's clean it up a bit. This step is not needed, it just makes it a bit\n",
      "easier to visualize the returned html to see what we need to target to extract\n",
      "our required information.\n",
      "\n",
      "    \n",
      "    \n",
      "           >>> from pprint import pprint\n",
      "           >>> pprint(r.html.search('Python 2.7 will retire in...{}Enable')[0])\n",
      "           ('</h1>\\n'\n",
      "    '        </div>\\n'\n",
      "    '        <div class=\"python-27-clock is-countdown\"><span class=\"countdown-row '\n",
      "    'countdown-show6\"><span class=\"countdown-section\"><span '\n",
      "    'class=\"countdown-amount\">1</span><span '\n",
      "    'class=\"countdown-period\">Year</span></span><span '\n",
      "    'class=\"countdown-section\"><span class=\"countdown-amount\">2</span><span '\n",
      "    'class=\"countdown-period\">Months</span></span><span '\n",
      "    'class=\"countdown-section\"><span class=\"countdown-amount\">28</span><span '\n",
      "    'class=\"countdown-period\">Days</span></span><span '\n",
      "    'class=\"countdown-section\"><span class=\"countdown-amount\">16</span><span '\n",
      "    'class=\"countdown-period\">Hours</span></span><span '\n",
      "    'class=\"countdown-section\"><span class=\"countdown-amount\">52</span><span '\n",
      "    'class=\"countdown-period\">Minutes</span></span><span '\n",
      "    'class=\"countdown-section\"><span class=\"countdown-amount\">46</span><span '\n",
      "    'class=\"countdown-period\">Seconds</span></span></span></div>\\n'\n",
      "    '        <div class=\"center\">\\n'\n",
      "    '            <div class=\"guido-button-block\">\\n'\n",
      "    '                <button class=\"js-guido-mode guido-button\">')\n",
      "\n",
      "The rendered html has all the same methods and attributes as above. Let's\n",
      "extract just the data that we want out of the clock into something easy to use\n",
      "elsewhere and introspect like a dictionary.\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> periods = [element.text for element in r.html.find('.countdown-period')]\n",
      "    >>> amounts = [element.text for element in r.html.find('.countdown-amount')]\n",
      "    >>> countdown_data = dict(zip(periods, amounts))\n",
      "    >>> countdown_data\n",
      "    {'Year': '1', 'Months': '2', 'Days': '5', 'Hours': '23', 'Minutes': '34', 'Seconds': '37'}\n",
      "\n",
      "Or you can do this async also:\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> async def get_pyclock():\n",
      "    ...     r = await asession.get('https://pythonclock.org/')\n",
      "    ...     await r.html.arender()\n",
      "    ...     return r\n",
      "    ...\n",
      "    >>> results = asession.run(get_pyclock, get_pyclock, get_pyclock)\n",
      "\n",
      "The rest of the code operates the same way as the synchronous version except\n",
      "that `results` is a list containing multiple response objects however the same\n",
      "basic processes can be applied as above to extract the data you want.\n",
      "\n",
      "Note, the first time you ever run the `render()` method, it will download\n",
      "Chromium into your home directory (e.g. `~/.pyppeteer/`). This only happens\n",
      "once.\n",
      "\n",
      "## Using without Requests\n",
      "\n",
      "You can also use this library without Requests:\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> from requests_html import HTML\n",
      "    >>> doc = \"\"\"<a href='https://httpbin.org'>\"\"\"\n",
      "    >>> html = HTML(html=doc)\n",
      "    >>> html.links\n",
      "    {'https://httpbin.org'}\n",
      "\n",
      "## Installation\n",
      "\n",
      "    \n",
      "    \n",
      "    $ pipenv install requests-html\n",
      "    ✨🍰✨\n",
      "\n",
      "Only **Python 3.6 and above** is supported.\n",
      "\n",
      "## About\n",
      "\n",
      "Pythonic HTML Parsing for Humans™\n",
      "\n",
      "html.python-requests.org\n",
      "\n",
      "### Topics\n",
      "\n",
      "python  html  http  scraping  requests  kennethreitz  beautifulsoup  lxml\n",
      "css-selectors  pyquery\n",
      "\n",
      "### Resources\n",
      "\n",
      "Readme\n",
      "\n",
      "### License\n",
      "\n",
      "MIT license\n",
      "\n",
      "### Code of conduct\n",
      "\n",
      "Code of conduct\n",
      "\n",
      "Activity\n",
      "\n",
      "Custom properties\n",
      "\n",
      "### Stars\n",
      "\n",
      "**13.8k** stars\n",
      "\n",
      "### Watchers\n",
      "\n",
      "**268** watching\n",
      "\n",
      "### Forks\n",
      "\n",
      "**986** forks\n",
      "\n",
      "Report repository\n",
      "\n",
      "##  Releases 1\n",
      "\n",
      "v0.10.0 Latest\n",
      "\n",
      "Feb 18, 2019\n",
      "\n",
      "##  Packages 0\n",
      "\n",
      "No packages published  \n",
      "\n",
      "##  Contributors 60\n",
      "\n",
      "  *   *   *   *   *   *   *   *   *   *   *   *   *   * \n",
      "\n",
      "\\+ 46 contributors\n",
      "\n",
      "## Languages\n",
      "\n",
      "  * Python 99.7%\n",
      "  * Makefile 0.3%\n",
      "\n",
      "## Footer\n",
      "\n",
      "(C) 2025 GitHub, Inc.\n",
      "\n",
      "### Footer navigation\n",
      "\n",
      "  * Terms\n",
      "  * Privacy\n",
      "  * Security\n",
      "  * Status\n",
      "  * Docs\n",
      "  * Contact\n",
      "  * Manage cookies \n",
      "  * Do not share my personal information \n",
      "\n",
      "You can’t perform that action at this time.\n",
      "\n",
      "\n",
      "  * Start Here\n",
      "  * Learn Python \n",
      "\n",
      "Python Tutorials →  \n",
      "In-depth articles and video courses Learning Paths →  \n",
      "Guided study plans for accelerated learning Quizzes →  \n",
      "Check your learning progress Browse Topics →  \n",
      "Focus on a specific area or skill level Community Chat →  \n",
      "Learn with other Pythonistas Office Hours →  \n",
      "Live Q&A; calls with Python experts Podcast →  \n",
      "Hear what’s new in the world of Python Books →  \n",
      "Round out your knowledge and learn offline Reference →  \n",
      "Concise definitions for common Python terms Code Mentor →Beta  \n",
      "Personalized code assistance & learning tools Unlock All Content →\n",
      "\n",
      "  * More \n",
      "\n",
      "Learner Stories Python Newsletter Python Job Board Meet the Team Become a\n",
      "Tutorial Writer Become a Video Instructor\n",
      "\n",
      "  * Search\n",
      "\n",
      "/\n",
      "\n",
      "  * Join\n",
      "  * Sign‑In\n",
      "\n",
      "— FREE Email Series —\n",
      "\n",
      "🐍 Python Tricks 💌\n",
      "\n",
      "Get Python Tricks »\n",
      "\n",
      "🔒 No spam. Unsubscribe any time.\n",
      "\n",
      "Browse Topics Guided Learning Paths  \n",
      "Basics Intermediate Advanced\n",
      "\n",
      "* * *\n",
      "\n",
      "api best-practices career community databases data-science data-structures\n",
      "data-viz devops django docker editors flask front-end gamedev gui machine-\n",
      "learning numpy projects python testing tools web-dev web-scraping\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "  * Choose the Right XML Parsing Model\n",
      "    * Document Object Model (DOM)\n",
      "    * Simple API for XML (SAX)\n",
      "    * Streaming API for XML (StAX)\n",
      "  * Learn About XML Parsers in Python's Standard Library\n",
      "    * xml.dom.minidom: Minimal DOM Implementation\n",
      "    * xml.sax: The SAX Interface for Python\n",
      "    * xml.dom.pulldom: Streaming Pull Parser\n",
      "    * xml.etree.ElementTree: A Lightweight, Pythonic Alternative\n",
      "  * Explore Third-Party XML Parser Libraries\n",
      "    * untangle: Convert XML to a Python Object\n",
      "    * xmltodict: Convert XML to a Python Dictionary\n",
      "    * lxml: Use ElementTree on Steroids\n",
      "    * BeautifulSoup: Deal With Malformed XML\n",
      "  * Bind XML Data to Python Objects\n",
      "    * Define Models With XPath Expressions\n",
      "    * Generate Models From an XML Schema\n",
      "  * Defuse the XML Bomb With Secure Parsers\n",
      "  * Conclusion\n",
      "\n",
      "Mark as Completed\n",
      "\n",
      "Share\n",
      "\n",
      "# A Roadmap to XML Parsers in Python\n",
      "\n",
      "by Bartosz Zaczyński intermediate\n",
      "\n",
      "Mark as Completed Share\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "  * Choose the Right XML Parsing Model\n",
      "    * Document Object Model (DOM)\n",
      "    * Simple API for XML (SAX)\n",
      "    * Streaming API for XML (StAX)\n",
      "  * Learn About XML Parsers in Python's Standard Library\n",
      "    * xml.dom.minidom: Minimal DOM Implementation\n",
      "    * xml.sax: The SAX Interface for Python\n",
      "    * xml.dom.pulldom: Streaming Pull Parser\n",
      "    * xml.etree.ElementTree: A Lightweight, Pythonic Alternative\n",
      "  * Explore Third-Party XML Parser Libraries\n",
      "    * untangle: Convert XML to a Python Object\n",
      "    * xmltodict: Convert XML to a Python Dictionary\n",
      "    * lxml: Use ElementTree on Steroids\n",
      "    * BeautifulSoup: Deal With Malformed XML\n",
      "  * Bind XML Data to Python Objects\n",
      "    * Define Models With XPath Expressions\n",
      "    * Generate Models From an XML Schema\n",
      "  * Defuse the XML Bomb With Secure Parsers\n",
      "  * Conclusion\n",
      "\n",
      "Remove ads\n",
      "\n",
      "If you've ever tried to parse an **XML document** in Python before, then you\n",
      "know how surprisingly difficult such a task can be. On the one hand, the Zen\n",
      "of Python promises only one obvious way to achieve your goal. At the same\n",
      "time, the standard library follows the batteries included motto by letting you\n",
      "choose from not one but several XML parsers. Luckily, the Python community\n",
      "solved this surplus problem by creating even more XML parsing libraries.\n",
      "\n",
      "Jokes aside, all XML parsers have their place in a world full of smaller or\n",
      "bigger challenges. It's worthwhile to familiarize yourself with the available\n",
      "tools.\n",
      "\n",
      "**In this tutorial, you 'll learn how to:**\n",
      "\n",
      "  * Choose the right XML **parsing model**\n",
      "  * Use the XML parsers in the **standard library**\n",
      "  * Use major XML parsing **libraries**\n",
      "  * Parse XML documents declaratively using **data binding**\n",
      "  * Use safe XML parsers to eliminate **security vulnerabilities**\n",
      "\n",
      "You can use this tutorial as a **roadmap** to guide you through the confusing\n",
      "world of XML parsers in Python. By the end of it, you'll be able to pick the\n",
      "right XML parser for a given problem. To get the most out of this tutorial,\n",
      "you should already be familiar with XML and its building blocks, as well as\n",
      "how to work with files in Python.\n",
      "\n",
      "**Free Bonus:** 5 Thoughts On Python Mastery, a free course for Python\n",
      "developers that shows you the roadmap and the mindset you'll need to take your\n",
      "Python skills to the next level.\n",
      "\n",
      "## Choose the Right XML Parsing Model\n",
      "\n",
      "It turns out that you can process XML documents using a few language-agnostic\n",
      "strategies. Each demonstrates different memory and speed trade-offs, which can\n",
      "partially justify the wide range of XML parsers available in Python. In the\n",
      "following section, you'll find out their differences and strengths.\n",
      "\n",
      "Remove ads\n",
      "\n",
      "### Document Object Model (DOM)\n",
      "\n",
      "Historically, the first and the most widespread model for parsing XML has been\n",
      "the DOM, or the Document Object Model, originally defined by the World Wide\n",
      "Web Consortium (W3C). You might have already heard about the DOM because web\n",
      "browsers expose a DOM interface through JavaScript to let you manipulate the\n",
      "HTML code of your websites. Both XML and HTML belong to the same family of\n",
      "markup languages, which makes parsing XML with the DOM possible.\n",
      "\n",
      "The DOM is arguably the most straightforward and versatile model to use. It\n",
      "defines a handful of **standard operations** for traversing and modifying\n",
      "document elements arranged in a hierarchy of objects. An abstract\n",
      "representation of the entire document tree is stored in memory, giving you\n",
      "**random access** to the individual elements.\n",
      "\n",
      "While the DOM tree allows for fast and **omnidirectional navigation** ,\n",
      "building its abstract representation in the first place can be time-consuming.\n",
      "Moreover, the XML gets **parsed at once** , as a whole, so it has to be\n",
      "reasonably small to fit the available memory. This renders the DOM suitable\n",
      "only for moderately large configuration files rather than multi-gigabyte XML\n",
      "databases.\n",
      "\n",
      "Use a DOM parser when convenience is more important than processing time and\n",
      "when memory is not an issue. Some typical use cases are when you need to parse\n",
      "a relatively small document or when you only need to do the parsing\n",
      "infrequently.\n",
      "\n",
      "### Simple API for XML (SAX)\n",
      "\n",
      "To address the shortcomings of the DOM, the Java community came up with a\n",
      "library through a collaborative effort, which then became an alternative model\n",
      "for parsing XML in other languages. There was no formal specification, only\n",
      "organic discussions on a mailing list. The end result was an **event-based\n",
      "streaming API** that operates sequentially on individual elements rather than\n",
      "the whole tree.\n",
      "\n",
      "Elements are processed from top to bottom in the same order they appear in the\n",
      "document. The parser triggers user-defined callbacks to handle specific XML\n",
      "nodes as it finds them in the document. This approach is known as **\" push\"\n",
      "parsing** because elements are pushed to your functions by the parser.\n",
      "\n",
      "SAX also lets you discard elements if you're not interested in them. This\n",
      "means it has a much lower memory footprint than DOM and can deal with\n",
      "arbitrarily large files, which is great for **single-pass processing** such as\n",
      "indexing, conversion to other formats, and so on.\n",
      "\n",
      "However, finding or modifying random tree nodes is cumbersome because it\n",
      "usually requires multiple passes on the document and tracking the visited\n",
      "nodes. SAX is also inconvenient for handling deeply nested elements. Finally,\n",
      "the SAX model just allows for **read-only** parsing.\n",
      "\n",
      "In short, SAX is cheap in terms of space and time but more difficult to use\n",
      "than DOM in most cases. It works well for parsing very large documents or\n",
      "parsing incoming XML data in real time.\n",
      "\n",
      "### Streaming API for XML (StAX)\n",
      "\n",
      "Although somewhat less popular in Python, this third approach to parsing XML\n",
      "builds on top of SAX. It extends the idea of **streaming** but uses a **\"\n",
      "pull\" parsing** model instead, which gives you more control. You can think of\n",
      "StAX as an iterator advancing a **cursor object** through an XML document,\n",
      "where custom handlers call the parser on demand and not the other way around.\n",
      "\n",
      "**Note:** It's possible to combine more than one XML parsing model. For\n",
      "example, you can use SAX or StAX to quickly find an interesting piece of data\n",
      "in the document and then build a DOM representation of only that particular\n",
      "branch in memory.\n",
      "\n",
      "Using StAX gives you more control over the parsing process and allows for more\n",
      "convenient **state management**. The events in the stream are only consumed\n",
      "when requested, enabling lazy evaluation. Other than that, its performance\n",
      "should be on par with SAX, depending on the parser implementation.\n",
      "\n",
      "## Learn About XML Parsers in Python's Standard Library\n",
      "\n",
      "In this section, you'll take a look at Python's built-in XML parsers, which\n",
      "are available to you in nearly every Python distribution. You're going to\n",
      "compare those parsers against a sample Scalable Vector Graphics (SVG) image,\n",
      "which is an XML-based format. By processing the same document with different\n",
      "parsers, you'll be able to choose the one that suits you best.\n",
      "\n",
      "The sample image, which you're about to save in a local file for reference,\n",
      "depicts a smiley face. It consists of the following XML content:\n",
      "\n",
      "XML\n",
      "\n",
      "    \n",
      "    \n",
      "    <?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
      "    <!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
      "      \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\" [\n",
      "        <!ENTITY custom_entity \"Hello\">\n",
      "    ]>\n",
      "    <svg xmlns=\"http://www.w3.org/2000/svg\"\n",
      "      xmlns:inkscape=\"http://www.inkscape.org/namespaces/inkscape\"\n",
      "      viewBox=\"-105 -100 210 270\" width=\"210\" height=\"270\">\n",
      "      <inkscape:custom x=\"42\" inkscape:z=\"555\">Some value</inkscape:custom>\n",
      "      <defs>\n",
      "        <linearGradient id=\"skin\" x1=\"0\" x2=\"0\" y1=\"0\" y2=\"1\">\n",
      "          <stop offset=\"0%\" stop-color=\"yellow\" stop-opacity=\"1.0\"/>\n",
      "          <stop offset=\"75%\" stop-color=\"gold\" stop-opacity=\"1.0\"/>\n",
      "          <stop offset=\"100%\" stop-color=\"orange\" stop-opacity=\"1\"/>\n",
      "        </linearGradient>\n",
      "      </defs>\n",
      "      <g id=\"smiley\" inkscape:groupmode=\"layer\" inkscape:label=\"Smiley\">\n",
      "        <!-- Head -->\n",
      "        <circle cx=\"0\" cy=\"0\" r=\"50\"\n",
      "          fill=\"url(#skin)\" stroke=\"orange\" stroke-width=\"2\"/>\n",
      "        <!-- Eyes -->\n",
      "        <ellipse cx=\"-20\" cy=\"-10\" rx=\"6\" ry=\"8\" fill=\"black\" stroke=\"none\"/>\n",
      "        <ellipse cx=\"20\" cy=\"-10\" rx=\"6\" ry=\"8\" fill=\"black\" stroke=\"none\"/>\n",
      "        <!-- Mouth -->\n",
      "        <path d=\"M-20 20 A25 25 0 0 0 20 20\"\n",
      "          fill=\"white\" stroke=\"black\" stroke-width=\"3\"/>\n",
      "      </g>\n",
      "      <text x=\"-40\" y=\"75\">&custom_entity; &lt;svg&gt;!</text>\n",
      "      <script>\n",
      "        <![CDATA[\n",
      "          console.log(\"CDATA disables XML parsing: <svg>\")\n",
      "          const smiley = document.getElementById(\"smiley\")\n",
      "          const eyes = document.querySelectorAll(\"ellipse\")\n",
      "          const setRadius = r => e => eyes.forEach(x => x.setAttribute(\"ry\", r))\n",
      "          smiley.addEventListener(\"mouseenter\", setRadius(2))\n",
      "          smiley.addEventListener(\"mouseleave\", setRadius(8))\n",
      "        ]]>\n",
      "      </script>\n",
      "    </svg>\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "It starts with an **XML declaration** , followed by a Document Type Definition\n",
      "(DTD) and the `<svg>` **root element**. The DTD is optional, but it can help\n",
      "validate your document structure if you decide to use an XML validator. The\n",
      "root element specifies the **default namespace** `xmlns` as well as a\n",
      "**prefixed namespace** `xmlns:inkscape` for editor-specific elements and\n",
      "attributes. The document also contains:\n",
      "\n",
      "  * Nested elements\n",
      "  * Attributes\n",
      "  * Comments\n",
      "  * Character data (`CDATA`)\n",
      "  * Predefined and custom entities\n",
      "\n",
      "Go ahead, save the XML in a file named _smiley.svg_ , and open it using a\n",
      "modern web browser, which will run the JavaScript snippet present at the end:\n",
      "\n",
      "The code adds an interactive component to the image. When you hover the mouse\n",
      "over the smiley face, it blinks its eyes. If you want to edit the smiley face\n",
      "using a convenient graphical user interface (GUI), then you can open the file\n",
      "using a vector graphics editor such as Adobe Illustrator or Inkscape.\n",
      "\n",
      "**Note:** Unlike JSON or YAML, some features of XML can be exploited by\n",
      "hackers. The standard XML parsers available in the `xml` package in Python are\n",
      "insecure and vulnerable to an array of attacks. To safely parse XML documents\n",
      "from an untrusted source, prefer secure alternatives. You can jump to the last\n",
      "section in this tutorial for more details.\n",
      "\n",
      "It's worth noting that Python's standard library defines **abstract\n",
      "interfaces** for parsing XML documents while letting you supply concrete\n",
      "parser implementation. In practice, you rarely do that because Python bundles\n",
      "a binding for the Expat library, which is a widely used open-source XML parser\n",
      "written in C. All of the following Python modules in the standard library use\n",
      "Expat under the hood by default.\n",
      "\n",
      "Unfortunately, while the Expat parser can tell you if your document is **well-\n",
      "formed** , it can't **validate** the structure of your documents against an\n",
      "XML Schema Definition (XSD) or a Document Type Definition (DTD). For that,\n",
      "you'll have to use one of the third-party libraries discussed later.\n",
      "\n",
      "Remove ads\n",
      "\n",
      "### `xml.dom.minidom`: Minimal DOM Implementation\n",
      "\n",
      "Considering that parsing XML documents using the DOM is arguably the most\n",
      "straightforward, you won't be that surprised to find a DOM parser in the\n",
      "Python standard library. What is surprising, though, is that there are\n",
      "actually two DOM parsers.\n",
      "\n",
      "The `xml.dom` package houses two modules to work with DOM in Python:\n",
      "\n",
      "  1. `xml.dom.minidom`\n",
      "  2. `xml.dom.pulldom`\n",
      "\n",
      "The first is a stripped-down implementation of the DOM interface conforming to\n",
      "a relatively old version of the W3C specification. It provides common objects\n",
      "defined by the DOM API such as `Document`, `Element`, and `Attr`. This module\n",
      "is poorly documented and has quite limited usefulness, as you're about to find\n",
      "out.\n",
      "\n",
      "The second module has a slightly misleading name because it defines a\n",
      "**streaming pull parser** , which can _optionally_ produce a DOM\n",
      "representation of the current node in the document tree. You'll find more\n",
      "information about the `pulldom` parser later.\n",
      "\n",
      "There are two functions in `minidom` that let you parse XML data from various\n",
      "data sources. One accepts either a filename or a file object, while another\n",
      "one expects a Python string:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> from xml.dom.minidom import parse, parseString\n",
      "    \n",
      "    >>> # Parse XML from a filename\n",
      "    >>> document = parse(\"smiley.svg\")\n",
      "    \n",
      "    >>> # Parse XML from a file object\n",
      "    >>> with open(\"smiley.svg\") as file:\n",
      "    ...     document = parse(file)\n",
      "    ...\n",
      "    \n",
      "    >>> # Parse XML from a Python string\n",
      "    >>> document = parseString(\"\"\"\\\n",
      "    ... <svg viewBox=\"-105 -100 210 270\">\n",
      "    ...   <!-- More content goes here... -->\n",
      "    ... </svg>\n",
      "    ... \"\"\")\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "The triple-quoted string helps embed a multiline string literal without using\n",
      "the continuation character (`\\`) at the end of each line. In any case, you'll\n",
      "end up with a `Document` instance, which exhibits the familiar DOM interface,\n",
      "letting you traverse the tree.\n",
      "\n",
      "Apart from that, you'll be able to access the XML declaration, DTD, and the\n",
      "root element:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> document = parse(\"smiley.svg\")\n",
      "    \n",
      "    >>> # XML Declaration\n",
      "    >>> document.version, document.encoding, document.standalone\n",
      "    ('1.0', 'UTF-8', False)\n",
      "    \n",
      "    >>> # Document Type Definition (DTD)\n",
      "    >>> dtd = document.doctype\n",
      "    >>> dtd.entities[\"custom_entity\"].childNodes\n",
      "    [<DOM Text node \"'Hello'\">]\n",
      "    \n",
      "    >>> # Document Root\n",
      "    >>> document.documentElement\n",
      "    <DOM Element: svg at 0x7fc78c62d790>\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "As you can see, even though the default XML parser in Python can't validate\n",
      "documents, it still lets you inspect `.doctype`, the DTD, if it's present.\n",
      "Note that the XML declaration and DTD are optional. If the XML declaration or\n",
      "a given XML attribute is missing, then the corresponding Python attributes\n",
      "will be `None`.\n",
      "\n",
      "To find an element by ID, you must use the `Document` instance rather than a\n",
      "specific parent `Element`. The sample SVG image has two nodes with an `id`\n",
      "attribute, but you can't find either of them:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> document.getElementById(\"skin\") is None\n",
      "    True\n",
      "    >>> document.getElementById(\"smiley\") is None\n",
      "    True\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "That may be surprising for someone who has only worked with HTML and\n",
      "JavaScript but hasn't worked with XML before. While HTML defines the semantics\n",
      "for certain elements and attributes such as `<body>` or `id`, XML doesn't\n",
      "attach any meaning to its building blocks. You need to mark an attribute as an\n",
      "ID explicitly using DTD or by calling `.setIdAttribute()` in Python, for\n",
      "example:\n",
      "\n",
      "Definition Style | Implementation  \n",
      "---|---  \n",
      "DTD | `<!ATTLIST linearGradient id ID #IMPLIED>`  \n",
      "Python | `linearGradient.setIdAttribute(\"id\")`  \n",
      "  \n",
      "However, using a DTD isn't enough to fix the problem if your document has a\n",
      "default namespace, which is the case for the sample SVG image. To address\n",
      "this, you can visit all elements recursively in Python, check whether they\n",
      "have the `id` attribute, and indicate it as their ID in one go:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> from xml.dom.minidom import parse, Node\n",
      "    \n",
      "    >>> def set_id_attribute(parent, attribute_name=\"id\"):\n",
      "    ...     if parent.nodeType == Node.ELEMENT_NODE:\n",
      "    ...         if parent.hasAttribute(attribute_name):\n",
      "    ...             parent.setIdAttribute(attribute_name)\n",
      "    ...     for child in parent.childNodes:\n",
      "    ...         set_id_attribute(child, attribute_name)\n",
      "    ...\n",
      "    >>> document = parse(\"smiley.svg\")\n",
      "    >>> set_id_attribute(document)\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "Your custom `set_id_attribute()` function takes a parent element and an\n",
      "optional name for the identity attribute, which defaults to `\"id\"`. When you\n",
      "call that function on your SVG document, then all children elements that have\n",
      "an `id` attribute will become accessible through the DOM API:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> document.getElementById(\"skin\")\n",
      "    <DOM Element: linearGradient at 0x7f82247703a0>\n",
      "    \n",
      "    >>> document.getElementById(\"smiley\")\n",
      "    <DOM Element: g at 0x7f8224770940>\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "Now, you're getting the expected XML element corresponding to the `id`\n",
      "attribute's value.\n",
      "\n",
      "Using an ID allows for finding at most one unique element, but you can also\n",
      "find a collection of similar elements by their **tag name**. Unlike the\n",
      "`.getElementById()` method, you can call `.getElementsByTagName()` on the\n",
      "document or a particular parent element to reduce the search scope:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> document.getElementsByTagName(\"ellipse\")\n",
      "    [\n",
      "        <DOM Element: ellipse at 0x7fa2c944f430>,\n",
      "        <DOM Element: ellipse at 0x7fa2c944f4c0>\n",
      "    ]\n",
      "    \n",
      "    >>> root = document.documentElement\n",
      "    >>> root.getElementsByTagName(\"ellipse\")\n",
      "    [\n",
      "        <DOM Element: ellipse at 0x7fa2c944f430>,\n",
      "        <DOM Element: ellipse at 0x7fa2c944f4c0>\n",
      "    ]\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "Notice that `.getElementsByTagName()` always returns a list of elements\n",
      "instead of a single element or `None`. Forgetting about it when you switch\n",
      "between both methods is a common source of errors.\n",
      "\n",
      "Unfortunately, elements like `<inkscape:custom>` that are **prefixed** with a\n",
      "namespace identifier won't be included. They must be searched using\n",
      "`.getElementsByTagNameNS()`, which expects different arguments:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> document.getElementsByTagNameNS(\n",
      "    ...     \"http://www.inkscape.org/namespaces/inkscape\",\n",
      "    ...     \"custom\"\n",
      "    ... )\n",
      "    ...\n",
      "    [<DOM Element: inkscape:custom at 0x7f97e3f2a3a0>]\n",
      "    \n",
      "    >>> document.getElementsByTagNameNS(\"*\", \"custom\")\n",
      "    [<DOM Element: inkscape:custom at 0x7f97e3f2a3a0>]\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "The first argument must be the XML namespace, which typically has the form of\n",
      "a domain name, while the second argument is the tag name. Notice that the\n",
      "namespace prefix is irrelevant! To search all namespaces, you can provide a\n",
      "wildcard character (`*`).\n",
      "\n",
      "**Note:** To find the namespaces declared in your XML document, you can check\n",
      "out the root element's attributes. In theory, they could be declared on any\n",
      "element, but the top-level one is where you'd usually find them.\n",
      "\n",
      "Once you locate the element you're interested in, you may use it to walk over\n",
      "the tree. However, another jarring quirk with `minidom` is how it handles\n",
      "**whitespace characters** between elements:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> element = document.getElementById(\"smiley\")\n",
      "    \n",
      "    >>> element.parentNode\n",
      "    <DOM Element: svg at 0x7fc78c62d790>\n",
      "    \n",
      "    >>> element.firstChild\n",
      "    <DOM Text node \"'\\n    '\">\n",
      "    \n",
      "    >>> element.lastChild\n",
      "    <DOM Text node \"'\\n  '\">\n",
      "    \n",
      "    >>> element.nextSibling\n",
      "    <DOM Text node \"'\\n  '\">\n",
      "    \n",
      "    >>> element.previousSibling\n",
      "    <DOM Text node \"'\\n  '\">\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "The newline characters and leading indentation are captured as separate tree\n",
      "elements, which is what the specification requires. Some parsers let you\n",
      "ignore these, but not the Python one. What you can do, however, is collapse\n",
      "whitespace in such nodes manually:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> def remove_whitespace(node):\n",
      "    ...     if node.nodeType == Node.TEXT_NODE:\n",
      "    ...         if node.nodeValue.strip() == \"\":\n",
      "    ...             node.nodeValue = \"\"\n",
      "    ...     for child in node.childNodes:\n",
      "    ...         remove_whitespace(child)\n",
      "    ...\n",
      "    >>> document = parse(\"smiley.svg\")\n",
      "    >>> set_id_attribute(document)\n",
      "    >>> remove_whitespace(document)\n",
      "    >>> document.normalize()\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "Note that you also have to `.normalize()` the document to combine adjacent\n",
      "text nodes. Otherwise, you could end up with a bunch of redundant XML elements\n",
      "with just whitespace. Again, recursion is the only way to visit tree elements\n",
      "since you can't iterate over the document and its elements with a loop.\n",
      "Finally, this should give you the expected result:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> element = document.getElementById(\"smiley\")\n",
      "    \n",
      "    >>> element.parentNode\n",
      "    <DOM Element: svg at 0x7fc78c62d790>\n",
      "    \n",
      "    >>> element.firstChild\n",
      "    <DOM Comment node \"' Head '\">\n",
      "    \n",
      "    >>> element.lastChild\n",
      "    <DOM Element: path at 0x7f8beea0f670>\n",
      "    \n",
      "    >>> element.nextSibling\n",
      "    <DOM Element: text at 0x7f8beea0f700>\n",
      "    \n",
      "    >>> element.previousSibling\n",
      "    <DOM Element: defs at 0x7f8beea0f160>\n",
      "    \n",
      "    >>> element.childNodes\n",
      "    [\n",
      "        <DOM Comment node \"' Head '\">,\n",
      "        <DOM Element: circle at 0x7f8beea0f4c0>,\n",
      "        <DOM Comment node \"' Eyes '\">,\n",
      "        <DOM Element: ellipse at 0x7fa2c944f430>,\n",
      "        <DOM Element: ellipse at 0x7fa2c944f4c0>,\n",
      "        <DOM Comment node \"' Mouth '\">,\n",
      "        <DOM Element: path at 0x7f8beea0f670>\n",
      "    ]\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "Elements expose a few helpful methods and properties to let you query their\n",
      "details:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> element = document.getElementsByTagNameNS(\"*\", \"custom\")[0]\n",
      "    \n",
      "    >>> element.prefix\n",
      "    'inkscape'\n",
      "    \n",
      "    >>> element.tagName\n",
      "    'inkscape:custom'\n",
      "    \n",
      "    >>> element.attributes\n",
      "    <xml.dom.minidom.NamedNodeMap object at 0x7f6c9d83ba80>\n",
      "    \n",
      "    >>> dict(element.attributes.items())\n",
      "    {'x': '42', 'inkscape:z': '555'}\n",
      "    \n",
      "    >>> element.hasChildNodes()\n",
      "    True\n",
      "    \n",
      "    >>> element.hasAttributes()\n",
      "    True\n",
      "    \n",
      "    >>> element.hasAttribute(\"x\")\n",
      "    True\n",
      "    \n",
      "    >>> element.getAttribute(\"x\")\n",
      "    '42'\n",
      "    \n",
      "    >>> element.getAttributeNode(\"x\")\n",
      "    <xml.dom.minidom.Attr object at 0x7f82244a05f0>\n",
      "    \n",
      "    >>> element.getAttribute(\"missing-attribute\")\n",
      "    ''\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "For instance, you can check an element's namespace, tag name, or attributes.\n",
      "If you ask for a missing attribute, then you'll get an empty string (`''`).\n",
      "\n",
      "Dealing with namespaced attributes isn't much different. You just have to\n",
      "remember to prefix the attribute name accordingly or provide the domain name:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> element.hasAttribute(\"z\")\n",
      "    False\n",
      "    \n",
      "    >>> element.hasAttribute(\"inkscape:z\")\n",
      "    True\n",
      "    \n",
      "    >>> element.hasAttributeNS(\n",
      "    ...     \"http://www.inkscape.org/namespaces/inkscape\",\n",
      "    ...     \"z\"\n",
      "    ... )\n",
      "    ...\n",
      "    True\n",
      "    \n",
      "    >>> element.hasAttributeNS(\"*\", \"z\")\n",
      "    False\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "Strangely enough, the wildcard character (`*`) doesn't work here as it did\n",
      "with the `.getElementsByTagNameNS()` method before.\n",
      "\n",
      "Since this tutorial is only about XML parsing, you'll need to check the\n",
      "`minidom` documentation for methods that modify the DOM tree. They mostly\n",
      "follow the W3C specification.\n",
      "\n",
      "As you can see, the `minidom` module isn't terribly convenient. Its main\n",
      "advantage comes from being part of the standard library, which means you don't\n",
      "have to install any external dependencies in your project to work with the\n",
      "DOM.\n",
      "\n",
      "Remove ads\n",
      "\n",
      "### `xml.sax`: The SAX Interface for Python\n",
      "\n",
      "To start working with SAX in Python, you can use the same `parse()` and\n",
      "`parseString()` convenience functions as before, but from the `xml.sax`\n",
      "package instead. You also have to provide at least one more required argument,\n",
      "which must be a **content handler** instance. In the spirit of Java, you\n",
      "provide one by subclassing a specific base class:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    from xml.sax import parse\n",
      "    from xml.sax.handler import ContentHandler\n",
      "    \n",
      "    class SVGHandler(ContentHandler):\n",
      "        pass\n",
      "    \n",
      "    parse(\"smiley.svg\", SVGHandler())\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "The content handler receives a **stream of events** corresponding to elements\n",
      "in your document as it's being parsed. Running this code won't do anything\n",
      "useful yet because your handler class is empty. To make it work, you'll need\n",
      "to overload one or more callback methods from the superclass.\n",
      "\n",
      "Fire up your favorite editor, type the following code, and save it in a file\n",
      "named `svg_handler.py`:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    # svg_handler.py\n",
      "    \n",
      "    from xml.sax.handler import ContentHandler\n",
      "    \n",
      "    class SVGHandler(ContentHandler):\n",
      "    \n",
      "        def startElement(self, name, attrs):\n",
      "            print(f\"BEGIN: <{name}>, {attrs.keys()}\")\n",
      "    \n",
      "        def endElement(self, name):\n",
      "            print(f\"END: </{name}>\")\n",
      "    \n",
      "        def characters(self, content):\n",
      "            if content.strip() != \"\":\n",
      "                print(\"CONTENT:\", repr(content))\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "This modified content handler prints out a few events onto the standard\n",
      "output. The SAX parser will call these three methods for you in response to\n",
      "finding the start tag, end tag, and some text between them. When you open an\n",
      "interactive session of the Python interpreter, import your content handler and\n",
      "give it a test drive. It should produce the following output:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> from xml.sax import parse\n",
      "    >>> from svg_handler import SVGHandler\n",
      "    >>> parse(\"smiley.svg\", SVGHandler())\n",
      "    BEGIN: <svg>, ['xmlns', 'xmlns:inkscape', 'viewBox', 'width', 'height']\n",
      "    BEGIN: <inkscape:custom>, ['x', 'inkscape:z']\n",
      "    CONTENT: 'Some value'\n",
      "    END: </inkscape:custom>\n",
      "    BEGIN: <defs>, []\n",
      "    BEGIN: <linearGradient>, ['id', 'x1', 'x2', 'y1', 'y2']\n",
      "    BEGIN: <stop>, ['offset', 'stop-color', 'stop-opacity']\n",
      "    END: </stop>\n",
      "    ⋮\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "That's essentially the observer design pattern, which lets you translate XML\n",
      "into another hierarchical format incrementally. Say you wanted to convert that\n",
      "SVG file into a simplified JSON representation. First, you'll want to store\n",
      "your content handler object in a separate variable to extract information from\n",
      "it later:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> from xml.sax import parse\n",
      "    >>> from svg_handler import SVGHandler\n",
      "    >>> handler = SVGHandler()\n",
      "    >>> parse(\"smiley.svg\", handler)\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "Since the SAX parser emits events without providing any context about the\n",
      "element it's found, you need to keep track of where you are in the tree.\n",
      "Therefore, it makes sense to push and pop the current element onto a stack,\n",
      "which you can simulate through a regular Python list. You may also define a\n",
      "helper property `.current_element` that will return the last element placed on\n",
      "the top of the stack:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    # svg_handler.py\n",
      "    \n",
      "    # ...\n",
      "    \n",
      "    class SVGHandler(ContentHandler):\n",
      "    \n",
      "        def __init__(self):\n",
      "            super().__init__()\n",
      "            self.element_stack = []\n",
      "    \n",
      "        @property\n",
      "        def current_element(self):\n",
      "            return self.element_stack[-1]\n",
      "    \n",
      "        # ...\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "When the SAX parser finds a new element, you can immediately capture its tag\n",
      "name and attributes while making placeholders for children elements and the\n",
      "value, both of which are optional. For now, you can store every element as a\n",
      "`dict` object. Replace your existing `.startElement()` method with a new\n",
      "implementation:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    # svg_handler.py\n",
      "    \n",
      "    # ...\n",
      "    \n",
      "    class SVGHandler(ContentHandler):\n",
      "    \n",
      "        # ...\n",
      "    \n",
      "        def startElement(self, name, attrs):\n",
      "            self.element_stack.append({\n",
      "                \"name\": name,\n",
      "                \"attributes\": dict(attrs),\n",
      "                \"children\": [],\n",
      "                \"value\": \"\"\n",
      "            })\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "The SAX parser gives you attributes as a mapping that you can convert to a\n",
      "plain Python dictionary with a call to the `dict()` function. The element\n",
      "value is often spread over multiple pieces that you can concatenate using the\n",
      "plus operator (`+`) or a corresponding augmented assignment statement:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    # svg_handler.py\n",
      "    \n",
      "    # ...\n",
      "    \n",
      "    class SVGHandler(ContentHandler):\n",
      "    \n",
      "        # ...\n",
      "    \n",
      "        def characters(self, content):\n",
      "            self.current_element[\"value\"] += content\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "Aggregating text in such a way will ensure that multiline content ends up in\n",
      "the current element. For example, the `<script>` tag in the sample SVG file\n",
      "contains six lines of JavaScript code, which trigger separate calls to the\n",
      "`characters()` callback.\n",
      "\n",
      "Finally, once the parser stumbles on a closing tag, you can pop the current\n",
      "element from the stack and append it to its parent's children. If there's only\n",
      "one element left, then it will be your document's root that you should keep\n",
      "for later. Other than that, you might want to clean the current element by\n",
      "removing keys with empty values:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    # svg_handler.py\n",
      "    \n",
      "    # ...\n",
      "    \n",
      "    class SVGHandler(ContentHandler):\n",
      "    \n",
      "        # ...\n",
      "    \n",
      "        def endElement(self, name):\n",
      "            clean(self.current_element)\n",
      "            if len(self.element_stack) > 1:\n",
      "                child = self.element_stack.pop()\n",
      "                self.current_element[\"children\"].append(child)\n",
      "    \n",
      "    def clean(element):\n",
      "        element[\"value\"] = element[\"value\"].strip()\n",
      "        for key in (\"attributes\", \"children\", \"value\"):\n",
      "            if not element[key]:\n",
      "                del element[key]\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "Note that `clean()` is a function defined outside of the class body. Cleaning\n",
      "must be done at the end since there's no way of knowing up front how many text\n",
      "pieces to concatenate there might be. You can expand the collapsible section\n",
      "below for a complete content handler's code.\n",
      "\n",
      "SAX Handler for SVG to JSON ConverterShow/Hide\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    # svg_handler.py\n",
      "    \n",
      "    from xml.sax.handler import ContentHandler\n",
      "    \n",
      "    class SVGHandler(ContentHandler):\n",
      "    \n",
      "        def __init__(self):\n",
      "            super().__init__()\n",
      "            self.element_stack = []\n",
      "    \n",
      "        @property\n",
      "        def current_element(self):\n",
      "            return self.element_stack[-1]\n",
      "    \n",
      "        def startElement(self, name, attrs):\n",
      "            self.element_stack.append({\n",
      "                \"name\": name,\n",
      "                \"attributes\": dict(attrs),\n",
      "                \"children\": [],\n",
      "                \"value\": \"\"\n",
      "            })\n",
      "    \n",
      "        def endElement(self, name):\n",
      "            clean(self.current_element)\n",
      "            if len(self.element_stack) > 1:\n",
      "                child = self.element_stack.pop()\n",
      "                self.current_element[\"children\"].append(child)\n",
      "    \n",
      "        def characters(self, content):\n",
      "            self.current_element[\"value\"] += content\n",
      "    \n",
      "    def clean(element):\n",
      "        element[\"value\"] = element[\"value\"].strip()\n",
      "        for key in (\"attributes\", \"children\", \"value\"):\n",
      "            if not element[key]:\n",
      "                del element[key]\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "Now, it's time to put everything to the test by parsing the XML, extracting\n",
      "the root element from your content handler, and dumping it to a JSON string:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> from xml.sax import parse\n",
      "    >>> from svg_handler import SVGHandler\n",
      "    >>> handler = SVGHandler()\n",
      "    >>> parse(\"smiley.svg\", handler)\n",
      "    >>> root = handler.current_element\n",
      "    \n",
      "    >>> import json\n",
      "    >>> print(json.dumps(root, indent=4))\n",
      "    {\n",
      "        \"name\": \"svg\",\n",
      "        \"attributes\": {\n",
      "            \"xmlns\": \"http://www.w3.org/2000/svg\",\n",
      "            \"xmlns:inkscape\": \"http://www.inkscape.org/namespaces/inkscape\",\n",
      "            \"viewBox\": \"-105 -100 210 270\",\n",
      "            \"width\": \"210\",\n",
      "            \"height\": \"270\"\n",
      "        },\n",
      "        \"children\": [\n",
      "            {\n",
      "                \"name\": \"inkscape:custom\",\n",
      "                \"attributes\": {\n",
      "                    \"x\": \"42\",\n",
      "                    \"inkscape:z\": \"555\"\n",
      "                },\n",
      "                \"value\": \"Some value\"\n",
      "            },\n",
      "    ⋮\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "It's worth noting that this implementation has no memory gain over DOM because\n",
      "it builds an abstract representation of the whole document just as before. The\n",
      "difference is that you've made a custom dictionary representation instead of\n",
      "the standard DOM tree. However, you could imagine writing directly to a file\n",
      "or a database instead of memory while receiving SAX events. That would\n",
      "effectively lift your computer memory limit.\n",
      "\n",
      "If you want to parse XML namespaces, then you'll need to create and configure\n",
      "the SAX parser yourself with a bit of boilerplate code and also implement\n",
      "slightly different callbacks:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    # svg_handler.py\n",
      "    \n",
      "    from xml.sax.handler import ContentHandler\n",
      "    \n",
      "    class SVGHandler(ContentHandler):\n",
      "    \n",
      "        def startPrefixMapping(self, prefix, uri):\n",
      "            print(f\"startPrefixMapping: {prefix=}, {uri=}\")\n",
      "    \n",
      "        def endPrefixMapping(self, prefix):\n",
      "            print(f\"endPrefixMapping: {prefix=}\")\n",
      "    \n",
      "        def startElementNS(self, name, qname, attrs):\n",
      "            print(f\"startElementNS: {name=}\")\n",
      "    \n",
      "        def endElementNS(self, name, qname):\n",
      "            print(f\"endElementNS: {name=}\")\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "These callbacks receive additional parameters about the element's namespace.\n",
      "To make the SAX parser actually trigger those callbacks instead of some of the\n",
      "earlier ones, you must explicitly enable **XML namespace** support:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> from xml.sax import make_parser\n",
      "    >>> from xml.sax.handler import feature_namespaces\n",
      "    >>> from svg_handler import SVGHandler\n",
      "    \n",
      "    >>> parser = make_parser()\n",
      "    >>> parser.setFeature(feature_namespaces, True)\n",
      "    >>> parser.setContentHandler(SVGHandler())\n",
      "    \n",
      "    >>> parser.parse(\"smiley.svg\")\n",
      "    startPrefixMapping: prefix=None, uri='http://www.w3.org/2000/svg'\n",
      "    startPrefixMapping: prefix='inkscape', uri='http://www.inkscape.org/namespaces/inkscape'\n",
      "    startElementNS: name=('http://www.w3.org/2000/svg', 'svg')\n",
      "    ⋮\n",
      "    endElementNS: name=('http://www.w3.org/2000/svg', 'svg')\n",
      "    endPrefixMapping: prefix='inkscape'\n",
      "    endPrefixMapping: prefix=None\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "Setting this feature turns the element `name` into a tuple comprised of the\n",
      "namespace's domain name and the tag name.\n",
      "\n",
      "The `xml.sax` package offers a decent event-based XML parser interface modeled\n",
      "after the original Java API. It's somewhat limited compared to the DOM but\n",
      "should be enough to implement a basic XML streaming push parser without\n",
      "resorting to third-party libraries. With this in mind, there's a less verbose\n",
      "pull parser available in Python, which you'll explore next.\n",
      "\n",
      "Remove ads\n",
      "\n",
      "### `xml.dom.pulldom`: Streaming Pull Parser\n",
      "\n",
      "The parsers in the Python standard library often work together. For example,\n",
      "the `xml.dom.pulldom` module wraps the parser from `xml.sax` to take advantage\n",
      "of buffering and read the document in chunks. At the same time, it uses the\n",
      "default DOM implementation from `xml.dom.minidom` for representing document\n",
      "elements. However, those elements are processed one at a time without bearing\n",
      "any relationship until you ask for it explicitly.\n",
      "\n",
      "**Note:** The XML namespace support is enabled by default in\n",
      "`xml.dom.pulldom`.\n",
      "\n",
      "While the SAX model follows the observer pattern, you can think of StAX as the\n",
      "iterator design pattern, which lets you loop over a **flat stream** of events.\n",
      "Once again, you can call the familiar `parse()` or `parseString()` functions\n",
      "imported from the module to parse the SVG image:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> from xml.dom.pulldom import parse\n",
      "    >>> event_stream = parse(\"smiley.svg\")\n",
      "    >>> for event, node in event_stream:\n",
      "    ...     print(event, node)\n",
      "    ...\n",
      "    START_DOCUMENT <xml.dom.minidom.Document object at 0x7f74f9283e80>\n",
      "    START_ELEMENT <DOM Element: svg at 0x7f74fde18040>\n",
      "    CHARACTERS <DOM Text node \"'\\n'\">\n",
      "    ⋮\n",
      "    END_ELEMENT <DOM Element: script at 0x7f74f92b3c10>\n",
      "    CHARACTERS <DOM Text node \"'\\n'\">\n",
      "    END_ELEMENT <DOM Element: svg at 0x7f74fde18040>\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "It takes only a few lines of code to parse the document. The most striking\n",
      "difference between `xml.sax` and `xml.dom.pulldom` is the lack of callbacks\n",
      "since you drive the whole process. You have a lot more freedom in structuring\n",
      "your code, and you don't need to use classes if you don't want to.\n",
      "\n",
      "Notice that the XML nodes pulled from the stream have types defined in\n",
      "`xml.dom.minidom`. But if you were to check their parents, siblings, and\n",
      "children, then you'd find out they know nothing about each other:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> from xml.dom.pulldom import parse, START_ELEMENT\n",
      "    >>> event_stream = parse(\"smiley.svg\")\n",
      "    >>> for event, node in event_stream:\n",
      "    ...     if event == START_ELEMENT:\n",
      "    ...         print(node.parentNode, node.previousSibling, node.childNodes)\n",
      "    <xml.dom.minidom.Document object at 0x7f90864f6e80> None []\n",
      "    None None []\n",
      "    None None []\n",
      "    None None []\n",
      "    ⋮\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "The relevant attributes are empty. Anyway, the pull parser can help in a\n",
      "hybrid approach to quickly look up some parent element and build a DOM tree\n",
      "only for the branch rooted in it:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    from xml.dom.pulldom import parse, START_ELEMENT\n",
      "    \n",
      "    def process_group(parent):\n",
      "        left_eye, right_eye = parent.getElementsByTagName(\"ellipse\")\n",
      "        # ...\n",
      "    \n",
      "    event_stream = parse(\"smiley.svg\")\n",
      "    for event, node in event_stream:\n",
      "        if event == START_ELEMENT:\n",
      "            if node.tagName == \"g\":\n",
      "                event_stream.expandNode(node)\n",
      "                process_group(node)\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "By calling `.expandNode()` on the event stream, you essentially move the\n",
      "iterator forward and parse XML nodes recursively until finding the matching\n",
      "closing tag of the parent element. The resulting node will have children with\n",
      "properly initialized attributes. Moreover, you'll be able to use the DOM\n",
      "methods on them.\n",
      "\n",
      "The pull parser offers an interesting alternative to DOM and SAX by combining\n",
      "the best of both worlds. It's efficient, flexible, and straightforward to use,\n",
      "leading to more compact and readable code. You could also use it to process\n",
      "multiple XML files at the same time more easily. That said, none of the XML\n",
      "parsers mentioned so far can match the elegance, simplicity, and completeness\n",
      "of the last one to arrive in Python's standard library.\n",
      "\n",
      "### `xml.etree.ElementTree`: A Lightweight, Pythonic Alternative\n",
      "\n",
      "The XML parsers you've come to know so far get the job done. However, they\n",
      "don't fit Python's philosophy very well, and that's no accident. While DOM\n",
      "follows the W3C specification and SAX was modeled after a Java API, neither\n",
      "feels particularly Pythonic.\n",
      "\n",
      "Even worse, both DOM and SAX parsers feel antiquated as some of their code in\n",
      "the CPython interpreter hasn't changed for more than two decades! At the time\n",
      "of writing this, their implementation is still incomplete and has missing\n",
      "typeshed stubs, which breaks code completion in code editors.\n",
      "\n",
      "Meanwhile, Python 2.5 brought a fresh perspective on parsing _and_ writing XML\n",
      "documents--the **ElementTree API**. It's a lightweight, efficient, elegant,\n",
      "and feature-rich interface that even some third-party libraries build on. To\n",
      "get started with it, you must import the `xml.etree.ElementTree` module, which\n",
      "is a bit of a mouthful. Therefore, it's customary to define an **alias** like\n",
      "this:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    import xml.etree.ElementTree as ET\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "In slightly older code, you may have seen the `cElementTree` module imported\n",
      "instead. It was an implementation several times faster than the same interface\n",
      "written in C. Today, the regular module uses the fast implementation whenever\n",
      "possible, so you don't need to bother anymore.\n",
      "\n",
      "You can use the ElementTree API by employing different parsing strategies:\n",
      "\n",
      "| Non-incremental | Incremental (Blocking) | Incremental (Non-blocking)  \n",
      "---|---|---|---  \n",
      "`ET.parse()` | ✔️ |  |   \n",
      "`ET.fromstring()` | ✔️ |  |   \n",
      "`ET.iterparse()` |  | ✔️ |   \n",
      "`ET.XMLPullParser` |  |  | ✔️  \n",
      "  \n",
      "The non-incremental strategy loads up the entire document into memory in a\n",
      "**DOM-like fashion**. There are two appropriately named functions in the\n",
      "module that allow for parsing a file or a Python string with XML content:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> import xml.etree.ElementTree as ET\n",
      "    \n",
      "    >>> # Parse XML from a filename\n",
      "    >>> ET.parse(\"smiley.svg\")\n",
      "    <xml.etree.ElementTree.ElementTree object at 0x7fa4c980a6a0>\n",
      "    \n",
      "    >>> # Parse XML from a file object\n",
      "    >>> with open(\"smiley.svg\") as file:\n",
      "    ...     ET.parse(file)\n",
      "    ...\n",
      "    <xml.etree.ElementTree.ElementTree object at 0x7fa4c96df340>\n",
      "    \n",
      "    >>> # Parse XML from a Python string\n",
      "    >>> ET.fromstring(\"\"\"\\\n",
      "    ... <svg viewBox=\"-105 -100 210 270\">\n",
      "    ...   <!-- More content goes here... -->\n",
      "    ... </svg>\n",
      "    ... \"\"\")\n",
      "    <Element 'svg' at 0x7fa4c987a1d0>\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "Parsing a file object or a filename with `parse()` returns an instance of the\n",
      "`ET.ElementTree` class, which represents the whole element hierarchy. On the\n",
      "other hand, parsing a string with `fromstring()` will return the specific root\n",
      "`ET.Element`.\n",
      "\n",
      "Alternatively, you can read the XML document incrementally with a streaming\n",
      "**pull parser** , which yields a sequence of events and elements:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> for event, element in ET.iterparse(\"smiley.svg\"):\n",
      "    ...     print(event, element.tag)\n",
      "    ...\n",
      "    end {http://www.inkscape.org/namespaces/inkscape}custom\n",
      "    end {http://www.w3.org/2000/svg}stop\n",
      "    end {http://www.w3.org/2000/svg}stop\n",
      "    end {http://www.w3.org/2000/svg}stop\n",
      "    end {http://www.w3.org/2000/svg}linearGradient\n",
      "    ⋮\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "By default, `iterparse()` emits only the `end` events associated with the\n",
      "closing XML tag. However, you can subscribe to other events as well. You can\n",
      "find them with string constants such as `\"comment\"`:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> import xml.etree.ElementTree as ET\n",
      "    >>> for event, element in ET.iterparse(\"smiley.svg\", [\"comment\"]):\n",
      "    ...     print(element.text.strip())\n",
      "    ...\n",
      "    Head\n",
      "    Eyes\n",
      "    Mouth\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "Here's a list of all the available event types:\n",
      "\n",
      "  * **`start`:** Start of an element\n",
      "  * **`end`:** End of an element\n",
      "  * **`comment`:** Comment element\n",
      "  * **`pi`:** Processing instruction, as in XSL\n",
      "  * **`start-ns`:** Start of a namespace\n",
      "  * **`end-ns`:** End of a namespace\n",
      "\n",
      "The downside of `iterparse()` is that it uses **blocking calls** to read the\n",
      "next chunk of data, which might be unsuitable for asynchronous code running on\n",
      "a single thread of execution. To alleviate that, you can look into\n",
      "`XMLPullParser`, which is a little bit more verbose:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    import xml.etree.ElementTree as ET\n",
      "    \n",
      "    async def receive_data(url):\n",
      "        \"\"\"Download chunks of bytes from the URL asynchronously.\"\"\"\n",
      "        yield b\"<svg \"\n",
      "        yield b\"viewBox=\\\"-105 -100 210 270\\\"\"\n",
      "        yield b\"></svg>\"\n",
      "    \n",
      "    async def parse(url, events=None):\n",
      "        parser = ET.XMLPullParser(events)\n",
      "        async for chunk in receive_data(url):\n",
      "            parser.feed(chunk)\n",
      "            for event, element in parser.read_events():\n",
      "                yield event, element\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "This hypothetical example feeds the parser with chunks of XML that can arrive\n",
      "a few seconds apart. Once there's enough content, you can iterate over a\n",
      "sequence of events and elements buffered by the parser. This **non-blocking**\n",
      "incremental parsing strategy allows for a truly concurrent parsing of multiple\n",
      "XML documents on the fly while you download them.\n",
      "\n",
      "Elements in the tree are mutable, iterable, and indexable sequences. They have\n",
      "a length corresponding to the number of their immediate children:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> import xml.etree.ElementTree as ET\n",
      "    >>> tree = ET.parse(\"smiley.svg\")\n",
      "    >>> root = tree.getroot()\n",
      "    \n",
      "    >>> # The length of an element equals the number of its children.\n",
      "    >>> len(root)\n",
      "    5\n",
      "    \n",
      "    >>> # The square brackets let you access a child by an index.\n",
      "    >>> root[1]\n",
      "    <Element '{http://www.w3.org/2000/svg}defs' at 0x7fe05d2e8860>\n",
      "    >>> root[2]\n",
      "    <Element '{http://www.w3.org/2000/svg}g' at 0x7fa4c9848400>\n",
      "    \n",
      "    >>> # Elements are mutable. For example, you can swap their children.\n",
      "    >>> root[2], root[1] = root[1], root[2]\n",
      "    \n",
      "    >>> # You can iterate over an element's children.\n",
      "    >>> for child in root:\n",
      "    ...     print(child.tag)\n",
      "    ...\n",
      "    {http://www.inkscape.org/namespaces/inkscape}custom\n",
      "    {http://www.w3.org/2000/svg}g\n",
      "    {http://www.w3.org/2000/svg}defs\n",
      "    {http://www.w3.org/2000/svg}text\n",
      "    {http://www.w3.org/2000/svg}script\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "Tag names might be prefixed with an optional namespace enclosed in a pair of\n",
      "curly braces (`{}`). The default XML namespace appears there, too, when\n",
      "defined. Notice how the swap assignment in the highlighted line made the `<g>`\n",
      "element come before `<defs>`. This shows the mutable nature of the sequence.\n",
      "\n",
      "Here are a few more element attributes and methods that are worth mentioning:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> element = root[0]\n",
      "    \n",
      "    >>> element.tag\n",
      "    '{http://www.inkscape.org/namespaces/inkscape}custom'\n",
      "    \n",
      "    >>> element.text\n",
      "    'Some value'\n",
      "    \n",
      "    >>> element.attrib\n",
      "    {'x': '42', '{http://www.inkscape.org/namespaces/inkscape}z': '555'}\n",
      "    \n",
      "    >>> element.get(\"x\")\n",
      "    '42'\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "One of the benefits of this API is how it uses Python's native data types.\n",
      "Above, it uses a Python dictionary for the element's attributes. In the\n",
      "previous modules, those were wrapped in less convenient adapters. Unlike the\n",
      "DOM, the ElementTree API doesn't expose methods or properties for walking over\n",
      "the tree in any direction, but there are a couple of better alternatives.\n",
      "\n",
      "As you've seen before, instances of the `Element` class implement the\n",
      "**sequence protocol** , letting you iterate over their direct children with a\n",
      "loop:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> for child in root:\n",
      "    ...     print(child.tag)\n",
      "    ...\n",
      "    {http://www.inkscape.org/namespaces/inkscape}custom\n",
      "    {http://www.w3.org/2000/svg}defs\n",
      "    {http://www.w3.org/2000/svg}g\n",
      "    {http://www.w3.org/2000/svg}text\n",
      "    {http://www.w3.org/2000/svg}script\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "You get the sequence of the root's immediate children. To go deeper into\n",
      "nested descendants, however, you'll have to call the `.iter()` method on the\n",
      "ancestor element:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> for descendant in root.iter():\n",
      "    ...     print(descendant.tag)\n",
      "    ...\n",
      "    {http://www.w3.org/2000/svg}svg\n",
      "    {http://www.inkscape.org/namespaces/inkscape}custom\n",
      "    {http://www.w3.org/2000/svg}defs\n",
      "    {http://www.w3.org/2000/svg}linearGradient\n",
      "    {http://www.w3.org/2000/svg}stop\n",
      "    {http://www.w3.org/2000/svg}stop\n",
      "    {http://www.w3.org/2000/svg}stop\n",
      "    {http://www.w3.org/2000/svg}g\n",
      "    {http://www.w3.org/2000/svg}circle\n",
      "    {http://www.w3.org/2000/svg}ellipse\n",
      "    {http://www.w3.org/2000/svg}ellipse\n",
      "    {http://www.w3.org/2000/svg}path\n",
      "    {http://www.w3.org/2000/svg}text\n",
      "    {http://www.w3.org/2000/svg}script\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "The root element has only five children but thirteen descendants in total.\n",
      "It's also possible to narrow down the descendants by **filtering** only\n",
      "specific tag names using an optional `tag` argument:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> tag_name = \"{http://www.w3.org/2000/svg}ellipse\"\n",
      "    >>> for descendant in root.iter(tag_name):\n",
      "    ...     print(descendant)\n",
      "    ...\n",
      "    <Element '{http://www.w3.org/2000/svg}ellipse' at 0x7f430baa03b0>\n",
      "    <Element '{http://www.w3.org/2000/svg}ellipse' at 0x7f430baa0450>\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "This time, you only got two `<ellipse>` elements. Remember to include the\n",
      "**XML namespace** , such as `{http://www.w3.org/2000/svg}`, in your tag name--\n",
      "as long as it's been defined. Otherwise, if you only provide the tag name\n",
      "without the right namespace, you could end up with fewer or more descendant\n",
      "elements than initially anticipated.\n",
      "\n",
      "Dealing with namespaces is more convenient when using `.iterfind()`, which\n",
      "accepts an optional mapping of prefixes to domain names. To indicate the\n",
      "**default namespace** , you can leave the key blank or assign an arbitrary\n",
      "prefix, which must be used in the tag name later:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> namespaces = {\n",
      "    ...     \"\": \"http://www.w3.org/2000/svg\",\n",
      "    ...     \"custom\": \"http://www.w3.org/2000/svg\"\n",
      "    ... }\n",
      "    \n",
      "    >>> for descendant in root.iterfind(\"g\", namespaces):\n",
      "    ...     print(descendant)\n",
      "    ...\n",
      "    <Element '{http://www.w3.org/2000/svg}g' at 0x7f430baa0270>\n",
      "    \n",
      "    >>> for descendant in root.iterfind(\"custom:g\", namespaces):\n",
      "    ...     print(descendant)\n",
      "    ...\n",
      "    <Element '{http://www.w3.org/2000/svg}g' at 0x7f430baa0270>\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "The namespace mapping lets you refer to the same element with different\n",
      "prefixes. Surprisingly, if you try to find those nested `<ellipse>` elements\n",
      "like before, then `.iterfind()` won't return anything because it expects an\n",
      "**XPath expression** rather than a simple tag name:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> for descendant in root.iterfind(\"ellipse\", namespaces):\n",
      "    ...     print(descendant)\n",
      "    ...\n",
      "    \n",
      "    >>> for descendant in root.iterfind(\"g/ellipse\", namespaces):\n",
      "    ...     print(descendant)\n",
      "    ...\n",
      "    <Element '{http://www.w3.org/2000/svg}ellipse' at 0x7f430baa03b0>\n",
      "    <Element '{http://www.w3.org/2000/svg}ellipse' at 0x7f430baa0450>\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "By coincidence, the string `\"g\"` happens to be a valid path relative to the\n",
      "current `root` element, which is why the function returned a non-empty result\n",
      "before. However, to find the ellipses nested one level deeper in the XML\n",
      "hierarchy, you need a more verbose path expression.\n",
      "\n",
      "ElementTree has limited syntax support for the XPath mini-language, which you\n",
      "can use to query elements in XML, similar to CSS selectors in HTML. There are\n",
      "other methods that accept such an expression:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> namespaces = {\"\": \"http://www.w3.org/2000/svg\"}\n",
      "    \n",
      "    >>> root.iterfind(\"defs\", namespaces)\n",
      "    <generator object prepare_child.<locals>.select at 0x7f430ba6d190>\n",
      "    \n",
      "    >>> root.findall(\"defs\", namespaces)\n",
      "    [<Element '{http://www.w3.org/2000/svg}defs' at 0x7f430ba09e00>]\n",
      "    \n",
      "    >>> root.find(\"defs\", namespaces)\n",
      "    <Element '{http://www.w3.org/2000/svg}defs' at 0x7f430ba09e00>\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "While `.iterfind()` yields matching elements lazily, `.findall()` returns a\n",
      "list, and `.find()` returns only the first matching element. Similarly, you\n",
      "can extract text enclosed between the opening and closing tags of elements\n",
      "using `.findtext()` or get the inner text of the entire document with\n",
      "`.itertext()`:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> namespaces = {\"i\": \"http://www.inkscape.org/namespaces/inkscape\"}\n",
      "    \n",
      "    >>> root.findtext(\"i:custom\", namespaces=namespaces)\n",
      "    'Some value'\n",
      "    \n",
      "    >>> for text in root.itertext():\n",
      "    ...     if text.strip() != \"\":\n",
      "    ...         print(text.strip())\n",
      "    ...\n",
      "    Some value\n",
      "    Hello <svg>!\n",
      "    console.log(\"CDATA disables XML parsing: <svg>\")\n",
      "    ⋮\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "You look for text embedded in a specific XML element first, then everywhere in\n",
      "the whole document. Searching by text is a powerful feature of the ElementTree\n",
      "API. It's possible to replicate it using other built-in parsers, but at the\n",
      "cost of increased code complexity and less convenience.\n",
      "\n",
      "The ElementTree API is probably the most intuitive one of them all. It's\n",
      "Pythonic, efficient, robust, and universal. Unless you have a specific reason\n",
      "to use DOM or SAX, this should be your default choice.\n",
      "\n",
      "Remove ads\n",
      "\n",
      "## Explore Third-Party XML Parser Libraries\n",
      "\n",
      "Occasionally, reaching for the XML parsers in the standard library might feel\n",
      "like picking up a sledgehammer to crack a nut. At other times, it's the\n",
      "opposite, and you wish for a parser that could do much more. For example, you\n",
      "might want to validate XML against a schema or use advanced XPath expressions.\n",
      "In those situations, it's best to check out the external libraries available\n",
      "on PyPI.\n",
      "\n",
      "Below, you'll find a selection of external libraries with varying degrees of\n",
      "complexity and sophistication.\n",
      "\n",
      "### `untangle`: Convert XML to a Python Object\n",
      "\n",
      "If you're looking for a one-liner that could turn your XML document into a\n",
      "Python object, then look no further. While it hasn't been updated in a few\n",
      "years, the `untangle` library might soon become your favorite way of parsing\n",
      "XML in Python. There's only one function to remember, and it accepts a URL, a\n",
      "filename, a file object, or an XML string:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> import untangle\n",
      "    \n",
      "    >>> # Parse XML from a URL\n",
      "    >>> untangle.parse(\"http://localhost:8000/smiley.svg\")\n",
      "    Element(name = None, attributes = None, cdata = )\n",
      "    \n",
      "    >>> # Parse XML from a filename\n",
      "    >>> untangle.parse(\"smiley.svg\")\n",
      "    Element(name = None, attributes = None, cdata = )\n",
      "    \n",
      "    >>> # Parse XML from a file object\n",
      "    >>> with open(\"smiley.svg\") as file:\n",
      "    ...     untangle.parse(file)\n",
      "    ...\n",
      "    Element(name = None, attributes = None, cdata = )\n",
      "    \n",
      "    >>> # Parse XML from a Python string\n",
      "    >>> untangle.parse(\"\"\"\\\n",
      "    ... <svg viewBox=\"-105 -100 210 270\">\n",
      "    ...   <!-- More content goes here... -->\n",
      "    ... </svg>\n",
      "    ... \"\"\")\n",
      "    Element(name = None, attributes = None, cdata = )\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "In each case, it returns an instance of the `Element` class. You can use the\n",
      "**dot operator** to access its children and the **square bracket** syntax to\n",
      "get XML attributes or one of the child nodes by index. To get the document's\n",
      "root element, for example, you can access it as if it was the object's\n",
      "property. To get one of the element's XML attributes, you may pass its name as\n",
      "a dictionary key:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> import untangle\n",
      "    >>> document = untangle.parse(\"smiley.svg\")\n",
      "    \n",
      "    >>> document.svg\n",
      "    Element(name = svg, attributes = {'xmlns': ...}, ...)\n",
      "    \n",
      "    >>> document.svg[\"viewBox\"]\n",
      "    '-105 -100 210 270'\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "There are no function or method names to remember. Instead, each parsed object\n",
      "is unique, so you really need to know the underlying XML document's structure\n",
      "to traverse it with `untangle`.\n",
      "\n",
      "To find out what the root element's name is, call `dir()` on the document:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> dir(document)\n",
      "    ['svg']\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "This reveals the names of the element's immediate children. Note that\n",
      "`untangle` redefines the meaning of `dir()` for its parsed documents. Usually,\n",
      "you call this built-in function to inspect a class or a Python module. The\n",
      "default implementation would return a list of attribute names rather than the\n",
      "child elements of an XML document.\n",
      "\n",
      "If there's more than one child with the given tag name, then you can iterate\n",
      "over them with a loop or refer to one by index:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> dir(document.svg)\n",
      "    ['defs', 'g', 'inkscape_custom', 'script', 'text']\n",
      "    \n",
      "    >>> dir(document.svg.defs.linearGradient)\n",
      "    ['stop', 'stop', 'stop']\n",
      "    \n",
      "    >>> for stop in document.svg.defs.linearGradient.stop:\n",
      "    ...     print(stop)\n",
      "    ...\n",
      "    Element <stop> with attributes {'offset': ...}, ...\n",
      "    Element <stop> with attributes {'offset': ...}, ...\n",
      "    Element <stop> with attributes {'offset': ...}, ...\n",
      "    \n",
      "    >>> document.svg.defs.linearGradient.stop[1]\n",
      "    Element(name = stop, attributes = {'offset': ...}, ...)\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "You might have noticed that the `<inkscape:custom>` element was renamed to\n",
      "`inkscape_custom`. Unfortunately, the library can't handle **XML namespaces**\n",
      "well, so if that's something you need to rely on, then you must look\n",
      "elsewhere.\n",
      "\n",
      "Because of the dot notation, element names in XML documents must be valid\n",
      "Python identifiers. If they're not, then `untangle` will automatically rewrite\n",
      "their names by replacing forbidden characters with an underscore:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> dir(untangle.parse(\"<com:company.web-app></com:company.web-app>\"))\n",
      "    ['com_company_web_app']\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "Children's tag names aren't the only object properties you can access.\n",
      "Elements have a few predefined object attributes that might be shown by\n",
      "calling `vars()`:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> element = document.svg.text\n",
      "    \n",
      "    >>> list(vars(element).keys())\n",
      "    ['_name', '_attributes', 'children', 'is_root', 'cdata']\n",
      "    \n",
      "    >>> element._name\n",
      "    'text'\n",
      "    \n",
      "    >>> element._attributes\n",
      "    {'x': '-40', 'y': '75'}\n",
      "    \n",
      "    >>> element.children\n",
      "    []\n",
      "    \n",
      "    >>> element.is_root\n",
      "    False\n",
      "    \n",
      "    >>> element.cdata\n",
      "    'Hello <svg>!'\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "Behind the scenes, `untangle` uses the built-in SAX parser, but because the\n",
      "library is implemented in pure Python and creates lots of heavyweight objects,\n",
      "it has considerably **poor performance**. While it's intended for reading tiny\n",
      "documents, you can still combine it with another approach to read multi-\n",
      "gigabyte XML files.\n",
      "\n",
      "Here's how. If you head over to Wikipedia archives, you can download one of\n",
      "their compressed XML files. The one at the top should contain a snapshot of\n",
      "the articles' abstracts:\n",
      "\n",
      "XML\n",
      "\n",
      "    \n",
      "    \n",
      "    <feed>\n",
      "      <doc>\n",
      "        <title>Wikipedia: Anarchism</title>\n",
      "        <url>https://en.wikipedia.org/wiki/Anarchism</url>\n",
      "        <abstract>Anarchism is a political philosophy...</abstract>\n",
      "        <links>\n",
      "          <sublink linktype=\"nav\">\n",
      "            <anchor>Etymology, terminology and definition</anchor>\n",
      "            <link>https://en.wikipedia.org/wiki/Anarchism#Etymology...</link>\n",
      "          </sublink>\n",
      "          <sublink linktype=\"nav\">\n",
      "            <anchor>History</anchor>\n",
      "            <link>https://en.wikipedia.org/wiki/Anarchism#History</link>\n",
      "          </sublink>\n",
      "          ⋮\n",
      "        </links>\n",
      "      </doc>\n",
      "      ⋮\n",
      "    </feed>\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "It's over 6 GB in size after download, which is perfect for this exercise. The\n",
      "idea is to scan the file to find the consecutive opening and closing `<doc>`\n",
      "tags and then parse the XML fragment between them using `untangle` for\n",
      "convenience.\n",
      "\n",
      "The built-in `mmap` module lets you create a **virtual view** of the file\n",
      "contents, even when it doesn't fit the available memory. This gives an\n",
      "impression of working with a huge string of bytes that supports searching and\n",
      "the regular slicing syntax. If you're interested in how to encapsulate this\n",
      "logic in a Python class and take advantage of a generator for lazy evaluation,\n",
      "then expand the collapsible section below.\n",
      "\n",
      "A Hybrid Approach to Parsing XMLShow/Hide\n",
      "\n",
      "Here's the complete code of the `XMLTagStream` class:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    import mmap\n",
      "    import untangle\n",
      "    \n",
      "    class XMLTagStream:\n",
      "        def __init__(self, path, tag_name, encoding=\"utf-8\"):\n",
      "            self.file = open(path)\n",
      "            self.stream = mmap.mmap(\n",
      "                self.file.fileno(), 0, access=mmap.ACCESS_READ\n",
      "            )\n",
      "            self.tag_name = tag_name\n",
      "            self.encoding = encoding\n",
      "            self.start_tag = f\"<{tag_name}>\".encode(encoding)\n",
      "            self.end_tag = f\"</{tag_name}>\".encode(encoding)\n",
      "    \n",
      "        def __enter__(self):\n",
      "            return self\n",
      "    \n",
      "        def __exit__(self, *args, **kwargs):\n",
      "            self.stream.close()\n",
      "            self.file.close()\n",
      "    \n",
      "        def __iter__(self):\n",
      "            end = 0\n",
      "            while (begin := self.stream.find(self.start_tag, end)) != -1:\n",
      "                end = self.stream.find(self.end_tag, begin)\n",
      "                yield self.parse(self.stream[begin: end + len(self.end_tag)])\n",
      "    \n",
      "        def parse(self, chunk):\n",
      "            document = untangle.parse(chunk.decode(self.encoding))\n",
      "            return getattr(document, self.tag_name)\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "It's a custom context manager, which uses the **iterator protocol** defined as\n",
      "an inline **generator function**. The resulting generator object loops over\n",
      "the XML document as if it was a long stream of characters.\n",
      "\n",
      "Note that the `while` loop takes advantage of fairly new Python syntax, the\n",
      "walrus operator (`:=`), to simplify the code. You can use this operator in\n",
      "**assignment expressions** , where an expression can be evaluated and assigned\n",
      "to a variable.\n",
      "\n",
      "Without getting into the nitty-gritty details, here's how you can use this\n",
      "custom class to go through a big XML file quickly while inspecting specific\n",
      "elements more thoroughly with `untangle`:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> with XMLTagStream(\"abstract.xml\", \"doc\") as stream:\n",
      "    ...     for doc in stream:\n",
      "    ...         print(doc.title.cdata.center(50, \"=\"))\n",
      "    ...         for sublink in doc.links.sublink:\n",
      "    ...             print(\"-\", sublink.anchor.cdata)\n",
      "    ...         if \"q\" == input(\"Press [q] to exit or any key to continue...\"):\n",
      "    ...             break\n",
      "    ...\n",
      "    ===============Wikipedia: Anarchism===============\n",
      "    - Etymology, terminology and definition\n",
      "    - History\n",
      "    - Pre-modern era\n",
      "    ⋮\n",
      "    Press [q] to exit or any key to continue...\n",
      "    ================Wikipedia: Autism=================\n",
      "    - Characteristics\n",
      "    - Social development\n",
      "    - Communication\n",
      "    ⋮\n",
      "    Press [q] to exit or any key to continue...\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "First, you open a file for reading and indicate the tag name that you want to\n",
      "find. Then, you iterate over those elements and receive a parsed fragment of\n",
      "the XML document. It's almost like looking through a tiny window moving over\n",
      "an infinitely long sheet of paper. That's a relatively surface-level example\n",
      "that ignores a few details, but it should give you a general idea of how to\n",
      "use such a hybrid parsing strategy.\n",
      "\n",
      "Remove ads\n",
      "\n",
      "### `xmltodict`: Convert XML to a Python Dictionary\n",
      "\n",
      "If you like JSON but you're not a fan of XML, then check out `xmltodict`,\n",
      "which tries to bridge the gap between both data formats. As the name implies,\n",
      "the library can parse an XML document and represent it as a Python dictionary,\n",
      "which also happens to be the target data type for JSON documents in Python.\n",
      "This makes **conversion between XML and JSON** possible.\n",
      "\n",
      "**Note:** Dictionaries are made up of key-value pairs, while XML documents are\n",
      "inherently hierarchical, which may lead to some information loss during the\n",
      "conversion. On top of that, XML has attributes, comments, processing\n",
      "instructions, and other ways of defining metadata that aren't available in\n",
      "dictionaries.\n",
      "\n",
      "Unlike the rest of the XML parsers so far, this one expects either a Python\n",
      "string or a file-like object open for reading in _binary_ mode:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> import xmltodict\n",
      "    \n",
      "    >>> xmltodict.parse(\"\"\"\\\n",
      "    ... <svg viewBox=\"-105 -100 210 270\">\n",
      "    ...   <!-- More content goes here... -->\n",
      "    ... </svg>\n",
      "    ... \"\"\")\n",
      "    OrderedDict([('svg', OrderedDict([('@viewBox', '-105 -100 210 270')]))])\n",
      "    \n",
      "    >>> with open(\"smiley.svg\", \"rb\") as file:\n",
      "    ...     xmltodict.parse(file)\n",
      "    ...\n",
      "    OrderedDict([('svg', ...)])\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "By default, the library returns an instance of the `OrderedDict` collection to\n",
      "retain **element order**. However, starting from Python 3.6, plain\n",
      "dictionaries also keep the insertion order. If you'd like to work with regular\n",
      "dictionaries instead, then pass `dict` as the `dict_constructor` argument to\n",
      "the `parse()` function:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> import xmltodict\n",
      "    \n",
      "    >>> with open(\"smiley.svg\", \"rb\") as file:\n",
      "    ...     xmltodict.parse(file, dict_constructor=dict)\n",
      "    ...\n",
      "    {'svg': ...}\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "Now, `parse()` returns a plain old dictionary with a familiar textual\n",
      "representation.\n",
      "\n",
      "To avoid **name conflicts** between XML elements and their attributes, the\n",
      "library automatically prefixes the latter with an `@` character. You may also\n",
      "ignore attributes completely by setting the `xml_attribs` flag appropriately:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> import xmltodict\n",
      "    \n",
      "    >>> # Rename attributes by default\n",
      "    >>> with open(\"smiley.svg\", \"rb\") as file:\n",
      "    ...     document = xmltodict.parse(file)\n",
      "    ...     print([x for x in document[\"svg\"] if x.startswith(\"@\")])\n",
      "    ...\n",
      "    ['@xmlns', '@xmlns:inkscape', '@viewBox', '@width', '@height']\n",
      "    \n",
      "    >>> # Ignore attributes when requested\n",
      "    >>> with open(\"smiley.svg\", \"rb\") as file:\n",
      "    ...     document = xmltodict.parse(file, xml_attribs=False)\n",
      "    ...     print([x for x in document[\"svg\"] if x.startswith(\"@\")])\n",
      "    ...\n",
      "    []\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "Yet another piece of information that gets ignored by default is the **XML\n",
      "namespace** declaration. These are treated like regular attributes, while the\n",
      "corresponding prefixes become part of the tag name. However, you can expand,\n",
      "rename, or skip some of the namespaces if you want to:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> import xmltodict\n",
      "    \n",
      "    >>> # Ignore namespaces by default\n",
      "    >>> with open(\"smiley.svg\", \"rb\") as file:\n",
      "    ...     document = xmltodict.parse(file)\n",
      "    ...     print(document.keys())\n",
      "    ...\n",
      "    odict_keys(['svg'])\n",
      "    \n",
      "    >>> # Process namespaces when requested\n",
      "    >>> with open(\"smiley.svg\", \"rb\") as file:\n",
      "    ...     document = xmltodict.parse(file, process_namespaces=True)\n",
      "    ...     print(document.keys())\n",
      "    ...\n",
      "    odict_keys(['http://www.w3.org/2000/svg:svg'])\n",
      "    \n",
      "    >>> # Rename and skip some namespaces\n",
      "    >>> namespaces = {\n",
      "    ...     \"http://www.w3.org/2000/svg\": \"svg\",\n",
      "    ...     \"http://www.inkscape.org/namespaces/inkscape\": None,\n",
      "    ... }\n",
      "    >>> with open(\"smiley.svg\", \"rb\") as file:\n",
      "    ...     document = xmltodict.parse(\n",
      "    ...         file, process_namespaces=True, namespaces=namespaces\n",
      "    ...     )\n",
      "    ...     print(document.keys())\n",
      "    ...     print(\"custom\" in document[\"svg:svg\"])\n",
      "    ...     print(\"inkscape:custom\" in document[\"svg:svg\"])\n",
      "    ...\n",
      "    odict_keys(['svg:svg'])\n",
      "    True\n",
      "    False\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "In the first example above, tag names don't include the XML namespace prefix.\n",
      "In the second example, they do because you requested to process them. Finally,\n",
      "in the third example, you collapsed the default namespace to `svg` while\n",
      "suppressing Inkscape's namespace with `None`.\n",
      "\n",
      "The default string representation of a Python dictionary might not be legible\n",
      "enough. To improve its presentation, you can pretty-print it or convert it to\n",
      "another format such as **JSON** or **YAML** :\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> import xmltodict\n",
      "    >>> with open(\"smiley.svg\", \"rb\") as file:\n",
      "    ...     document = xmltodict.parse(file, dict_constructor=dict)\n",
      "    ...\n",
      "    \n",
      "    >>> from pprint import pprint as pp\n",
      "    >>> pp(document)\n",
      "    {'svg': {'@height': '270',\n",
      "             '@viewBox': '-105 -100 210 270',\n",
      "             '@width': '210',\n",
      "             '@xmlns': 'http://www.w3.org/2000/svg',\n",
      "             '@xmlns:inkscape': 'http://www.inkscape.org/namespaces/inkscape',\n",
      "             'defs': {'linearGradient': {'@id': 'skin',\n",
      "             ⋮\n",
      "    \n",
      "    >>> import json\n",
      "    >>> print(json.dumps(document, indent=4, sort_keys=True))\n",
      "    {\n",
      "        \"svg\": {\n",
      "            \"@height\": \"270\",\n",
      "            \"@viewBox\": \"-105 -100 210 270\",\n",
      "            \"@width\": \"210\",\n",
      "            \"@xmlns\": \"http://www.w3.org/2000/svg\",\n",
      "            \"@xmlns:inkscape\": \"http://www.inkscape.org/namespaces/inkscape\",\n",
      "            \"defs\": {\n",
      "                \"linearGradient\": {\n",
      "                 ⋮\n",
      "    \n",
      "    >>> import yaml  # Install first with 'pip install PyYAML'\n",
      "    >>> print(yaml.dump(document))\n",
      "    svg:\n",
      "      '@height': '270'\n",
      "      '@viewBox': -105 -100 210 270\n",
      "      '@width': '210'\n",
      "      '@xmlns': http://www.w3.org/2000/svg\n",
      "      '@xmlns:inkscape': http://www.inkscape.org/namespaces/inkscape\n",
      "      defs:\n",
      "        linearGradient:\n",
      "        ⋮\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "The `xmltodict` library allows for converting the document the other way\n",
      "around--that is, from a Python dictionary back to an XML string:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> import xmltodict\n",
      "    \n",
      "    >>> with open(\"smiley.svg\", \"rb\") as file:\n",
      "    ...     document = xmltodict.parse(file, dict_constructor=dict)\n",
      "    ...\n",
      "    \n",
      "    >>> xmltodict.unparse(document)\n",
      "    '<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<svg...'\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "The dictionary may come in handy as an intermediate format when converting\n",
      "data from JSON or YAML to XML, should there be such a need.\n",
      "\n",
      "There are a bunch more features in the `xmltodict` library, such as streaming,\n",
      "so feel free to explore them on your own. However, this library is a bit dated\n",
      "too. Besides, it's the next library that should be on your radar if you're\n",
      "really seeking advanced XML parsing features.\n",
      "\n",
      "### `lxml`: Use ElementTree on Steroids\n",
      "\n",
      "If you want the best performance, the broadest spectrum of functionality, and\n",
      "the most familiar interface all wrapped in one package, then install `lxml`\n",
      "and forget about the rest of the libraries. It's a **Python binding** for the\n",
      "C libraries libxml2 and libxslt, which support several standards, including\n",
      "XPath, XML Schema, and XSLT.\n",
      "\n",
      "The library is compatible with Python's **ElementTree API** , which you\n",
      "learned about earlier in this tutorial. That means you can reuse your existing\n",
      "code by replacing only a single import statement:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    import lxml.etree as ET\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "This will give you a great **performance boost**. On top of that, the `lxml`\n",
      "library comes with an extensive set of features and provides different ways of\n",
      "using them. For example, it lets you **validate** your XML documents against\n",
      "several schema languages, one of which is the XML Schema Definition:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> import lxml.etree as ET\n",
      "    \n",
      "    >>> xml_schema = ET.XMLSchema(\n",
      "    ...     ET.fromstring(\"\"\"\\\n",
      "    ...         <xsd:schema xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\">\n",
      "    ...             <xsd:element name=\"parent\"/>\n",
      "    ...             <xsd:complexType name=\"SomeType\">\n",
      "    ...                 <xsd:sequence>\n",
      "    ...                     <xsd:element name=\"child\" type=\"xsd:string\"/>\n",
      "    ...                 </xsd:sequence>\n",
      "    ...             </xsd:complexType>\n",
      "    ...         </xsd:schema>\"\"\"))\n",
      "    \n",
      "    >>> valid = ET.fromstring(\"<parent><child></child></parent>\")\n",
      "    >>> invalid = ET.fromstring(\"<child><parent></parent></child>\")\n",
      "    \n",
      "    >>> xml_schema.validate(valid)\n",
      "    True\n",
      "    \n",
      "    >>> xml_schema.validate(invalid)\n",
      "    False\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "None of the XML parsers in Python's standard library have the capability to\n",
      "validate documents. Meanwhile, `lxml` lets you define an `XMLSchema` object\n",
      "and run documents through it while remaining largely compatible with the\n",
      "ElementTree API.\n",
      "\n",
      "Besides the ElementTree API, `lxml` supports an alternative lxml.objectify\n",
      "interface, which you'll cover later in the data binding section.\n",
      "\n",
      "Remove ads\n",
      "\n",
      "### `BeautifulSoup`: Deal With Malformed XML\n",
      "\n",
      "You won't typically use the last library in this comparison for parsing XML\n",
      "since you mostly encounter it web scraping HTML documents. That said, it's\n",
      "capable of parsing XML just as well. BeautifulSoup comes with a **pluggable\n",
      "architecture** that lets you choose the underlying parser. The `lxml` one\n",
      "described earlier is actually recommended by the official documentation and is\n",
      "currently the only XML parser supported by the library.\n",
      "\n",
      "Depending on the kind of documents you'll want to parse, the desired\n",
      "efficiency, and feature availability, you can select one of these parsers:\n",
      "\n",
      "Document Type | Parser Name | Python Library | Speed  \n",
      "---|---|---|---  \n",
      "HTML | `\"html.parser\"` | - | Moderate  \n",
      "HTML | `\"html5lib\"` | `html5lib` | Slow  \n",
      "HTML | `\"lxml\"` | `lxml` | Fast  \n",
      "XML | `\"lxml-xml\"` or `\"xml\"` | `lxml` | Fast  \n",
      "  \n",
      "Other than speed, there are noticeable differences between the individual\n",
      "parsers. For example, some of them are more forgiving than others when it\n",
      "comes to malformed elements, while others emulate web browsers better.\n",
      "\n",
      "**Fun Fact:** The library's name refers to the tag soup, which describes\n",
      "syntactically or structurally incorrect HTML code.\n",
      "\n",
      "Assuming you've already installed the `lxml` and `beautifulsoup4` libraries\n",
      "into your active virtual environment, you can start parsing XML documents\n",
      "right away. You only need to import `BeautifulSoup`:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    from bs4 import BeautifulSoup\n",
      "    \n",
      "    # Parse XML from a file object\n",
      "    with open(\"smiley.svg\") as file:\n",
      "        soup = BeautifulSoup(file, features=\"lxml-xml\")\n",
      "    \n",
      "    # Parse XML from a Python string\n",
      "    soup = BeautifulSoup(\"\"\"\\\n",
      "    <svg viewBox=\"-105 -100 210 270\">\n",
      "      <!-- More content goes here... -->\n",
      "    </svg>\n",
      "    \"\"\", features=\"lxml-xml\")\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "If you accidentally specified a different parser, say `lxml`, then the library\n",
      "would add missing HTML tags such as `<body>` to the parsed document for you.\n",
      "That probably isn't what you intended in this case, so be careful when\n",
      "specifying the parser name.\n",
      "\n",
      "BeautifulSoup is a powerful tool for parsing XML documents because it can\n",
      "**handle invalid content** and it has a **rich API** for extracting\n",
      "information. Have a look at how it copes with incorrectly nested tags,\n",
      "forbidden characters, and badly placed text:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> from bs4 import BeautifulSoup\n",
      "    \n",
      "    >>> soup = BeautifulSoup(\"\"\"\\\n",
      "    ... <parent>\n",
      "    ...     <child>Forbidden < character </parent>\n",
      "    ...     </child>\n",
      "    ... ignored\n",
      "    ... \"\"\", features=\"lxml-xml\")\n",
      "    \n",
      "    >>> print(soup.prettify())\n",
      "    <?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
      "    <parent>\n",
      "     <child>\n",
      "      Forbidden\n",
      "     </child>\n",
      "    </parent>\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "A different parser would raise an exception and surrender as soon as it\n",
      "detected something wrong with the document. Here, not only did it ignore the\n",
      "problems, but it also figured out sensible ways to fix some of them. The\n",
      "elements are properly nested now and have no invalid content.\n",
      "\n",
      "There are way too many methods of locating elements with BeautifulSoup to\n",
      "cover them all here. Usually, you'll call a variant of `.find()` or\n",
      "`.findall()` on the soup element:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> from bs4 import BeautifulSoup\n",
      "    \n",
      "    >>> with open(\"smiley.svg\") as file:\n",
      "    ...     soup = BeautifulSoup(file, features=\"lxml-xml\")\n",
      "    ...\n",
      "    \n",
      "    >>> soup.find_all(\"ellipse\", limit=1)\n",
      "    [<ellipse cx=\"-20\" cy=\"-10\" fill=\"black\" rx=\"6\" ry=\"8\" stroke=\"none\"/>]\n",
      "    \n",
      "    >>> soup.find(x=42)\n",
      "    <inkscape:custom inkscape:z=\"555\" x=\"42\">Some value</inkscape:custom>\n",
      "    \n",
      "    >>> soup.find(\"stop\", {\"stop-color\": \"gold\"})\n",
      "    <stop offset=\"75%\" stop-color=\"gold\" stop-opacity=\"1.0\"/>\n",
      "    \n",
      "    >>> soup.find(text=lambda x: \"value\" in x).parent\n",
      "    <inkscape:custom inkscape:z=\"555\" x=\"42\">Some value</inkscape:custom>\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "The `limit` parameter is similar to the `LIMIT` clause in MySQL, which lets\n",
      "you decide how many results you want to receive at most. It will return the\n",
      "specified number of results or fewer. That's no coincidence. You can think of\n",
      "these search methods as being a simple query language with powerful filters.\n",
      "\n",
      "The search interface is very flexible but is outside the scope of this\n",
      "tutorial. You can check the library's documentation for more details or read\n",
      "yet another tutorial about web scraping in Python that touches on\n",
      "BeautifulSoup.\n",
      "\n",
      "## Bind XML Data to Python Objects\n",
      "\n",
      "Say you want to consume a real-time data feed over a low-latency WebSocket\n",
      "connection with messages exchanged in XML format. For the purposes of this\n",
      "presentation, you're going to use a web browser to broadcast your mouse and\n",
      "keyboard events to the Python server. You'll build a **custom protocol** and\n",
      "use **data binding** to translate XML into native Python objects.\n",
      "\n",
      "The idea behind data binding is to define a data model _declaratively_ while\n",
      "letting the program figure out how to extract a valuable piece of information\n",
      "from the XML at runtime. If you've ever worked with Django models, then this\n",
      "concept should sound familiar.\n",
      "\n",
      "First, begin by designing your data model. It's going to consist of two types\n",
      "of events:\n",
      "\n",
      "  1. `KeyboardEvent`\n",
      "  2. `MouseEvent`\n",
      "\n",
      "Each can represent a few specialized subtypes, like a keypress or key release\n",
      "for the keyboard and a click or right-click for the mouse. Here's a sample XML\n",
      "message produced in response to holding down the `Shift`+`2` key combination:\n",
      "\n",
      "XML\n",
      "\n",
      "    \n",
      "    \n",
      "    <KeyboardEvent>\n",
      "        <Type>keydown</Type>\n",
      "        <Timestamp>253459.17999999982</Timestamp>\n",
      "        <Key>\n",
      "            <Code>Digit2</Code>\n",
      "            <Unicode>@</Unicode>\n",
      "        </Key>\n",
      "        <Modifiers>\n",
      "            <Alt>false</Alt>\n",
      "            <Ctrl>false</Ctrl>\n",
      "            <Shift>true</Shift>\n",
      "            <Meta>false</Meta>\n",
      "        </Modifiers>\n",
      "    </KeyboardEvent>\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "This message contains a specific keyboard event type, a timestamp, the key\n",
      "code and its Unicode, as well as the modifier keys such as `Alt`, `Ctrl`, or\n",
      "`Shift`. The meta key is usually the `Win` or `Cmd` key, depending on your\n",
      "keyboard layout.\n",
      "\n",
      "Similarly, a mouse event could look like this:\n",
      "\n",
      "XML\n",
      "\n",
      "    \n",
      "    \n",
      "    <MouseEvent>\n",
      "        <Type>mousemove</Type>\n",
      "        <Timestamp>52489.07000000145</Timestamp>\n",
      "        <Cursor>\n",
      "            <Delta x=\"-4\" y=\"8\"/>\n",
      "            <Window x=\"171\" y=\"480\"/>\n",
      "            <Screen x=\"586\" y=\"690\"/>\n",
      "        </Cursor>\n",
      "        <Buttons bitField=\"0\"/>\n",
      "        <Modifiers>\n",
      "            <Alt>false</Alt>\n",
      "            <Ctrl>true</Ctrl>\n",
      "            <Shift>false</Shift>\n",
      "            <Meta>false</Meta>\n",
      "        </Modifiers>\n",
      "    </MouseEvent>\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "Instead of the key, however, there's the mouse cursor position and a bit field\n",
      "encoding the mouse buttons pressed during the event. A bit field of zero\n",
      "indicates that no button was pressed.\n",
      "\n",
      "As soon as a client makes the connection, it will start flooding the server\n",
      "with messages. The protocol won't consist of any handshakes, heartbeats,\n",
      "graceful shutdowns, topic subscriptions, or control messages. You can code\n",
      "this in JavaScript by registering event handlers and creating a `WebSocket`\n",
      "object in less than fifty lines of code.\n",
      "\n",
      "However, implementing the client isn't the point of this exercise. Since you\n",
      "don't need to understand it, just expand the collapsible section below to\n",
      "reveal the HTML code with embedded JavaScript and save it in a file named\n",
      "whatever you like.\n",
      "\n",
      "A WebSocket Client in JavaScript and HTMLShow/Hide\n",
      "\n",
      "HTML\n",
      "\n",
      "    \n",
      "    \n",
      "    <!DOCTYPE html>\n",
      "    <html>\n",
      "    <head>\n",
      "        <meta charset=\"utf-8\">\n",
      "        <title>Real-Time Data Feed</title>\n",
      "    </head>\n",
      "    <body>\n",
      "        <script>\n",
      "            const ws = new WebSocket(\"ws://localhost:8000\")\n",
      "            ws.onopen = event => {\n",
      "                [\"keydown\", \"keyup\"].forEach(name =>\n",
      "                    window.addEventListener(name, event =>\n",
      "                        ws.send(`\\\n",
      "    <KeyboardEvent>\n",
      "        <Type>${event.type}</Type>\n",
      "        <Timestamp>${event.timeStamp}</Timestamp>\n",
      "        <Key>\n",
      "            <Code>${event.code}</Code>\n",
      "            <Unicode>${event.key}</Unicode>\n",
      "        </Key>\n",
      "        <Modifiers>\n",
      "            <Alt>${event.altKey}</Alt>\n",
      "            <Ctrl>${event.ctrlKey}</Ctrl>\n",
      "            <Shift>${event.shiftKey}</Shift>\n",
      "            <Meta>${event.metaKey}</Meta>\n",
      "        </Modifiers>\n",
      "    </KeyboardEvent>`))\n",
      "                );\n",
      "                [\"mousedown\", \"mouseup\", \"mousemove\"].forEach(name =>\n",
      "                    window.addEventListener(name, event =>\n",
      "                        ws.send(`\\\n",
      "    <MouseEvent>\n",
      "        <Type>${event.type}</Type>\n",
      "        <Timestamp>${event.timeStamp}</Timestamp>\n",
      "        <Cursor>\n",
      "            <Delta x=\"${event.movementX}\" y=\"${event.movementY}\"/>\n",
      "            <Window x=\"${event.clientX}\" y=\"${event.clientY}\"/>\n",
      "            <Screen x=\"${event.screenX}\" y=\"${event.screenY}\"/>\n",
      "        </Cursor>\n",
      "        <Buttons bitField=\"${event.buttons}\"/>\n",
      "        <Modifiers>\n",
      "            <Alt>${event.altKey}</Alt>\n",
      "            <Ctrl>${event.ctrlKey}</Ctrl>\n",
      "            <Shift>${event.shiftKey}</Shift>\n",
      "            <Meta>${event.metaKey}</Meta>\n",
      "        </Modifiers>\n",
      "    </MouseEvent>`))\n",
      "                )\n",
      "            }\n",
      "        </script>\n",
      "    </body>\n",
      "    </html>\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "The client connects to a local server listening on port 8000. Once you save\n",
      "the HTML code in a file, you'll be able to open it with your favorite web\n",
      "browser. But before that, you'll need to implement the server.\n",
      "\n",
      "Python doesn't come with WebSocket support, but you can install the\n",
      "`websockets` library into your active virtual environment. You're also going\n",
      "to need `lxml` later, so it's a good moment to install both dependencies in\n",
      "one go:\n",
      "\n",
      "Shell\n",
      "\n",
      "    \n",
      "    \n",
      "    $ python -m pip install websockets lxml\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "Finally, you can scaffold a minimal asynchronous web server:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    # server.py\n",
      "    \n",
      "    import asyncio\n",
      "    import websockets\n",
      "    \n",
      "    async def handle_connection(websocket, path):\n",
      "        async for message in websocket:\n",
      "            print(message)\n",
      "    \n",
      "    if __name__ == \"__main__\":\n",
      "        future = websockets.serve(handle_connection, \"localhost\", 8000)\n",
      "        asyncio.get_event_loop().run_until_complete(future)\n",
      "        asyncio.get_event_loop().run_forever()\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "When you start the server and open the saved HTML file in a web browser, you\n",
      "should see XML messages appear in the standard output in response to your\n",
      "mouse moves and key presses. You can open the client in multiple tabs or even\n",
      "multiple browsers simultaneously!\n",
      "\n",
      "Remove ads\n",
      "\n",
      "### Define Models With XPath Expressions\n",
      "\n",
      "Right now, your messages arrive in plain string format. It's not very\n",
      "convenient to work with the messages in this format. Fortunately, you can turn\n",
      "them into compound Python objects with a single line of code using the\n",
      "`lxml.objectify` module:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    # server.py\n",
      "    \n",
      "    import asyncio\n",
      "    import websockets\n",
      "    import lxml.objectify\n",
      "    \n",
      "    async def handle_connection(websocket, path):\n",
      "        async for message in websocket:\n",
      "            try:\n",
      "                xml = lxml.objectify.fromstring(message)\n",
      "            except SyntaxError:\n",
      "                print(\"Malformed XML message:\", repr(message))\n",
      "            else:\n",
      "                if xml.tag == \"KeyboardEvent\":\n",
      "                    if xml.Type == \"keyup\":\n",
      "                        print(\"Key:\", xml.Key.Unicode)\n",
      "                elif xml.tag == \"MouseEvent\":\n",
      "                    screen = xml.Cursor.Screen\n",
      "                    print(\"Mouse:\", screen.get(\"x\"), screen.get(\"y\"))\n",
      "                else:\n",
      "                    print(\"Unrecognized event type\")\n",
      "    \n",
      "    # ...\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "As long as the XML parsing is successful, you can inspect the root element's\n",
      "usual properties, such as the tag name, attributes, inner text, and so on.\n",
      "You'll be able to use the dot operator to navigate deep into the element tree.\n",
      "In most cases, the library will recognize a suitable Python data type and\n",
      "convert the value for you.\n",
      "\n",
      "After saving those changes and restarting the server, you'll need to reload\n",
      "the page in your web browser to make a new WebSocket connection. Here's a\n",
      "sample output of the modified program:\n",
      "\n",
      "Shell\n",
      "\n",
      "    \n",
      "    \n",
      "    $ python server.py\n",
      "    Mouse: 820 121\n",
      "    Mouse: 820 122\n",
      "    Mouse: 820 123\n",
      "    Mouse: 820 124\n",
      "    Mouse: 820 125\n",
      "    Key: a\n",
      "    Mouse: 820 125\n",
      "    Mouse: 820 125\n",
      "    Key: a\n",
      "    Key: A\n",
      "    Key: Shift\n",
      "    Mouse: 821 125\n",
      "    Mouse: 821 125\n",
      "    Mouse: 820 123\n",
      "    ⋮\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "Sometimes, XML may contain tag names that aren't valid Python identifiers, or\n",
      "you might want to adapt the message structure to fit your data model. In such\n",
      "a case, an interesting option would be defining custom **model classes** with\n",
      "descriptors that declare how to look up information using XPath expressions.\n",
      "That's the part that starts to resemble Django models or Pydantic schema\n",
      "definitions.\n",
      "\n",
      "You're going to use a custom `XPath` descriptor and an accompanying `Model`\n",
      "class, which provide reusable properties for your data models. The descriptor\n",
      "expects an XPath expression for element lookup in the received message. The\n",
      "underlying implementation is a bit advanced, so feel free to copy the code\n",
      "from the collapsible section below.\n",
      "\n",
      "XPath Descriptor and the Model ClassShow/Hide\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    import lxml.objectify\n",
      "    \n",
      "    class XPath:\n",
      "        def __init__(self, expression, /, default=None, multiple=False):\n",
      "            self.expression = expression\n",
      "            self.default = default\n",
      "            self.multiple = multiple\n",
      "    \n",
      "        def __set_name__(self, owner, name):\n",
      "            self.attribute_name = name\n",
      "            self.annotation = owner.__annotations__.get(name)\n",
      "    \n",
      "        def __get__(self, instance, owner):\n",
      "            value = self.extract(instance.xml)\n",
      "            instance.__dict__[self.attribute_name] = value\n",
      "            return value\n",
      "    \n",
      "        def extract(self, xml):\n",
      "            elements = xml.xpath(self.expression)\n",
      "            if elements:\n",
      "                if self.multiple:\n",
      "                    if self.annotation:\n",
      "                        return [self.annotation(x) for x in elements]\n",
      "                    else:\n",
      "                        return elements\n",
      "                else:\n",
      "                    first = elements[0]\n",
      "                    if self.annotation:\n",
      "                        return self.annotation(first)\n",
      "                    else:\n",
      "                        return first\n",
      "            else:\n",
      "                return self.default\n",
      "    \n",
      "    class Model:\n",
      "        \"\"\"Abstract base class for your models.\"\"\"\n",
      "        def __init__(self, data):\n",
      "            if isinstance(data, str):\n",
      "                self.xml = lxml.objectify.fromstring(data)\n",
      "            elif isinstance(data, lxml.objectify.ObjectifiedElement):\n",
      "                self.xml = data\n",
      "            else:\n",
      "                raise TypeError(\"Unsupported data type:\", type(data))\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "Assuming you already have the desired `XPath` descriptor and the `Model`\n",
      "abstract base class in your module, you might use them to define\n",
      "`KeyboardEvent` and `MouseEvent` message types along with reusable building\n",
      "blocks to avoid repetition. There are infinite ways to do so, but here's one\n",
      "example:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    # ...\n",
      "    \n",
      "    class Event(Model):\n",
      "        \"\"\"Base class for event messages with common elements.\"\"\"\n",
      "        type_: str = XPath(\"./Type\")\n",
      "        timestamp: float = XPath(\"./Timestamp\")\n",
      "    \n",
      "    class Modifiers(Model):\n",
      "        alt: bool = XPath(\"./Alt\")\n",
      "        ctrl: bool = XPath(\"./Ctrl\")\n",
      "        shift: bool = XPath(\"./Shift\")\n",
      "        meta: bool = XPath(\"./Meta\")\n",
      "    \n",
      "    class KeyboardEvent(Event):\n",
      "        key: str = XPath(\"./Key/Code\")\n",
      "        modifiers: Modifiers = XPath(\"./Modifiers\")\n",
      "    \n",
      "    class MouseEvent(Event):\n",
      "        x: int = XPath(\"./Cursor/Screen/@x\")\n",
      "        y: int = XPath(\"./Cursor/Screen/@y\")\n",
      "        modifiers: Modifiers = XPath(\"./Modifiers\")\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "The `XPath` descriptor allows for **lazy evaluation** so that elements of the\n",
      "XML messages are looked up only when requested. More specifically, they're\n",
      "only looked up when you access a property on the event object. Moreover, the\n",
      "results are **cached** to avoid running the same XPath query more than once.\n",
      "The descriptor also respects type annotations and converts deserialized data\n",
      "to the right Python type automatically.\n",
      "\n",
      "Using those event objects isn't much different from the ones auto-generated by\n",
      "`lxml.objectify` before:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    if xml.tag == \"KeyboardEvent\":\n",
      "        event = KeyboardEvent(xml)\n",
      "        if event.type_ == \"keyup\":\n",
      "            print(\"Key:\", event.key)\n",
      "    elif xml.tag == \"MouseEvent\":\n",
      "        event = MouseEvent(xml)\n",
      "        print(\"Mouse:\", event.x, event.y)\n",
      "    else:\n",
      "        print(\"Unrecognized event type\")\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "There's an additional step of creating new objects of the specific event type.\n",
      "But other than that, it gives you more flexibility in terms of structuring\n",
      "your model independently of the XML protocol. Additionally, it's possible to\n",
      "derive new model attributes based on the ones in the received messages and add\n",
      "more methods on top of that.\n",
      "\n",
      "### Generate Models From an XML Schema\n",
      "\n",
      "Implementing model classes is a tedious and error-prone task. However, as long\n",
      "as your model mirrors the XML messages, you can take advantage of an automated\n",
      "tool to generate the necessary code for you based on XML Schema. The downside\n",
      "of such code is that it's usually less readable than if written by hand.\n",
      "\n",
      "One of the oldest third-party modules to allow that was PyXB, which mimics\n",
      "Java's popular JAXB library. Unfortunately, it was last released several years\n",
      "ago and was targeting legacy Python versions. You can look into a similar yet\n",
      "actively maintained `generateDS` alternative, which generates data structures\n",
      "from XML Schema.\n",
      "\n",
      "Let's say you have this `models.xsd` schema file describing your\n",
      "`KeyboardEvent` message:\n",
      "\n",
      "XML\n",
      "\n",
      "    \n",
      "    \n",
      "    <xsd:schema xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\">\n",
      "        <xsd:element name=\"KeyboardEvent\" type=\"KeyboardEventType\"/>\n",
      "        <xsd:complexType name=\"KeyboardEventType\">\n",
      "            <xsd:sequence>\n",
      "                <xsd:element type=\"xsd:string\" name=\"Type\"/>\n",
      "                <xsd:element type=\"xsd:float\" name=\"Timestamp\"/>\n",
      "                <xsd:element type=\"KeyType\" name=\"Key\"/>\n",
      "                <xsd:element type=\"ModifiersType\" name=\"Modifiers\"/>\n",
      "            </xsd:sequence>\n",
      "        </xsd:complexType>\n",
      "        <xsd:complexType name=\"KeyType\">\n",
      "            <xsd:sequence>\n",
      "                <xsd:element type=\"xsd:string\" name=\"Code\"/>\n",
      "                <xsd:element type=\"xsd:string\" name=\"Unicode\"/>\n",
      "            </xsd:sequence>\n",
      "        </xsd:complexType>\n",
      "        <xsd:complexType name=\"ModifiersType\">\n",
      "            <xsd:sequence>\n",
      "                <xsd:element type=\"xsd:string\" name=\"Alt\"/>\n",
      "                <xsd:element type=\"xsd:string\" name=\"Ctrl\"/>\n",
      "                <xsd:element type=\"xsd:string\" name=\"Shift\"/>\n",
      "                <xsd:element type=\"xsd:string\" name=\"Meta\"/>\n",
      "            </xsd:sequence>\n",
      "        </xsd:complexType>\n",
      "    </xsd:schema>\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "A schema tells the XML parser what elements to expect, their order, and their\n",
      "level in the tree. It also restricts the allowed values for the XML\n",
      "attributes. Any discrepancies between these declarations and an actual XML\n",
      "document should render it invalid and make the parser reject the document.\n",
      "\n",
      "Additionally, some tools can leverage this information to produce a piece of\n",
      "code that hides the details of XML parsing from you. After installing the\n",
      "library, you should be able to run the `generateDS` command in your active\n",
      "virtual environment:\n",
      "\n",
      "Shell\n",
      "\n",
      "    \n",
      "    \n",
      "    $ generateDS -o models.py models.xsd\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "It will create a new file named `models.py` in the same directory with the\n",
      "generated Python source code. You can then import that module and use it to\n",
      "parse the incoming messages:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> from models import parseString\n",
      "    \n",
      "    >>> event = parseString(\"\"\"\\\n",
      "    ... <KeyboardEvent>\n",
      "    ...     <Type>keydown</Type>\n",
      "    ...     <Timestamp>253459.17999999982</Timestamp>\n",
      "    ...     <Key>\n",
      "    ...         <Code>Digit2</Code>\n",
      "    ...         <Unicode>@</Unicode>\n",
      "    ...     </Key>\n",
      "    ...     <Modifiers>\n",
      "    ...         <Alt>false</Alt>\n",
      "    ...         <Ctrl>false</Ctrl>\n",
      "    ...         <Shift>true</Shift>\n",
      "    ...         <Meta>false</Meta>\n",
      "    ...     </Modifiers>\n",
      "    ... </KeyboardEvent>\"\"\", silence=True)\n",
      "    \n",
      "    >>> event.Type, event.Key.Code\n",
      "    ('keydown', 'Digit2')\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "It looks similar to the `lxml.objectify` example shown earlier. The difference\n",
      "is that using data binding enforces compliance with the schema, whereas\n",
      "`lxml.objectify` produces objects dynamically no matter if they're\n",
      "semantically correct.\n",
      "\n",
      "Remove ads\n",
      "\n",
      "## Defuse the XML Bomb With Secure Parsers\n",
      "\n",
      "The XML parsers in Python's standard library are vulnerable to a host of\n",
      "security threats that can lead to denial-of-service (DoS) or data loss, at\n",
      "best. That isn't their fault, to be fair. They just follow the specification\n",
      "of the XML standard, which is more complicated and powerful than most people\n",
      "know.\n",
      "\n",
      "**Note:** Please be advised that you should use the information you're about\n",
      "to see wisely. You don't want to wind up being the attacker, exposing yourself\n",
      "to legal consequences, or facing lifetime banishment from using a particular\n",
      "service.\n",
      "\n",
      "One of the most common attacks is the **XML Bomb** , also known as the billion\n",
      "laughs attack. The attack exploits **entity expansion** in DTD to blow up the\n",
      "memory and occupy the CPU for as long as possible. All you need to stop an\n",
      "unprotected web server from receiving new traffic are these few lines of XML\n",
      "code:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    import xml.etree.ElementTree as ET\n",
      "    ET.fromstring(\"\"\"\\\n",
      "    <?xml version=\"1.0\"?>\n",
      "    <!DOCTYPE lolz [\n",
      "     <!ENTITY lol \"lol\">\n",
      "     <!ELEMENT lolz (#PCDATA)>\n",
      "     <!ENTITY lol1 \"&lol;&lol;&lol;&lol;&lol;&lol;&lol;&lol;&lol;&lol;\">\n",
      "     <!ENTITY lol2 \"&lol1;&lol1;&lol1;&lol1;&lol1;&lol1;&lol1;&lol1;&lol1;&lol1;\">\n",
      "     <!ENTITY lol3 \"&lol2;&lol2;&lol2;&lol2;&lol2;&lol2;&lol2;&lol2;&lol2;&lol2;\">\n",
      "     <!ENTITY lol4 \"&lol3;&lol3;&lol3;&lol3;&lol3;&lol3;&lol3;&lol3;&lol3;&lol3;\">\n",
      "     <!ENTITY lol5 \"&lol4;&lol4;&lol4;&lol4;&lol4;&lol4;&lol4;&lol4;&lol4;&lol4;\">\n",
      "     <!ENTITY lol6 \"&lol5;&lol5;&lol5;&lol5;&lol5;&lol5;&lol5;&lol5;&lol5;&lol5;\">\n",
      "     <!ENTITY lol7 \"&lol6;&lol6;&lol6;&lol6;&lol6;&lol6;&lol6;&lol6;&lol6;&lol6;\">\n",
      "     <!ENTITY lol8 \"&lol7;&lol7;&lol7;&lol7;&lol7;&lol7;&lol7;&lol7;&lol7;&lol7;\">\n",
      "     <!ENTITY lol9 \"&lol8;&lol8;&lol8;&lol8;&lol8;&lol8;&lol8;&lol8;&lol8;&lol8;\">\n",
      "    ]>\n",
      "    <lolz>&lol9;</lolz>\"\"\")\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "A naïve parser will try to resolve the custom entity `&lol9;` placed in the\n",
      "document root by inspecting the DTD. However, that entity itself refers to\n",
      "another entity several times, which refers to yet another entity, and so\n",
      "forth. When you run the script above, you'll notice something disturbing about\n",
      "your memory and the processing unit:\n",
      "\n",
      "Look how the main memory and the swap partition are exhausted in just a matter\n",
      "of seconds while one of the CPUs works at 100% of its capacity. The recording\n",
      "stops abruptly when the system memory becomes full and then resumes after the\n",
      "Python process gets killed.\n",
      "\n",
      "Another popular type of attack known as XXE takes advantage of **general\n",
      "external entities** to read local files and make network requests.\n",
      "Nevertheless, starting from Python 3.7.1, this feature has been disabled by\n",
      "default to increase security. If you trust your data, then you can tell the\n",
      "SAX parser to process external entities anyway:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> from xml.sax import make_parser\n",
      "    >>> from xml.sax.handler import feature_external_ges\n",
      "    \n",
      "    >>> parser = make_parser()\n",
      "    >>> parser.setFeature(feature_external_ges, True)\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "This parser will be able to read local files on your computer. It may pull\n",
      "usernames on a Unix-like operating system, for example:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> from xml.dom.minidom import parseString\n",
      "    \n",
      "    >>> xml = \"\"\"\\\n",
      "    ... <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "    ... <!DOCTYPE root [\n",
      "    ...     <!ENTITY usernames SYSTEM \"/etc/passwd\">\n",
      "    ... ]>\n",
      "    ... <root>&usernames;</root>\"\"\"\n",
      "    \n",
      "    >>> document = parseString(xml, parser)\n",
      "    >>> print(document.documentElement.toxml())\n",
      "    <root>root:x:0:0:root:/root:/bin/bash\n",
      "    daemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin\n",
      "    bin:x:2:2:bin:/bin:/usr/sbin/nologin\n",
      "    ⋮\n",
      "    realpython:x:1001:1001:Real Python,,,:/home/realpython:/bin/bash\n",
      "    </root>\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "It's perfectly feasible to send that data over the network to a remote server!\n",
      "\n",
      "Now, how can you protect yourself from such attacks? The Python official\n",
      "documentation prominently warns you about the risks of using the built-in XML\n",
      "parsers and recommends switching to an external package in mission-critical\n",
      "applications. While not distributed with Python, `defusedxml` is a **drop-in\n",
      "replacement** for all the parsers in the standard library.\n",
      "\n",
      "The library imposes strict limits and disables a lot of the dangerous XML\n",
      "features. It should stop most of the well-known attacks, including the two\n",
      "just described. To use it, grab the library from PyPI and replace your import\n",
      "statements accordingly:\n",
      "\n",
      "Python\n",
      "\n",
      "    \n",
      "    \n",
      "    >>> import defusedxml.ElementTree as ET\n",
      "    >>> ET.parse(\"bomb.xml\")\n",
      "    Traceback (most recent call last):\n",
      "      ...\n",
      "        raise EntitiesForbidden(name, value, base, sysid, pubid, notation_name)\n",
      "    defusedxml.common.EntitiesForbidden:\n",
      "     EntitiesForbidden(name='lol', system_id=None, public_id=None)\n",
      "    \n",
      "\n",
      "Copied!\n",
      "\n",
      "That's it! Forbidden features won't make it through anymore.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "The XML data format is a mature and surprisingly powerful standard that is\n",
      "still in use today, especially in the enterprise setting. Choosing the right\n",
      "XML parser is crucial in finding the **sweet spot** between performance,\n",
      "security, compliance, and convenience.\n",
      "\n",
      "This tutorial puts a detailed **roadmap** in your hand to navigate the\n",
      "confusing maze of XML parsers in Python. You know where to take the shortcuts\n",
      "and how to avoid dead ends, saving you lots of time.\n",
      "\n",
      "**In this tutorial, you learned how to:**\n",
      "\n",
      "  * Choose the right XML **parsing model**\n",
      "  * Use the XML parsers in the **standard library**\n",
      "  * Use major **XML parsing libraries**\n",
      "  * Parse XML documents declaratively using **data binding**\n",
      "  * Use safe XML parsers to eliminate **security vulnerabilities**\n",
      "\n",
      "Now, you understand the different strategies for parsing XML documents as well\n",
      "as their strengths and weaknesses. With this knowledge, you're able to pick\n",
      "the most suitable XML parser for your specific use case and even **combine**\n",
      "more than one to read multi-gigabyte XML files faster.\n",
      "\n",
      "Mark as Completed\n",
      "\n",
      "Share\n",
      "\n",
      "🐍 Python Tricks 💌\n",
      "\n",
      "Get a short & sweet **Python Trick** delivered to your inbox every couple of\n",
      "days. No spam ever. Unsubscribe any time. Curated by the Real Python team.\n",
      "\n",
      "Send Me Python Tricks »\n",
      "\n",
      "About **Bartosz Zaczyński**\n",
      "\n",
      "Bartosz is a bootcamp instructor, author, and polyglot programmer in love with\n",
      "Python. He helps his students get into software engineering by sharing over a\n",
      "decade of commercial experience in the IT industry.\n",
      "\n",
      "» More about Bartosz\n",
      "\n",
      "* * *\n",
      "\n",
      "_Each tutorial at Real Python is created by a team of developers so that it\n",
      "meets our high quality standards. The team members who worked on this tutorial\n",
      "are:_\n",
      "\n",
      "Aldren\n",
      "\n",
      "David\n",
      "\n",
      "Geir Arne\n",
      "\n",
      "Sadie\n",
      "\n",
      "Master _Real-World Python Skills_ With Unlimited Access to Real Python\n",
      "\n",
      "**Join us and get access to thousands of tutorials, hands-on video courses,\n",
      "and a community of expert  Pythonistas:**\n",
      "\n",
      "Level Up Your Python Skills »\n",
      "\n",
      "Master _Real-World Python Skills_  \n",
      "With Unlimited Access to Real Python\n",
      "\n",
      "**Join us and get access to thousands of tutorials, hands-on video courses,\n",
      "and a community of expert Pythonistas:**\n",
      "\n",
      "Level Up Your Python Skills »\n",
      "\n",
      "What Do You Think?\n",
      "\n",
      "**Rate this article:**\n",
      "\n",
      "LinkedIn Twitter Bluesky Facebook Email\n",
      "\n",
      "What’s your #1 takeaway or favorite thing you learned? How are you going to\n",
      "put your newfound skills to use? Leave a comment below and let us know.\n",
      "\n",
      "**Commenting Tips:** The most useful comments are those written with the goal\n",
      "of learning from or helping out other students. Get tips for asking good\n",
      "questions and get answers to common questions in our support portal.\n",
      "\n",
      "* * *\n",
      "\n",
      "Looking for a real-time conversation? Visit the Real Python Community Chat or\n",
      "join the next \"Office Hours\" Live Q&A; Session. Happy Pythoning!\n",
      "\n",
      "Keep Learning\n",
      "\n",
      "Related Topics: intermediate\n",
      "\n",
      "Related Tutorials:\n",
      "\n",
      "  * Reading and Writing CSV Files in Python\n",
      "  * Unicode & Character Encodings in Python: A Painless Guide\n",
      "  * Python Exceptions: An Introduction\n",
      "  * Working With JSON Data in Python\n",
      "  * Serialize Your Data With Python\n",
      "\n",
      "## Keep reading Real Python by creating a free account or signing in:\n",
      "\n",
      "_Continue »\n",
      "\n",
      "Already have an account? Sign-In\n",
      "\n",
      "Almost there! Complete this form and click the button below to gain instant\n",
      "access:\n",
      "\n",
      "×\n",
      "\n",
      "5 Thoughts On Python Mastery\n",
      "\n",
      "Start the Class »\n",
      "\n",
      "🔒 No spam. We take your privacy seriously.\n",
      "\n",
      "Remove ads\n",
      "\n",
      "© 2012–2025 Real Python ⋅ Newsletter ⋅ Podcast ⋅ YouTube ⋅ Twitter ⋅ Facebook\n",
      "⋅ Instagram ⋅ Python Tutorials ⋅ Search ⋅ Privacy Policy ⋅ Energy Policy ⋅\n",
      "Advertise ⋅ Contact  \n",
      "Happy Pythoning!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for document in documents:\n",
    "    print(document.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99190a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://www.scrapingbee.com/blog/how-to-make-pythons-beautiful-soup-faster-performance/\",\n",
    "    \"https://www.scrapingbee.com/blog/async-scraping-in-python/\",\n",
    "    \"https://docs.python.org/3/library/asyncio.html#module-asyncio\",\n",
    "    \"https://realpython.com/python-xml-parser/#lxml-use-elementtree-on-steroids\",\n",
    "    \"https://github.com/psf/requests-html\"\n",
    "]\n",
    "\n",
    "user_agents = UserAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62b6ab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making use of fake user agents to try to bypass the bot detection\n",
    "# Using lxml which is much faster than html.parser and is useful for both xml and html\n",
    "# Using session for all the requests instead of creating a new one for every request\n",
    "# Making use of asyncio to make the requests faster (in parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d692a395",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_url(session: aiohttp.ClientSession, url: str, user_agent: str) -> str:\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "    async with session.get(url, headers=headers, timeout=10) as response:\n",
    "        content = await response.text()\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a080c914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(html: str):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    # Retrieve the source of the page\n",
    "    soup.title\n",
    "    print(soup.title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6529b4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # Creating a list of tasks\n",
    "        tasks = [fetch_url(session, url, user_agents.random) for url in urls]\n",
    "        \n",
    "        # After creating the tasks, we gather them\n",
    "        htmls = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # Parsing the htmls\n",
    "        for html in htmls:\n",
    "            parse(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ff397c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Tips on How to make Python's Beautiful Soup faster when scraping | ScrapingBee\n",
      "How to use asyncio to scrape websites with Python | ScrapingBee\n",
      "asyncio — Asynchronous I/O — Python 3.13.2 documentation\n",
      "A Roadmap to XML Parsers in Python – Real Python\n",
      "GitHub - psf/requests-html: Pythonic HTML Parsing for Humans™\n",
      "         1093942 function calls (1093327 primitive calls) in 1.220 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        5    0.000    0.000    0.587    0.117 1246109926.py:1(parse)\n",
      "        3    0.000    0.000    0.613    0.204 2585750515.py:1(main)\n",
      "       49    0.000    0.000    0.007    0.000 4132580340.py:1(fetch_url)\n",
      "        5    0.000    0.000    0.000    0.000 <attrs generated init aiohttp.client.ClientTimeout>:1(__init__)\n",
      "     2076    0.001    0.000    0.002    0.000 <frozen _collections_abc>:823(items)\n",
      "     2076    0.000    0.000    0.000    0.000 <frozen _collections_abc>:845(__init__)\n",
      "     2076    0.000    0.000    0.000    0.000 <frozen _collections_abc>:892(__iter__)\n",
      "        7    0.000    0.000    0.000    0.000 <frozen abc>:121(__subclasscheck__)\n",
      "        9    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:1390(_handle_fromlist)\n",
      "       10    0.000    0.000    0.000    0.000 <string>:1(<lambda>)\n",
      "    13142    0.008    0.000    0.052    0.000 __init__.py:1053(handle_endtag)\n",
      "    20304    0.005    0.000    0.008    0.000 __init__.py:1065(handle_data)\n",
      "        5    0.000    0.000    0.000    0.000 __init__.py:108(lookup)\n",
      "        4    0.000    0.000    0.000    0.000 __init__.py:183(dumps)\n",
      "        5    0.000    0.000    0.000    0.000 __init__.py:204(__init__)\n",
      "        5    0.000    0.000    0.434    0.087 __init__.py:209(__init__)\n",
      "        5    0.000    0.000    0.000    0.000 __init__.py:274(initialize_soup)\n",
      "       10    0.000    0.000    0.000    0.000 __init__.py:282(reset)\n",
      "    13147    0.003    0.000    0.003    0.000 __init__.py:290(can_be_empty_element)\n",
      "       10    0.000    0.000    0.000    0.000 __init__.py:311(deprecated_argument)\n",
      "    13142    0.032    0.000    0.140    0.000 __init__.py:382(_replace_cdata_list_attribute_values)\n",
      "        5    0.000    0.000    0.000    0.000 __init__.py:595(__init__)\n",
      "      219    0.000    0.000    0.000    0.000 __init__.py:609(__missing__)\n",
      "    13147    0.002    0.000    0.003    0.000 __init__.py:636(set_up_substitutions)\n",
      "        5    0.000    0.000    0.434    0.087 __init__.py:650(_feed)\n",
      "        5    0.000    0.000    0.000    0.000 __init__.py:666(reset)\n",
      "        5    0.000    0.000    0.000    0.000 __init__.py:669(update)\n",
      "      116    0.000    0.000    0.000    0.000 __init__.py:680(<genexpr>)\n",
      "        5    0.000    0.000    0.000    0.000 __init__.py:721(warn_if_markup_looks_like_xml)\n",
      "    15430    0.012    0.000    0.021    0.000 __init__.py:732(string_container)\n",
      "    13142    0.012    0.000    0.022    0.000 __init__.py:786(popTag)\n",
      "    13147    0.012    0.000    0.016    0.000 __init__.py:809(pushTag)\n",
      "    39475    0.027    0.000    0.110    0.000 __init__.py:826(endData)\n",
      "    15430    0.015    0.000    0.035    0.000 __init__.py:867(object_was_parsed)\n",
      "     6146    0.006    0.000    0.006    0.000 __init__.py:906(_linkage_fixer)\n",
      "    13142    0.016    0.000    0.042    0.000 __init__.py:950(_popToTag)\n",
      "    13142    0.031    0.000    0.276    0.000 __init__.py:987(handle_starttag)\n",
      "        4    0.000    0.000    0.000    0.000 _base.py:328(__init__)\n",
      "      4/2    0.000    0.000    0.000    0.000 _base.py:337(_invoke_callbacks)\n",
      "        5    0.000    0.000    0.000    0.000 _base.py:383(cancelled)\n",
      "        5    0.000    0.000    0.000    0.000 _base.py:393(done)\n",
      "        5    0.000    0.000    0.000    0.000 _base.py:398(__get_result)\n",
      "        4    0.000    0.000    0.000    0.000 _base.py:408(add_done_callback)\n",
      "        5    0.000    0.000    0.000    0.000 _base.py:428(result)\n",
      "        5    0.000    0.000    0.000    0.000 _base.py:463(exception)\n",
      "        4    0.000    0.000    0.000    0.000 _base.py:497(set_running_or_notify_cancel)\n",
      "      4/2    0.000    0.000    0.000    0.000 _base.py:537(set_result)\n",
      "        5    0.000    0.000    0.000    0.000 _lxml.py:110(initialize_soup)\n",
      "        5    0.000    0.000    0.000    0.000 _lxml.py:121(_register_namespaces)\n",
      "        5    0.000    0.000    0.000    0.000 _lxml.py:156(parser_for)\n",
      "        5    0.000    0.000    0.000    0.000 _lxml.py:170(__init__)\n",
      "    41942    0.006    0.000    0.007    0.000 _lxml.py:187(_getNsTag)\n",
      "       10    0.000    0.000    0.000    0.000 _lxml.py:196(prepare_markup)\n",
      "        5    0.000    0.000    0.000    0.000 _lxml.py:309(close)\n",
      "    13142    0.056    0.000    0.382    0.000 _lxml.py:312(start)\n",
      "    13142    0.001    0.000    0.001    0.000 _lxml.py:401(_prefix_for_namespace)\n",
      "    13142    0.017    0.000    0.137    0.000 _lxml.py:412(end)\n",
      "    20282    0.010    0.000    0.020    0.000 _lxml.py:442(data)\n",
      "        5    0.000    0.000    0.000    0.000 _lxml.py:447(doctype)\n",
      "       17    0.000    0.000    0.000    0.000 _lxml.py:454(comment)\n",
      "        5    0.000    0.000    0.000    0.000 _lxml.py:474(default_parser)\n",
      "        5    0.031    0.006    0.434    0.087 _lxml.py:477(feed)\n",
      "        8    0.000    0.000    0.001    0.000 _staggered.py:110(run_one_coro)\n",
      "        8    0.000    0.000    0.000    0.000 _staggered.py:26(_wait_one)\n",
      "        4    0.000    0.000    0.000    0.000 _staggered.py:33(_on_completion)\n",
      "        8    0.000    0.000    0.000    0.000 _staggered.py:47(staggered_race)\n",
      "        4    0.000    0.000    0.000    0.000 _url.py:1284(with_fragment)\n",
      "        5    0.000    0.000    0.000    0.000 _url.py:345(__new__)\n",
      "       14    0.000    0.000    0.000    0.000 _weakrefset.py:39(_remove)\n",
      "       14    0.000    0.000    0.000    0.000 _weakrefset.py:85(add)\n",
      "        1    0.000    0.000    0.000    0.000 abc.py:176(__init__)\n",
      "        3    0.000    0.000    0.001    0.000 asyncio.py:200(_handle_events)\n",
      "        2    0.000    0.000    0.000    0.000 asyncio.py:210(call_at)\n",
      "        1    0.000    0.000    0.000    0.000 asyncio.py:225(add_callback)\n",
      "        9    0.000    0.000    0.000    0.000 attrsettr.py:43(__getattr__)\n",
      "        9    0.000    0.000    0.000    0.000 attrsettr.py:66(_get_attr_opt)\n",
      "        5    0.000    0.000    0.000    0.000 base_events.py:100(_ipaddr_info)\n",
      "       10    0.000    0.000    0.001    0.000 base_events.py:1024(create_connection)\n",
      "       10    0.000    0.000    0.001    0.000 base_events.py:1171(_create_connection_transport)\n",
      "        5    0.000    0.000    0.000    0.000 base_events.py:1457(_ensure_resolved)\n",
      "       51    0.000    0.000    0.000    0.000 base_events.py:1907(_add_callback)\n",
      "       15    0.000    0.000    0.000    0.000 base_events.py:1917(_timer_handle_cancelled)\n",
      "        5    0.000    0.000    0.000    0.000 base_events.py:192(_set_nodelay)\n",
      "        1    0.000    0.000    0.000    0.000 base_events.py:2002(_set_coroutine_origin_tracking)\n",
      "      263    0.000    0.000    0.000    0.000 base_events.py:2017(get_debug)\n",
      "       10    0.000    0.000    0.000    0.000 base_events.py:202(_check_ssl_socket)\n",
      "        1    0.000    0.000    0.000    0.000 base_events.py:2020(set_debug)\n",
      "       56    0.000    0.000    0.000    0.000 base_events.py:446(create_future)\n",
      "       10    0.000    0.000    0.000    0.000 base_events.py:450(create_task)\n",
      "      159    0.000    0.000    0.000    0.000 base_events.py:543(_check_closed)\n",
      "        4    0.000    0.000    0.000    0.000 base_events.py:547(_check_default_executor)\n",
      "       10    0.000    0.000    0.000    0.000 base_events.py:724(is_closed)\n",
      "        5    0.000    0.000    0.000    0.000 base_events.py:734(is_running)\n",
      "      135    0.000    0.000    0.000    0.000 base_events.py:738(time)\n",
      "       16    0.000    0.000    0.000    0.000 base_events.py:747(call_later)\n",
      "       22    0.000    0.000    0.000    0.000 base_events.py:771(call_at)\n",
      "      107    0.000    0.000    0.000    0.000 base_events.py:789(call_soon)\n",
      "      112    0.000    0.000    0.000    0.000 base_events.py:818(_call_soon)\n",
      "      6/3    0.000    0.000    0.000    0.000 base_events.py:842(call_soon_threadsafe)\n",
      "        4    0.000    0.000    0.000    0.000 base_events.py:853(run_in_executor)\n",
      "        8    0.000    0.000    0.000    0.000 base_events.py:898(getaddrinfo)\n",
      "       41    0.000    0.000    0.000    0.000 base_futures.py:13(isfuture)\n",
      "        5    0.000    0.000    0.000    0.000 base_protocol.py:19(__init__)\n",
      "       24    0.000    0.000    0.000    0.000 base_protocol.py:27(connected)\n",
      "        5    0.000    0.000    0.000    0.000 base_protocol.py:32(writing_paused)\n",
      "        1    0.000    0.000    0.000    0.000 base_protocol.py:50(pause_reading)\n",
      "        1    0.000    0.000    0.000    0.000 base_protocol.py:58(resume_reading)\n",
      "        5    0.000    0.000    0.000    0.000 base_protocol.py:66(connection_made)\n",
      "        2    0.000    0.000    0.000    0.000 calendar.py:691(timegm)\n",
      "        5    0.000    0.000    0.000    0.000 client.py:1135(_prepare_headers)\n",
      "        5    0.000    0.000    0.000    0.000 client.py:1197(get)\n",
      "        2    0.000    0.000    0.000    0.000 client.py:1257(close)\n",
      "        7    0.000    0.000    0.000    0.000 client.py:1267(closed)\n",
      "        5    0.000    0.000    0.000    0.000 client.py:1355(trust_env)\n",
      "        1    0.000    0.000    0.000    0.000 client.py:1389(__aenter__)\n",
      "        2    0.000    0.000    0.000    0.000 client.py:1392(__aexit__)\n",
      "        5    0.000    0.000    0.000    0.000 client.py:1405(__init__)\n",
      "       25    0.000    0.000    0.004    0.000 client.py:1424(__aenter__)\n",
      "        5    0.000    0.000    0.000    0.000 client.py:1428(__aexit__)\n",
      "        1    0.000    0.000    0.000    0.000 client.py:268(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 client.py:300(<lambda>)\n",
      "        1    0.000    0.000    0.000    0.000 client.py:434(__del__)\n",
      "        5    0.000    0.000    0.000    0.000 client.py:462(_build_url)\n",
      "       25    0.000    0.000    0.004    0.000 client.py:470(_request)\n",
      "        1    0.000    0.000    0.000    0.000 client_proto.py:137(pause_reading)\n",
      "        1    0.000    0.000    0.000    0.000 client_proto.py:141(resume_reading)\n",
      "        5    0.000    0.000    0.000    0.000 client_proto.py:169(set_response_params)\n",
      "       11    0.000    0.000    0.000    0.000 client_proto.py:205(_drop_timeout)\n",
      "       35    0.000    0.000    0.000    0.000 client_proto.py:210(_reschedule_timeout)\n",
      "        5    0.000    0.000    0.000    0.000 client_proto.py:222(start_timeout)\n",
      "       29    0.001    0.000    0.007    0.000 client_proto.py:239(data_received)\n",
      "        5    0.000    0.000    0.000    0.000 client_proto.py:26(__init__)\n",
      "        5    0.000    0.000    0.000    0.000 client_proto.py:47(upgraded)\n",
      "        5    0.000    0.000    0.000    0.000 client_proto.py:51(should_close)\n",
      "        5    0.000    0.000    0.000    0.000 client_proto.py:66(close)\n",
      "        9    0.000    0.000    0.000    0.000 client_reqrep.py:1003(connection)\n",
      "       10    0.000    0.000    0.001    0.000 client_reqrep.py:1048(start)\n",
      "        5    0.000    0.000    0.000    0.000 client_reqrep.py:1102(_response_eof)\n",
      "       10    0.000    0.000    0.000    0.000 client_reqrep.py:1132(release)\n",
      "       20    0.000    0.000    0.000    0.000 client_reqrep.py:1169(_release_connection)\n",
      "        5    0.000    0.000    0.000    0.000 client_reqrep.py:1177(_wait_released)\n",
      "       15    0.000    0.000    0.000    0.000 client_reqrep.py:1190(_cleanup_writer)\n",
      "        5    0.000    0.000    0.000    0.000 client_reqrep.py:1195(_notify_content)\n",
      "        5    0.000    0.000    0.000    0.000 client_reqrep.py:1201(wait_for_close)\n",
      "       29    0.000    0.000    0.001    0.000 client_reqrep.py:1214(read)\n",
      "        5    0.000    0.000    0.000    0.000 client_reqrep.py:1234(get_encoding)\n",
      "       29    0.000    0.000    0.004    0.000 client_reqrep.py:1257(text)\n",
      "        5    0.000    0.000    0.000    0.000 client_reqrep.py:1300(__aenter__)\n",
      "        5    0.000    0.000    0.000    0.000 client_reqrep.py:1304(__aexit__)\n",
      "        6    0.000    0.000    0.000    0.000 client_reqrep.py:174(_merge_ssl_params)\n",
      "        5    0.000    0.000    0.000    0.000 client_reqrep.py:285(__init__)\n",
      "       10    0.000    0.000    0.000    0.000 client_reqrep.py:382(is_ssl)\n",
      "       10    0.000    0.000    0.000    0.000 client_reqrep.py:385(ssl)\n",
      "        5    0.000    0.000    0.000    0.000 client_reqrep.py:389(connection_key)\n",
      "        5    0.000    0.000    0.000    0.000 client_reqrep.py:415(port)\n",
      "        5    0.000    0.000    0.000    0.000 client_reqrep.py:419(request_info)\n",
      "        5    0.000    0.000    0.000    0.000 client_reqrep.py:430(update_host)\n",
      "        5    0.000    0.000    0.000    0.000 client_reqrep.py:440(update_version)\n",
      "        5    0.000    0.000    0.000    0.000 client_reqrep.py:455(update_headers)\n",
      "        5    0.000    0.000    0.000    0.000 client_reqrep.py:480(update_auto_headers)\n",
      "        5    0.000    0.000    0.000    0.000 client_reqrep.py:499(update_cookies)\n",
      "        5    0.000    0.000    0.000    0.000 client_reqrep.py:524(update_content_encoding)\n",
      "        5    0.000    0.000    0.000    0.000 client_reqrep.py:564(update_auth)\n",
      "        5    0.000    0.000    0.000    0.000 client_reqrep.py:580(update_body_from_data)\n",
      "        5    0.000    0.000    0.000    0.000 client_reqrep.py:611(update_expect_continue)\n",
      "        5    0.000    0.000    0.000    0.000 client_reqrep.py:623(update_proxy)\n",
      "        5    0.000    0.000    0.000    0.000 client_reqrep.py:694(send)\n",
      "        5    0.000    0.000    0.000    0.000 client_reqrep.py:854(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 client_reqrep.py:929(url)\n",
      "        5    0.000    0.000    0.000    0.000 client_reqrep.py:947(headers)\n",
      "        5    0.000    0.000    0.000    0.000 client_reqrep.py:969(__del__)\n",
      "        5    0.000    0.000    0.000    0.000 compression_utils.py:100(__init__)\n",
      "       49    0.000    0.000    0.005    0.000 compression_utils.py:114(decompress_sync)\n",
      "        5    0.000    0.000    0.000    0.000 compression_utils.py:133(flush)\n",
      "        5    0.000    0.000    0.000    0.000 compression_utils.py:19(encoding_to_mode)\n",
      "        5    0.000    0.000    0.000    0.000 compression_utils.py:30(__init__)\n",
      "        8    0.000    0.000    0.000    0.000 connector.py:1005(_resolve_host_with_throttle)\n",
      "        1    0.000    0.000    0.000    0.000 connector.py:104(__init__)\n",
      "       20    0.000    0.000    0.002    0.000 connector.py:1046(_create_connection)\n",
      "        5    0.000    0.000    0.000    0.000 connector.py:1060(_get_ssl_context)\n",
      "        1    0.000    0.000    0.000    0.000 connector.py:108(__await__)\n",
      "        5    0.000    0.000    0.000    0.000 connector.py:1093(_get_fingerprint)\n",
      "       15    0.000    0.000    0.001    0.000 connector.py:1102(_wrap_create_connection)\n",
      "        1    0.000    0.000    0.000    0.000 connector.py:112(__del__)\n",
      "        5    0.000    0.000    0.000    0.000 connector.py:125(__init__)\n",
      "        5    0.000    0.000    0.000    0.000 connector.py:1317(_convert_hosts_to_addr_infos)\n",
      "       20    0.000    0.000    0.002    0.000 connector.py:1338(_create_direct_connection)\n",
      "        5    0.000    0.000    0.000    0.000 connector.py:144(__del__)\n",
      "        5    0.000    0.000    0.000    0.000 connector.py:158(__bool__)\n",
      "        5    0.000    0.000    0.000    0.000 connector.py:169(transport)\n",
      "       25    0.000    0.000    0.000    0.000 connector.py:175(protocol)\n",
      "        4    0.000    0.000    0.000    0.000 connector.py:179(add_callback)\n",
      "        5    0.000    0.000    0.000    0.000 connector.py:183(_notify_release)\n",
      "        5    0.000    0.000    0.000    0.000 connector.py:197(release)\n",
      "        1    0.000    0.000    0.000    0.000 connector.py:241(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 connector.py:314(__del__)\n",
      "        5    0.000    0.000    0.000    0.000 connector.py:357(force_close)\n",
      "        1    0.000    0.000    0.000    0.000 connector.py:419(_cleanup_closed)\n",
      "        1    0.000    0.000    0.000    0.000 connector.py:442(close)\n",
      "        1    0.000    0.000    0.000    0.000 connector.py:447(_close)\n",
      "        6    0.000    0.000    0.000    0.000 connector.py:487(closed)\n",
      "        5    0.000    0.000    0.000    0.000 connector.py:495(_available_connections)\n",
      "       20    0.000    0.000    0.002    0.000 connector.py:520(connect)\n",
      "        5    0.000    0.000    0.000    0.000 connector.py:615(_get)\n",
      "        5    0.000    0.000    0.000    0.000 connector.py:657(_release_waiter)\n",
      "        5    0.000    0.000    0.000    0.000 connector.py:683(_release_acquired)\n",
      "        5    0.000    0.000    0.000    0.000 connector.py:696(_release)\n",
      "        1    0.000    0.000    0.000    0.000 connector.py:735(__init__)\n",
      "        5    0.000    0.000    0.000    0.000 connector.py:740(__contains__)\n",
      "        4    0.000    0.000    0.000    0.000 connector.py:743(add)\n",
      "        5    0.000    0.000    0.000    0.000 connector.py:759(next_addrs)\n",
      "        1    0.000    0.000    0.000    0.000 connector.py:835(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 connector.py:883(close)\n",
      "       10    0.000    0.000    0.000    0.000 connector.py:914(_resolve_host)\n",
      "        2    0.000    0.000    0.000    0.000 contextlib.py:104(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 contextlib.py:132(__enter__)\n",
      "        2    0.000    0.000    0.000    0.000 contextlib.py:141(__exit__)\n",
      "        2    0.000    0.000    0.000    0.000 contextlib.py:299(helper)\n",
      "       13    0.000    0.000    0.000    0.000 contextlib.py:440(__init__)\n",
      "       13    0.000    0.000    0.000    0.000 contextlib.py:443(__enter__)\n",
      "       13    0.000    0.000    0.000    0.000 contextlib.py:446(__exit__)\n",
      "        2    0.000    0.000    0.000    0.000 cookiejar.py:173(_do_expiration)\n",
      "        3    0.000    0.000    0.000    0.000 cookiejar.py:223(_expire_cookie)\n",
      "        2    0.000    0.000    0.000    0.000 cookiejar.py:231(update_cookies)\n",
      "        5    0.000    0.000    0.000    0.000 cookiejar.py:305(filter_cookies)\n",
      "        4    0.000    0.000    0.000    0.000 cookiejar.py:383(_is_domain_match)\n",
      "        2    0.000    0.000    0.000    0.000 cookiejar.py:399(_parse_date)\n",
      "        8    0.000    0.000    0.000    0.000 cookiejar.py:423(<genexpr>)\n",
      "        1    0.000    0.000    0.000    0.000 cookiejar.py:87(__init__)\n",
      "       18    0.000    0.000    0.000    0.000 cookies.py:195(_unquote)\n",
      "        4    0.000    0.000    0.000    0.000 cookies.py:271(__init__)\n",
      "       22    0.000    0.000    0.000    0.000 cookies.py:291(__setitem__)\n",
      "        4    0.000    0.000    0.000    0.000 cookies.py:303(__eq__)\n",
      "        4    0.000    0.000    0.000    0.000 cookies.py:331(set)\n",
      "        9    0.000    0.000    0.000    0.000 cookies.py:465(__init__)\n",
      "        4    0.000    0.000    0.000    0.000 cookies.py:469(__set)\n",
      "        4    0.000    0.000    0.000    0.000 cookies.py:475(__setitem__)\n",
      "        4    0.000    0.000    0.000    0.000 cookies.py:509(load)\n",
      "        4    0.000    0.000    0.000    0.000 cookies.py:523(__parse_string)\n",
      "        4    0.000    0.000    0.000    0.000 cookies.py:591(value_decode)\n",
      "       20    0.000    0.000    0.000    0.000 coroutines.py:32(iscoroutine)\n",
      "        1    0.000    0.000    0.000    0.000 decorator.py:200(fix)\n",
      "        1    0.000    0.000    0.000    0.000 decorator.py:232(fun)\n",
      "       10    0.000    0.000    0.001    0.000 element.py:1079(_find_all)\n",
      "    15430    0.013    0.000    0.025    0.000 element.py:1291(__new__)\n",
      "        5    0.000    0.000    0.000    0.000 element.py:1504(_string_for_name_and_ids)\n",
      "    13147    0.023    0.000    0.176    0.000 element.py:1618(__init__)\n",
      "       10    0.000    0.000    0.000    0.000 element.py:1873(_all_strings)\n",
      "        5    0.000    0.000    0.000    0.000 element.py:200(__new__)\n",
      "      357    0.000    0.000    0.000    0.000 element.py:2160(get)\n",
      "      119    0.000    0.000    0.000    0.000 element.py:2173(get_attribute_list)\n",
      "      214    0.000    0.000    0.000    0.000 element.py:2219(__bool__)\n",
      "        5    0.000    0.000    0.000    0.000 element.py:2223(__setitem__)\n",
      "       10    0.000    0.000    0.001    0.000 element.py:2249(__getattr__)\n",
      "     7815    0.005    0.000    0.008    0.000 element.py:2272(__eq__)\n",
      "    39544    0.112    0.000    0.121    0.000 element.py:238(__setitem__)\n",
      "       10    0.000    0.000    0.001    0.000 element.py:2684(find)\n",
      "       10    0.000    0.000    0.001    0.000 element.py:2715(find_all)\n",
      "      420    0.000    0.000    0.000    0.000 element.py:2763(descendants)\n",
      "       10    0.000    0.000    0.000    0.000 element.py:2869(__init__)\n",
      "    44007    0.023    0.000    0.023    0.000 element.py:376(setup)\n",
      "        5    0.000    0.000    0.000    0.000 element.py:524(get_text)\n",
      "       15    0.000    0.000    0.000    0.000 element.py:657(_last_descendant)\n",
      "        4    0.000    0.000    0.000    0.000 encoder.py:105(__init__)\n",
      "        4    0.000    0.000    0.000    0.000 encoder.py:183(encode)\n",
      "        4    0.000    0.000    0.000    0.000 encoder.py:205(iterencode)\n",
      "      162    0.000    0.000    0.000    0.000 enum.py:1123(__new__)\n",
      "      108    0.000    0.000    0.000    0.000 enum.py:1538(_get_value)\n",
      "       22    0.000    0.000    0.000    0.000 enum.py:1545(__or__)\n",
      "       14    0.000    0.000    0.000    0.000 enum.py:1556(__and__)\n",
      "      162    0.000    0.000    0.000    0.000 enum.py:720(__call__)\n",
      "       22    0.000    0.000    0.000    0.000 events.py:111(__init__)\n",
      "       64    0.000    0.000    0.000    0.000 events.py:127(__lt__)\n",
      "       15    0.000    0.000    0.000    0.000 events.py:155(cancel)\n",
      "      144    0.000    0.000    0.000    0.000 events.py:36(__init__)\n",
      "       20    0.000    0.000    0.000    0.000 events.py:72(cancel)\n",
      "        5    0.000    0.000    0.000    0.000 events.py:83(cancelled)\n",
      "   160/45    0.005    0.000    0.006    0.000 events.py:86(_run)\n",
      "        5    0.000    0.000    0.020    0.004 fake.py:179(getBrowser)\n",
      "        5    0.005    0.001    0.020    0.004 fake.py:224(_filter_useragents)\n",
      "    49895    0.019    0.000    0.019    0.000 fake.py:243(<lambda>)\n",
      "        5    0.000    0.000    0.026    0.005 fake.py:279(__getattr__)\n",
      "        5    0.000    0.000    0.026    0.005 fake.py:340(random)\n",
      "       20    0.000    0.000    0.001    0.000 filter.py:129(filter)\n",
      "       10    0.000    0.000    0.001    0.000 filter.py:163(find_all)\n",
      "       10    0.000    0.000    0.000    0.000 filter.py:224(__init__)\n",
      "       10    0.000    0.000    0.000    0.000 filter.py:259(_base_match)\n",
      "       10    0.000    0.000    0.000    0.000 filter.py:321(matches_tag)\n",
      "       10    0.000    0.000    0.000    0.000 filter.py:378(__init__)\n",
      "       10    0.000    0.000    0.000    0.000 filter.py:437(includes_everything)\n",
      "       30    0.000    0.000    0.000    0.000 filter.py:490(_make_match_rules)\n",
      "      214    0.000    0.000    0.000    0.000 filter.py:538(matches_tag)\n",
      "      400    0.000    0.000    0.000    0.000 filter.py:717(match)\n",
      "       62    0.000    0.000    0.000    0.000 futures.py:119(get_loop)\n",
      "       67    0.000    0.000    0.001    0.000 futures.py:161(__schedule_callbacks)\n",
      "       37    0.000    0.000    0.000    0.000 futures.py:175(cancelled)\n",
      "      316    0.000    0.000    0.000    0.000 futures.py:181(done)\n",
      "      118    0.000    0.000    0.000    0.000 futures.py:189(result)\n",
      "       14    0.000    0.000    0.000    0.000 futures.py:205(exception)\n",
      "       86    0.000    0.000    0.000    0.000 futures.py:220(add_done_callback)\n",
      "        8    0.000    0.000    0.000    0.000 futures.py:236(remove_done_callback)\n",
      "       67    0.000    0.000    0.001    0.000 futures.py:251(set_result)\n",
      "      104    0.000    0.000    0.000    0.000 futures.py:286(__await__)\n",
      "       63    0.000    0.000    0.000    0.000 futures.py:301(_get_loop)\n",
      "        5    0.000    0.000    0.000    0.000 futures.py:347(_copy_future_state)\n",
      "        4    0.000    0.000    0.000    0.000 futures.py:367(_chain_future)\n",
      "        5    0.000    0.000    0.000    0.000 futures.py:383(_set_state)\n",
      "        5    0.000    0.000    0.000    0.000 futures.py:389(_call_check_cancel)\n",
      "      4/2    0.000    0.000    0.000    0.000 futures.py:396(_call_set_state)\n",
      "        4    0.000    0.000    0.000    0.000 futures.py:411(wrap_future)\n",
      "       70    0.000    0.000    0.000    0.000 futures.py:72(__init__)\n",
      "       70    0.000    0.000    0.000    0.000 futures.py:91(__del__)\n",
      "        2    0.000    0.000    0.000    0.000 helpers.py:118(__await__)\n",
      "        5    0.000    0.000    0.000    0.000 helpers.py:185(strip_auth_from_url)\n",
      "        7    0.000    0.000    0.000    0.000 helpers.py:435(is_ip_address)\n",
      "        1    0.000    0.000    0.000    0.000 helpers.py:500(weakref_handle)\n",
      "        5    0.000    0.000    0.000    0.000 helpers.py:546(__init__)\n",
      "        5    0.000    0.000    0.000    0.000 helpers.py:559(register)\n",
      "        5    0.000    0.000    0.000    0.000 helpers.py:567(start)\n",
      "        5    0.000    0.000    0.000    0.000 helpers.py:577(timer)\n",
      "        5    0.000    0.000    0.000    0.000 helpers.py:622(__init__)\n",
      "       33    0.000    0.000    0.000    0.000 helpers.py:628(assert_timeout)\n",
      "       34    0.000    0.000    0.000    0.000 helpers.py:633(__enter__)\n",
      "       34    0.000    0.000    0.000    0.000 helpers.py:650(__exit__)\n",
      "       10    0.000    0.000    0.000    0.000 helpers.py:682(ceil_timeout)\n",
      "       30    0.000    0.000    0.000    0.000 helpers.py:743(set_result)\n",
      "        5    0.000    0.000    0.000    0.000 helpers.py:759(set_exception)\n",
      "        1    0.000    0.000    0.000    0.000 history.py:1007(writeout_cache)\n",
      "        2    0.000    0.000    0.000    0.000 history.py:1048(hold)\n",
      "        1    0.000    0.000    0.000    0.000 history.py:90(only_when_enabled)\n",
      "        1    0.000    0.000    0.000    0.000 history.py:991(_writeout_input_cache)\n",
      "        1    0.000    0.000    0.000    0.000 history.py:999(_writeout_output_cache)\n",
      "        4    0.000    0.000    0.000    0.000 hmac.py:117(update)\n",
      "        1    0.000    0.000    0.000    0.000 hmac.py:122(copy)\n",
      "        1    0.000    0.000    0.000    0.000 hmac.py:139(_current)\n",
      "        1    0.000    0.000    0.000    0.000 hmac.py:161(hexdigest)\n",
      "        5    0.000    0.000    0.000    0.000 http_parser.py:1010(feed_eof)\n",
      "       42    0.000    0.000    0.000    0.000 http_parser.py:1020(begin_http_chunk_receiving)\n",
      "       42    0.000    0.000    0.000    0.000 http_parser.py:1023(end_http_chunk_receiving)\n",
      "        5    0.000    0.000    0.000    0.000 http_parser.py:954(__init__)\n",
      "       49    0.000    0.000    0.006    0.000 http_parser.py:978(feed_data)\n",
      "        5    0.000    0.000    0.000    0.000 http_writer.py:158(write_headers)\n",
      "        5    0.000    0.000    0.000    0.000 http_writer.py:169(set_eof)\n",
      "        5    0.000    0.000    0.000    0.000 http_writer.py:59(__init__)\n",
      "        5    0.000    0.000    0.000    0.000 http_writer.py:87(_write)\n",
      "        9    0.000    0.000    0.000    0.000 idna.py:155(encode)\n",
      "       10    0.000    0.000    0.001    0.000 impl.py:128(_connect_sock)\n",
      "       10    0.000    0.000    0.000    0.000 impl.py:14(start_connection)\n",
      "        4    0.000    0.000    0.000    0.000 impl.py:186(_interleave_addrinfos)\n",
      "       46    0.000    0.000    0.000    0.000 impl.py:205(<genexpr>)\n",
      "        8    0.000    0.000    0.000    0.000 impl.py:79(<genexpr>)\n",
      "        5    0.000    0.000    0.001    0.000 inspect.py:1358(getfullargspec)\n",
      "        5    0.000    0.000    0.000    0.000 inspect.py:176(get_annotations)\n",
      "        5    0.000    0.000    0.000    0.000 inspect.py:2397(_signature_from_function)\n",
      "     10/5    0.000    0.000    0.000    0.000 inspect.py:2501(_signature_from_callable)\n",
      "       20    0.000    0.000    0.000    0.000 inspect.py:2743(__init__)\n",
      "       44    0.000    0.000    0.000    0.000 inspect.py:2796(name)\n",
      "       25    0.000    0.000    0.000    0.000 inspect.py:2800(default)\n",
      "       35    0.000    0.000    0.000    0.000 inspect.py:2804(annotation)\n",
      "       30    0.000    0.000    0.000    0.000 inspect.py:2808(kind)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:2888(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:2896(args)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:2919(kwargs)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:2949(apply_defaults)\n",
      "        5    0.000    0.000    0.000    0.000 inspect.py:3029(__init__)\n",
      "       25    0.000    0.000    0.000    0.000 inspect.py:3076(<genexpr>)\n",
      "        9    0.000    0.000    0.000    0.000 inspect.py:3089(parameters)\n",
      "       10    0.000    0.000    0.000    0.000 inspect.py:3093(return_annotation)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:3133(_bind)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:3272(bind)\n",
      "       10    0.000    0.000    0.000    0.000 inspect.py:378(isfunction)\n",
      "        4    0.000    0.000    0.000    0.000 ioloop.py:541(time)\n",
      "        2    0.000    0.000    0.000    0.000 ioloop.py:596(call_later)\n",
      "        3    0.000    0.000    0.001    0.000 ioloop.py:742(_run_callback)\n",
      "        3    0.000    0.000    0.000    0.000 iostream.py:138(_event_pipe)\n",
      "        4    0.000    0.000    0.000    0.000 iostream.py:157(_handle_event)\n",
      "        2    0.000    0.000    0.000    0.000 iostream.py:213(_is_master_process)\n",
      "        2    0.000    0.000    0.000    0.000 iostream.py:216(_check_mp_mode)\n",
      "        2    0.000    0.000    0.000    0.000 iostream.py:255(closed)\n",
      "        3    0.000    0.000    0.000    0.000 iostream.py:259(schedule)\n",
      "        1    0.000    0.000    0.000    0.000 iostream.py:271(send_multipart)\n",
      "        2    0.000    0.000    0.000    0.000 iostream.py:276(<lambda>)\n",
      "        2    0.000    0.000    0.000    0.000 iostream.py:278(_really_send)\n",
      "       10    0.000    0.000    0.000    0.000 iostream.py:505(parent_header)\n",
      "       10    0.000    0.000    0.000    0.000 iostream.py:550(_is_master_process)\n",
      "       10    0.000    0.000    0.000    0.000 iostream.py:577(_schedule_flush)\n",
      "        2    0.000    0.000    0.000    0.000 iostream.py:587(_schedule_in_thread)\n",
      "        1    0.000    0.000    0.000    0.000 iostream.py:616(_flush)\n",
      "       10    0.000    0.000    0.000    0.000 iostream.py:655(write)\n",
      "        2    0.000    0.000    0.000    0.000 iostream.py:710(_flush_buffers)\n",
      "        1    0.000    0.000    0.000    0.000 iostream.py:718(_rotate_buffers)\n",
      "        1    0.000    0.000    0.000    0.000 iostream.py:725(_hooks)\n",
      "      226    0.002    0.000    0.003    0.000 ipkernel.py:775(_clean_thread_parent_frames)\n",
      "        2    0.000    0.000    0.000    0.000 jsonutil.py:107(json_default)\n",
      "        2    0.000    0.000    0.000    0.000 jsonutil.py:38(_ensure_tzinfo)\n",
      "        1    0.000    0.000    0.000    0.000 kernelbase.py:570(<lambda>)\n",
      "     84/3    0.001    0.000    0.031    0.010 nest_asyncio.py:100(_run_once)\n",
      "        2    0.000    0.000    0.000    0.000 nest_asyncio.py:141(manage_run)\n",
      "        3    0.000    0.000    0.000    0.000 nest_asyncio.py:37(_get_event_loop)\n",
      "        5    0.000    0.000    0.000    0.000 queue.py:209(_qsize)\n",
      "        5    0.000    0.000    0.000    0.000 queue.py:97(empty)\n",
      "        1    0.000    0.000    0.000    0.000 queues.py:173(qsize)\n",
      "        1    0.000    0.000    0.000    0.000 queues.py:225(get)\n",
      "        1    0.000    0.000    0.000    0.000 queues.py:256(get_nowait)\n",
      "        1    0.000    0.000    0.000    0.000 queues.py:322(_consume_expired)\n",
      "        1    0.000    0.000    0.000    0.000 queues.py:59(_set_timeout)\n",
      "        5    0.000    0.000    0.000    0.000 random.py:242(_randbelow_with_getrandbits)\n",
      "        5    0.000    0.000    0.000    0.000 random.py:341(choice)\n",
      "        1    0.000    0.000    0.000    0.000 resolver.py:30(__init__)\n",
      "        8    0.000    0.000    0.000    0.000 resolver.py:33(resolve)\n",
      "       20    0.000    0.000    0.000    0.000 selector_events.py:1055(write)\n",
      "        3    0.000    0.000    0.000    0.000 selector_events.py:129(_read_from_self)\n",
      "      6/3    0.000    0.000    0.000    0.000 selector_events.py:141(_write_to_self)\n",
      "       15    0.000    0.000    0.000    0.000 selector_events.py:260(_ensure_fd_no_transport)\n",
      "        5    0.000    0.000    0.000    0.000 selector_events.py:278(_add_reader)\n",
      "        5    0.000    0.000    0.000    0.000 selector_events.py:315(_add_writer)\n",
      "        5    0.000    0.000    0.000    0.000 selector_events.py:331(_remove_writer)\n",
      "        5    0.000    0.000    0.000    0.000 selector_events.py:369(remove_writer)\n",
      "       10    0.000    0.000    0.001    0.000 selector_events.py:631(sock_connect)\n",
      "        5    0.000    0.000    0.000    0.000 selector_events.py:656(_sock_connect)\n",
      "        5    0.000    0.000    0.000    0.000 selector_events.py:679(_sock_write_done)\n",
      "        5    0.000    0.000    0.000    0.000 selector_events.py:683(_sock_connect_cb)\n",
      "        5    0.000    0.000    0.001    0.000 selector_events.py:75(_make_ssl_transport)\n",
      "       85    0.000    0.000    0.000    0.000 selector_events.py:750(_process_events)\n",
      "        5    0.000    0.000    0.000    0.000 selector_events.py:779(__init__)\n",
      "        5    0.000    0.000    0.000    0.000 selector_events.py:838(set_protocol)\n",
      "       10    0.000    0.000    0.000    0.000 selector_events.py:845(is_closing)\n",
      "        5    0.000    0.000    0.000    0.000 selector_events.py:848(is_reading)\n",
      "        5    0.000    0.000    0.000    0.000 selector_events.py:925(_add_reader)\n",
      "        5    0.000    0.000    0.000    0.000 selector_events.py:936(__init__)\n",
      "        5    0.000    0.000    0.000    0.000 selector_events.py:961(set_protocol)\n",
      "       35    0.000    0.000    0.021    0.001 selector_events.py:969(_read_ready)\n",
      "       35    0.000    0.000    0.021    0.001 selector_events.py:972(_read_ready__get_buffer)\n",
      "       15    0.000    0.000    0.000    0.000 selectors.py:180(get_key)\n",
      "       30    0.000    0.000    0.000    0.000 selectors.py:21(_fileobj_to_fd)\n",
      "       30    0.000    0.000    0.000    0.000 selectors.py:215(_fileobj_lookup)\n",
      "       10    0.000    0.000    0.000    0.000 selectors.py:234(register)\n",
      "        5    0.000    0.000    0.000    0.000 selectors.py:247(unregister)\n",
      "       15    0.000    0.000    0.000    0.000 selectors.py:272(get_map)\n",
      "       51    0.000    0.000    0.000    0.000 selectors.py:275(_key_from_fd)\n",
      "       10    0.000    0.000    0.000    0.000 selectors.py:351(register)\n",
      "        5    0.000    0.000    0.000    0.000 selectors.py:365(unregister)\n",
      "     85/1    0.001    0.000    0.000    0.000 selectors.py:451(select)\n",
      "       15    0.000    0.000    0.000    0.000 selectors.py:69(__getitem__)\n",
      "        1    0.000    0.000    0.000    0.000 session.py:198(utcnow)\n",
      "        1    0.000    0.000    0.000    0.000 session.py:272(msg_header)\n",
      "        1    0.000    0.000    0.000    0.000 session.py:281(extract_header)\n",
      "        1    0.000    0.000    0.000    0.000 session.py:600(msg_id)\n",
      "        1    0.000    0.000    0.000    0.000 session.py:645(msg_header)\n",
      "        1    0.000    0.000    0.000    0.000 session.py:649(msg)\n",
      "        1    0.000    0.000    0.000    0.000 session.py:675(sign)\n",
      "        1    0.000    0.000    0.000    0.000 session.py:690(serialize)\n",
      "        1    0.000    0.000    0.000    0.000 session.py:754(send)\n",
      "        4    0.000    0.000    0.000    0.000 session.py:92(json_packer)\n",
      "       93    0.000    0.000    0.000    0.000 socket.py:100(_intenum_converter)\n",
      "        5    0.000    0.000    0.000    0.000 socket.py:221(__init__)\n",
      "       20    0.000    0.000    0.000    0.000 socket.py:517(family)\n",
      "       15    0.000    0.000    0.000    0.000 socket.py:523(type)\n",
      "       23    0.000    0.000    0.000    0.000 socket.py:632(send)\n",
      "        2    0.000    0.000    0.000    0.000 socket.py:709(send_multipart)\n",
      "        4    0.000    0.000    0.000    0.000 socket.py:780(recv_multipart)\n",
      "      4/1    0.000    0.000    0.000    0.000 socket.py:961(getaddrinfo)\n",
      "        5    0.000    0.000    0.000    0.000 ssl.py:441(_encode_hostname)\n",
      "        5    0.000    0.000    0.000    0.000 ssl.py:465(wrap_bio)\n",
      "        5    0.000    0.000    0.000    0.000 ssl.py:803(_create)\n",
      "      208    0.000    0.000    0.002    0.000 ssl.py:849(read)\n",
      "        5    0.000    0.000    0.000    0.000 ssl.py:861(write)\n",
      "        5    0.000    0.000    0.000    0.000 ssl.py:869(getpeercert)\n",
      "        5    0.000    0.000    0.000    0.000 ssl.py:894(cipher)\n",
      "        5    0.000    0.000    0.000    0.000 ssl.py:905(compression)\n",
      "       10    0.000    0.000    0.008    0.001 ssl.py:914(do_handshake)\n",
      "        5    0.000    0.000    0.000    0.000 ssl.py:918(unwrap)\n",
      "        5    0.000    0.000    0.000    0.000 sslproto.py:103(is_closing)\n",
      "        5    0.000    0.000    0.000    0.000 sslproto.py:106(close)\n",
      "        1    0.000    0.000    0.000    0.000 sslproto.py:130(pause_reading)\n",
      "        1    0.000    0.000    0.000    0.000 sslproto.py:138(resume_reading)\n",
      "        5    0.000    0.000    0.000    0.000 sslproto.py:211(write)\n",
      "        5    0.000    0.000    0.000    0.000 sslproto.py:270(__init__)\n",
      "        5    0.000    0.000    0.000    0.000 sslproto.py:353(_set_app_protocol)\n",
      "        5    0.000    0.000    0.000    0.000 sslproto.py:364(_wakeup_waiter)\n",
      "       10    0.000    0.000    0.000    0.000 sslproto.py:374(_get_app_transport)\n",
      "        5    0.000    0.000    0.000    0.000 sslproto.py:382(_is_transport_closing)\n",
      "        5    0.000    0.000    0.001    0.000 sslproto.py:385(connection_made)\n",
      "       35    0.000    0.000    0.000    0.000 sslproto.py:429(get_buffer)\n",
      "       35    0.000    0.000    0.020    0.001 sslproto.py:438(buffer_updated)\n",
      "        5    0.000    0.000    0.000    0.000 sslproto.py:488(_get_extra_info)\n",
      "       20    0.000    0.000    0.000    0.000 sslproto.py:496(_set_state)\n",
      "        5    0.000    0.000    0.001    0.000 sslproto.py:536(_start_handshake)\n",
      "       10    0.000    0.000    0.010    0.001 sslproto.py:561(_do_handshake)\n",
      "        5    0.000    0.000    0.002    0.000 sslproto.py:571(_on_handshake_complete)\n",
      "       10    0.000    0.000    0.000    0.000 sslproto.py:61(add_flowcontrol_defaults)\n",
      "        5    0.000    0.000    0.000    0.000 sslproto.py:612(_start_shutdown)\n",
      "        5    0.000    0.000    0.000    0.000 sslproto.py:643(_do_flush)\n",
      "        5    0.000    0.000    0.000    0.000 sslproto.py:648(_do_shutdown)\n",
      "        5    0.000    0.000    0.000    0.000 sslproto.py:678(_write_appdata)\n",
      "        5    0.000    0.000    0.000    0.000 sslproto.py:702(_do_write)\n",
      "       56    0.000    0.000    0.001    0.000 sslproto.py:718(_process_outgoing)\n",
      "       41    0.000    0.000    0.010    0.000 sslproto.py:727(_do_read)\n",
      "       41    0.000    0.000    0.010    0.000 sslproto.py:778(_do_read__copied)\n",
      "       56    0.000    0.000    0.000    0.000 sslproto.py:823(_control_app_writing)\n",
      "       56    0.000    0.000    0.000    0.000 sslproto.py:852(_get_write_buffer_size)\n",
      "        5    0.000    0.000    0.000    0.000 sslproto.py:855(_set_write_buffer_limits)\n",
      "        1    0.000    0.000    0.000    0.000 sslproto.py:863(_pause_reading)\n",
      "        1    0.000    0.000    0.000    0.000 sslproto.py:866(_resume_reading)\n",
      "        1    0.000    0.000    0.000    0.000 sslproto.py:870(resume)\n",
      "        5    0.000    0.000    0.000    0.000 sslproto.py:88(__init__)\n",
      "       41    0.000    0.000    0.000    0.000 sslproto.py:881(_control_ssl_reading)\n",
      "        5    0.000    0.000    0.000    0.000 sslproto.py:890(_set_read_buffer_limits)\n",
      "       41    0.000    0.000    0.000    0.000 sslproto.py:896(_get_read_buffer_size)\n",
      "        5    0.000    0.000    0.000    0.000 sslproto.py:93(get_extra_info)\n",
      "        5    0.000    0.000    0.000    0.000 streams.py:135(__init__)\n",
      "        5    0.000    0.000    0.000    0.000 streams.py:180(exception)\n",
      "        5    0.000    0.000    0.000    0.000 streams.py:183(set_exception)\n",
      "       10    0.000    0.000    0.000    0.000 streams.py:201(on_eof)\n",
      "        5    0.000    0.000    0.000    0.000 streams.py:210(feed_eof)\n",
      "        5    0.000    0.000    0.000    0.000 streams.py:234(is_eof)\n",
      "       46    0.000    0.000    0.001    0.000 streams.py:273(feed_data)\n",
      "       42    0.000    0.000    0.000    0.000 streams.py:292(begin_http_chunk_receiving)\n",
      "       42    0.000    0.000    0.000    0.000 streams.py:300(end_http_chunk_receiving)\n",
      "       48    0.000    0.000    0.001    0.000 streams.py:330(_wait)\n",
      "       29    0.000    0.000    0.001    0.000 streams.py:390(read)\n",
      "       57    0.000    0.000    0.001    0.000 streams.py:432(readany)\n",
      "       41    0.000    0.000    0.000    0.000 streams.py:508(_read_nowait_chunk)\n",
      "       33    0.000    0.000    0.000    0.000 streams.py:536(_read_nowait)\n",
      "        5    0.000    0.000    0.000    0.000 streams.py:625(__init__)\n",
      "        5    0.000    0.000    0.000    0.000 streams.py:655(feed_data)\n",
      "       10    0.000    0.000    0.000    0.000 streams.py:667(read)\n",
      "       14    0.000    0.000    0.000    0.000 tasks.py:111(__init__)\n",
      "       14    0.000    0.000    0.000    0.000 tasks.py:142(__del__)\n",
      "       44    0.000    0.000    0.000    0.000 tasks.py:252(cancelling)\n",
      "        4    0.000    0.000    0.000    0.000 tasks.py:272(__eager_start)\n",
      "    63/10    0.000    0.000    0.027    0.003 tasks.py:291(__step)\n",
      "    68/64    0.000    0.000    0.623    0.010 tasks.py:308(__step_run_and_handle_result)\n",
      "       53    0.000    0.000    0.595    0.011 tasks.py:383(__wakeup)\n",
      "       11    0.000    0.000    0.000    0.000 tasks.py:670(ensure_future)\n",
      "        1    0.000    0.000    0.000    0.000 tasks.py:710(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 tasks.py:731(gather)\n",
      "        5    0.000    0.000    0.000    0.000 tasks.py:767(_done_callback)\n",
      "        4    0.000    0.000    0.000    0.000 tasks.py:865(shield)\n",
      "        4    0.000    0.000    0.000    0.000 tasks.py:905(_inner_done_callback)\n",
      "        4    0.000    0.000    0.000    0.000 tasks.py:922(_outer_done_callback)\n",
      "        5    0.000    0.000    0.000    0.000 tcp_helpers.py:24(tcp_nodelay)\n",
      "        4    0.000    0.000    0.000    0.000 thread.py:165(submit)\n",
      "        4    0.000    0.000    0.000    0.000 thread.py:184(_adjust_thread_count)\n",
      "        4    0.000    0.000    0.000    0.000 thread.py:48(__init__)\n",
      "      4/1    0.000    0.000    0.000    0.000 thread.py:54(run)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:1155(_wait_for_tstate_lock)\n",
      "     1243    0.001    0.000    0.001    0.000 threading.py:1198(ident)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:1222(is_alive)\n",
      "        4    0.000    0.000    0.000    0.000 threading.py:124(RLock)\n",
      "      113    0.001    0.000    0.001    0.000 threading.py:1535(enumerate)\n",
      "        4    0.000    0.000    0.000    0.000 threading.py:277(__init__)\n",
      "       42    0.000    0.000    0.000    0.000 threading.py:299(__enter__)\n",
      "       42    0.000    0.000    0.000    0.000 threading.py:302(__exit__)\n",
      "        1    0.000    0.000    0.000    0.000 threading.py:308(_release_save)\n",
      "        1    0.000    0.000    0.000    0.000 threading.py:311(_acquire_restore)\n",
      "        5    0.000    0.000    0.000    0.000 threading.py:314(_is_owned)\n",
      "        8    0.000    0.000    0.000    0.000 threading.py:394(notify)\n",
      "        4    0.000    0.000    0.000    0.000 threading.py:424(notify_all)\n",
      "        4    0.000    0.000    0.000    0.000 threading.py:468(acquire)\n",
      "        4    0.000    0.000    0.000    0.000 threading.py:515(release)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:601(is_set)\n",
      "        1    0.000    0.000    0.000    0.000 threading.py:627(clear)\n",
      "       10    0.000    0.000    0.000    0.000 timeouts.py:129(timeout)\n",
      "       10    0.000    0.000    0.000    0.000 timeouts.py:33(__init__)\n",
      "       10    0.000    0.000    0.000    0.000 timeouts.py:50(reschedule)\n",
      "       10    0.000    0.000    0.000    0.000 timeouts.py:85(__aenter__)\n",
      "       10    0.000    0.000    0.000    0.000 timeouts.py:97(__aexit__)\n",
      "        1    0.000    0.000    0.000    0.000 traitlets.py:1512(_notify_trait)\n",
      "        1    0.000    0.000    0.000    0.000 traitlets.py:1523(notify_change)\n",
      "        1    0.000    0.000    0.000    0.000 traitlets.py:1527(_notify_observers)\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:2304(validate)\n",
      "        1    0.000    0.000    0.000    0.000 traitlets.py:2558(_validate_bounds)\n",
      "        1    0.000    0.000    0.000    0.000 traitlets.py:2635(validate)\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:3474(validate)\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:3486(validate_elements)\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:3624(validate_elements)\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:3631(set)\n",
      "       19    0.000    0.000    0.000    0.000 traitlets.py:629(get)\n",
      "       19    0.000    0.000    0.000    0.000 traitlets.py:676(__get__)\n",
      "        3    0.000    0.000    0.000    0.000 traitlets.py:689(set)\n",
      "        3    0.000    0.000    0.000    0.000 traitlets.py:708(__set__)\n",
      "        3    0.000    0.000    0.000    0.000 traitlets.py:718(_validate)\n",
      "        3    0.000    0.000    0.000    0.000 traitlets.py:727(_cross_validate)\n",
      "        5    0.000    0.000    0.000    0.000 transports.py:14(__init__)\n",
      "        5    0.000    0.000    0.000    0.000 transports.py:19(get_extra_info)\n",
      "        5    0.000    0.000    0.000    0.000 transports.py:270(__init__)\n",
      "        5    0.000    0.000    0.000    0.000 transports.py:314(_set_write_buffer_limits)\n",
      "        5    0.000    0.000    0.000    0.000 trsock.py:15(__init__)\n",
      "        5    0.000    0.000    0.000    0.000 trsock.py:18(family)\n",
      "        5    0.000    0.000    0.000    0.000 trsock.py:73(setsockopt)\n",
      "        7    0.000    0.000    0.000    0.000 typing.py:1221(__instancecheck__)\n",
      "       16    0.000    0.000    0.000    0.000 typing.py:1285(__hash__)\n",
      "        7    0.000    0.000    0.000    0.000 typing.py:1492(__subclasscheck__)\n",
      "        4    0.000    0.000    0.000    0.000 typing.py:1546(__getitem__)\n",
      "    42078    0.003    0.000    0.003    0.000 typing.py:2187(cast)\n",
      "    28873    0.010    0.000    0.010    0.000 typing.py:392(inner)\n",
      "        2    0.000    0.000    0.000    0.000 tz.py:74(utcoffset)\n",
      "        3    0.000    0.000    0.000    0.000 unix_events.py:81(_process_self_data)\n",
      "        1    0.000    0.000    0.000    0.000 utils.py:10(addr_to_addr_infos)\n",
      "       15    0.000    0.000    0.000    0.000 weakref.py:133(__getitem__)\n",
      "        5    0.000    0.000    0.000    0.000 weakref.py:164(__setitem__)\n",
      "        5    0.000    0.000    0.000    0.000 weakref.py:347(__new__)\n",
      "        5    0.000    0.000    0.000    0.000 weakref.py:352(__init__)\n",
      "        9    0.000    0.000    0.000    0.000 zmqstream.py:529(receiving)\n",
      "        5    0.000    0.000    0.000    0.000 zmqstream.py:533(sending)\n",
      "        4    0.000    0.000    0.000    0.000 zmqstream.py:547(_run_callback)\n",
      "        4    0.000    0.000    0.001    0.000 zmqstream.py:574(_handle_events)\n",
      "        4    0.000    0.000    0.001    0.000 zmqstream.py:615(_handle_recv)\n",
      "        5    0.000    0.000    0.000    0.000 zmqstream.py:654(_rebuild_io_state)\n",
      "        5    0.000    0.000    0.000    0.000 zmqstream.py:677(_update_handler)\n",
      "        1    0.000    0.000    0.000    0.000 zmqstream.py:685(<lambda>)\n",
      "    15471    0.004    0.000    0.004    0.000 {built-in method __new__ of type object at 0x937ea0}\n",
      "        7    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}\n",
      "       64    0.000    0.000    0.000    0.000 {built-in method _asyncio._enter_task}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method _asyncio._get_running_loop}\n",
      "       64    0.000    0.000    0.000    0.000 {built-in method _asyncio._leave_task}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method _asyncio._register_eager_task}\n",
      "       14    0.000    0.000    0.000    0.000 {built-in method _asyncio._register_task}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method _asyncio._set_running_loop}\n",
      "        8    0.000    0.000    0.000    0.000 {built-in method _asyncio._swap_current_task}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method _asyncio._unregister_eager_task}\n",
      "       44    0.000    0.000    0.000    0.000 {built-in method _asyncio.current_task}\n",
      "       20    0.000    0.000    0.000    0.000 {built-in method _asyncio.get_running_loop}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method _codecs.lookup}\n",
      "       97    0.000    0.000    0.000    0.000 {built-in method _contextvars.copy_context}\n",
      "       11    0.000    0.000    0.000    0.000 {built-in method _heapq.heappop}\n",
      "       25    0.000    0.000    0.000    0.000 {built-in method _heapq.heappush}\n",
      "      4/1    0.000    0.000    0.000    0.000 {built-in method _socket.getaddrinfo}\n",
      "        5    0.000    0.000    0.000    0.000 {built-in method _socket.inet_pton}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _thread.allocate_lock}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}\n",
      "      112    0.000    0.000    0.000    0.000 {built-in method builtins.any}\n",
      "       25    0.000    0.000    0.000    0.000 {built-in method builtins.callable}\n",
      "       80    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
      "    22390    0.002    0.000    0.002    0.000 {built-in method builtins.hasattr}\n",
      "       16    0.000    0.000    0.000    0.000 {built-in method builtins.hash}\n",
      "211171/211157    0.021    0.000    0.021    0.000 {built-in method builtins.isinstance}\n",
      "        7    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
      "    66612    0.009    0.000    0.009    0.000 {built-in method builtins.len}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.locals}\n",
      "      118    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
      "       29    0.000    0.000    0.000    0.000 {built-in method builtins.min}\n",
      "      419    0.000    0.000    0.000    0.000 {built-in method builtins.next}\n",
      "        5    0.000    0.000    0.000    0.000 {built-in method builtins.print}\n",
      "        5    0.000    0.000    0.000    0.000 {built-in method from_iterable}\n",
      "       34    0.000    0.000    0.000    0.000 {built-in method math.ceil}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method now}\n",
      "       15    0.000    0.000    0.000    0.000 {built-in method posix.getpid}\n",
      "      144    0.000    0.000    0.000    0.000 {built-in method time.monotonic}\n",
      "        7    0.000    0.000    0.000    0.000 {built-in method time.time}\n",
      "        5    0.000    0.000    0.000    0.000 {built-in method zlib.decompressobj}\n",
      "       20    0.000    0.000    0.000    0.000 {method '__contains__' of 'frozenset' objects}\n",
      "       32    0.000    0.000    0.000    0.000 {method '__enter__' of '_thread.RLock' objects}\n",
      "       10    0.000    0.000    0.000    0.000 {method '__enter__' of '_thread.lock' objects}\n",
      "      156    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}\n",
      "       25    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method '__exit__' of 'sqlite3.Connection' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {method '_is_owned' of '_thread.RLock' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method '_wrap_bio' of '_ssl._SSLContext' objects}\n",
      "     10/7    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method 'add' of 'multidict._multidict.MultiDict' objects}\n",
      "       40    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'add_done_callback' of '_asyncio.Future' objects}\n",
      "      225    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
      "    63047    0.010    0.000    0.010    0.000 {method 'append' of 'list' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method 'bit_length' of 'int' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method 'cipher' of '_ssl._SSLSocket' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'clear' of 'dict' objects}\n",
      "       11    0.000    0.000    0.000    0.000 {method 'clear' of 'list' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'clear' of 'set' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'close' of '_io.StringIO' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method 'compression' of '_ssl._SSLSocket' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method 'connect' of '_socket.socket' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'copy' of '_hashlib.HMAC' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'copy' of 'dict' objects}\n",
      "       10    0.002    0.000    0.002    0.000 {method 'decode' of 'bytes' objects}\n",
      "       49    0.005    0.000    0.005    0.000 {method 'decompress' of 'zlib.Decompress' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "       20    0.000    0.000    0.000    0.000 {method 'discard' of 'set' objects}\n",
      "       10    0.008    0.001    0.008    0.001 {method 'do_handshake' of '_ssl._SSLSocket' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method 'done' of '_asyncio.Future' objects}\n",
      "    19/14    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}\n",
      "       24    0.000    0.000    0.000    0.000 {method 'end' of 're.Match' objects}\n",
      "       15    0.000    0.000    0.000    0.000 {method 'endswith' of 'str' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'execute' of 'sqlite3.Connection' objects}\n",
      "        8    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}\n",
      "       15    0.000    0.000    0.000    0.000 {method 'fileno' of '_socket.socket' objects}\n",
      "     8223    0.005    0.000    0.005    0.000 {method 'findall' of 're.Pattern' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'finditer' of 're.Pattern' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method 'flush' of 'zlib.Decompress' objects}\n",
      "       20    0.000    0.000    0.000    0.000 {method 'format' of 'str' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {method 'fullmatch' of 're.Pattern' objects}\n",
      "       10    0.000    0.000    0.000    0.000 {method 'get' of '_contextvars.ContextVar' objects}\n",
      "    64713    0.011    0.000    0.011    0.000 {method 'get' of 'dict' objects}\n",
      "       10    0.000    0.000    0.000    0.000 {method 'get' of 'multidict._multidict.MultiDictProxy' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'get_loop' of '_asyncio.Future' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {method 'get_nowait' of '_queue.SimpleQueue' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method 'getall' of 'multidict._multidict.MultiDictProxy' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method 'getpeercert' of '_ssl._SSLSocket' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method 'getpeername' of '_socket.socket' objects}\n",
      "        7    0.000    0.000    0.000    0.000 {method 'getrandbits' of '_random.Random' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method 'getsockname' of '_socket.socket' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method 'getsockopt' of '_socket.socket' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'getvalue' of '_io.StringIO' objects}\n",
      "       64    0.000    0.000    0.000    0.000 {method 'group' of 're.Match' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'groups' of 're.Match' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'hexdigest' of '_hashlib.HMAC' objects}\n",
      "        7    0.000    0.000    0.000    0.000 {method 'isdigit' of 'str' objects}\n",
      "       20    0.000    0.000    0.000    0.000 {method 'isidentifier' of 'str' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'isoformat' of 'datetime.datetime' objects}\n",
      "    24261    0.004    0.000    0.004    0.000 {method 'items' of 'dict' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'items' of 'mappingproxy' objects}\n",
      "       10    0.000    0.000    0.000    0.000 {method 'items' of 'multidict._multidict.MultiDict' objects}\n",
      "       61    0.000    0.000    0.000    0.000 {method 'join' of 'bytes' objects}\n",
      "    15439    0.003    0.000    0.003    0.000 {method 'join' of 'str' objects}\n",
      "    11518    0.002    0.000    0.002    0.000 {method 'keys' of 'dict' objects}\n",
      "    11131    0.002    0.000    0.002    0.000 {method 'lower' of 'str' objects}\n",
      "       46    0.000    0.000    0.000    0.000 {method 'match' of 're.Pattern' objects}\n",
      "     85/1    0.581    0.007    0.000    0.000 {method 'poll' of 'select.epoll' objects}\n",
      "      174    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}\n",
      "    13587    0.002    0.000    0.002    0.000 {method 'pop' of 'list' objects}\n",
      "      216    0.000    0.000    0.000    0.000 {method 'popleft' of 'collections.deque' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {method 'put' of '_queue.SimpleQueue' objects}\n",
      "       56    0.000    0.000    0.000    0.000 {method 'read' of '_ssl.MemoryBIO' objects}\n",
      "      208    0.002    0.000    0.002    0.000 {method 'read' of '_ssl._SSLSocket' objects}\n",
      "        6    0.000    0.000    0.000    0.000 {method 'recv' of '_socket.socket' objects}\n",
      "       35    0.000    0.000    0.000    0.000 {method 'recv_into' of '_socket.socket' objects}\n",
      "       10    0.000    0.000    0.000    0.000 {method 'register' of 'select.epoll' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'release' of '_thread.lock' objects}\n",
      "        9    0.000    0.000    0.000    0.000 {method 'remove' of 'set' objects}\n",
      "        9    0.000    0.000    0.000    0.000 {method 'replace' of 'str' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'result' of '_asyncio.Future' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method 'reverse' of 'list' objects}\n",
      "        9    0.000    0.000    0.000    0.000 {method 'rstrip' of 'str' objects}\n",
      "   164/24    0.000    0.000    0.027    0.001 {method 'run' of '_contextvars.Context' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method 'search' of 're.Pattern' objects}\n",
      "    25/23    0.000    0.000    0.000    0.000 {method 'send' of '_socket.socket' objects}\n",
      "    68/64    0.000    0.000    0.623    0.010 {method 'send' of 'coroutine' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'set_result' of '_asyncio.Future' objects}\n",
      "       10    0.000    0.000    0.000    0.000 {method 'setblocking' of '_socket.socket' objects}\n",
      "       10    0.000    0.000    0.000    0.000 {method 'setsockopt' of '_socket.socket' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method 'shutdown' of '_ssl._SSLSocket' objects}\n",
      "        9    0.000    0.000    0.000    0.000 {method 'split' of 'bytes' objects}\n",
      "       15    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method 'unregister' of 'select.epoll' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {method 'update' of '_hashlib.HMAC' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}\n",
      "       14    0.000    0.000    0.000    0.000 {method 'upper' of 'str' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {method 'values' of 'collections.OrderedDict' objects}\n",
      "      229    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
      "        6    0.000    0.000    0.000    0.000 {method 'values' of 'mappingproxy' objects}\n",
      "       10    0.000    0.000    0.000    0.000 {method 'write' of '_io.StringIO' objects}\n",
      "       35    0.000    0.000    0.000    0.000 {method 'write' of '_ssl.MemoryBIO' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method 'write' of '_ssl._SSLSocket' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    cProfile.run(\"asyncio.run(main())\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
