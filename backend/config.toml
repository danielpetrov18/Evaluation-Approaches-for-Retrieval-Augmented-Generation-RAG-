[completion]
provider = "litellm"
concurrent_request_limit = 1

[completion.generation_config]
model = "ollama/llama3.1"
temperature = 0.1
top_p = 1
max_tokens_to_sample = 1024
stream = false

[completion.generation_config.add_generation_kwargs]

[database]
provider = "postgres"

[embedding]
provider = "ollama"
base_model = "nomic-embed-text"
base_dimension = 768
batch_size = 32
add_title_as_prefix = true
concurrent_request_limit = 32

[ingestion]
provider = "unstructured_local"
strategy = "auto"
chunking_strategy = "by_title"
new_after_n_chars = 512
max_characters = 1024
combine_under_n_chars = 128
similarity_threshold = 0.7
chunk_size = "512"
chunk_overlap = "128"
excluded_parsers = [
    "mp4",
]
chunks_for_document_summary = 16
document_summary_model = "ollama/llama3.1"

[ingestion.extra_parsers]
pdf = "zerox"

[ingestion.chunk_enrichment_settings]
enable_chunk_enrichment = true
strategies = [
    "semantic",
]
semantic_neighbors = 5
semantic_similarity_threshold = "0.65"

[ingestion.chunk_enrichment_settings.generation_config]
model = "ollama/llama3.1"

[logging]
level = "DEBUG"
