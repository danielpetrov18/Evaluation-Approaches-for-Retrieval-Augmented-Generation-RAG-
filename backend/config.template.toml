[completion]
provider = "litellm"
concurrent_request_limit = 1

  [completion.generation_config]
  model = "ollama/${OLLAMA_CHAT_MODEL}"
  temperature = 0.1
  top_p = 1
  max_tokens_to_sample = 1_024
  stream = false
  add_generation_kwargs = { }

[database]
provider = "postgres"

[embedding]
provider = "ollama"
base_model = "nomic-embed-text"
base_dimension = 768
batch_size = 32
add_title_as_prefix = true
concurrent_request_limit = 32

[ingestion]
provider = "unstructured_local"
strategy = "auto"
chunking_strategy = "by_title"
new_after_n_chars = 512
max_characters = 1024
combine_under_n_chars = 128
similarity_threshold = 0.7
chunk_size = "${CHUNK_SIZE}"
chunk_overlap = "${CHUNK_OVERLAP}"
excluded_parsers = [ "mp4" ]
chunks_for_document_summary = 16
document_summary_model = "ollama/${OLLAMA_CHAT_MODEL}"

  [ingestion.extra_parsers]
  pdf = "zerox"

  [ingestion.chunk_enrichment_settings]
  enable_chunk_enrichment = true # disabled by default
  strategies = ["semantic"]      # Only similarity search, neighbor search is also available
  semantic_neighbors = 5         # Similar chunks to consider
  semantic_similarity_threshold = "${SIMILARITY_THRESHOLD}"  # Minimum similarity score to consider a chunk for enrichment
  generation_config = { model = "ollama/${OLLAMA_CHAT_MODEL}" }

[logging]
level = "DEBUG"