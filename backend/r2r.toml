[database]
provider = "postgres" 

[completion]
provider = "litellm"
concurrent_request_limit = 1

    [completion.generation_config]
    model = "ollama/llama3.1"
    temperature = 0.4
    top_p = 0.95
    max_tokens_to_sample = 1_024
    stream = false
    add_generation_kwargs = {}

[ingestion]
provider = "unstructured_local"
strategy = "auto"
chunking_strategy = "by_title"
new_after_n_chars = 512
max_characters = 1_024
combine_under_n_chars = 128
overlap = 128

    [ingestion.extra_parsers]
    pdf = "zerox"

    [ingestion.chunk_enrichment_settings]
    enable_chunk_enrichment = true 
    strategies = ["semantic", "neighborhood"]
    forward_chunks = 3            
    backward_chunks = 3           
    semantic_neighbors = 10       
    semantic_similarity_threshold = 0.70  
    generation_config = { model = "ollama/llama3.1" }

[embedding]
provider = "ollama"
base_model = "mxbai-embed-large"
base_dimension = 1024
batch_size = 32
concurrent_request_limit = 32
add_title_as_prefix = true

[logging]
level = "DEBUG"