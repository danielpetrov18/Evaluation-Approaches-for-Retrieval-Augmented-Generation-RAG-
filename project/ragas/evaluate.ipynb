{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Retrieval-Augmented Generation (RAG) Applications: Importance and Impact  \n",
    "\n",
    "Evaluating RAG-based applications is crucial to ensure the reliability, relevance, and efficiency of their responses. Unlike using LLMs by themselves, RAG-based applications combine the power of Large-Language-Models with vector stores and retrieval components, which introduces complexity. Hence, evaluation of different components in RAG pipelines is crucial. \n",
    "\n",
    "#### **Key Pitfalls in RAG Applications**  \n",
    "- **Hallucination**: Even with retrieved context, LLMs can still generate inaccurate or misleading responses.  \n",
    "- **Irrelevant Retrieval**: Poor retrieval results in the model relying on incomplete or incorrect context.  \n",
    "- **Latency & Efficiency**: Combining retrieval and generation can introduce delays, requiring performance optimization. \n",
    "\n",
    "#### **Advantages of Proper Evaluation**  \n",
    "- **Improved Accuracy**: Ensures the factual correctness of responses by using a knowledge base.  \n",
    "- **Enhanced Relevance**: Helps refine retrieval mechanisms, ensuring responses are more context-aware.  \n",
    "- **Robustness & Adaptability**: Identifies failure points, enabling continuous improvements to model behavior.  \n",
    "\n",
    "#### **Impact on Future Development**  \n",
    "A structured evaluation process enables better benchmarking, allowing for iterative improvements in retrieval models, embeddings, and LLM fine-tuning. It also promotes transparency in AI decision-making, paving the way for more explainable and ethical AI systems and thus gaining a wider adoption. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "\"\"\"\n",
    "(OPTIONAL STEP)\n",
    "\n",
    "If one wants to have a structured way of displaying the results of the evaluation, \n",
    "one can do so from here: https://app.ragas.io/. \n",
    "There one can sign-up and receive an app token. Once retrieved you can create a `env` file with the following content:\n",
    "RAGAS_APP_TOKEN=\"apt.......-9f6ed\"\n",
    "\n",
    "Additionally, Langsmith can be used to trace the evaluation in real-time, since RAGAs is built with Langchain.\n",
    "For that follow the link and follow the steps:\n",
    "https://docs.ragas.io/en/latest/howtos/integrations/langsmith/#tracing-ragas-metrics\n",
    "\"\"\"\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the evaluation dataset\n",
    "\n",
    "According to **RAGAs** an **evaluation dataset** is a homogeneous collection of **data samples** with the sole purpose of measuring the capabilities and performance of an AI application.\n",
    "\n",
    "- **Structure**:\n",
    "\n",
    "    - Contains either **SingleTurnSample** or **MultiTurnSample** objects instances, each of them representing a unique interaction between a **Scenario**.\n",
    "\n",
    "    - **NOTE**: The dataset can contain **ONLY** a single type of sample type. They cannot be mixed together into a single dataset.\n",
    "\n",
    "**Samples** represent a single unit of interaction with the underlying system. As mentioned they can be either **SingleTurnSample** or **MultiTurnSample**.\n",
    "\n",
    "For this project I focus solely on **SingleTurnSample** objects, since I'm evaluating independent queries, not multi-turn interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset\n",
    "\n",
    "eval_dataset = EvaluationDataset.from_jsonl(\"dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "from ragas.evaluation import evaluate\n",
    "from ragas.run_config import RunConfig\n",
    "from ragas.cache import DiskCacheBackend\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "ollama_llm = OllamaLLM(\n",
    "    model=\"llama3.1\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.1,\n",
    "    num_ctx=24000,\n",
    "    format=\"json\"\n",
    ")\n",
    "\n",
    "ollama_embeddings = OllamaEmbeddings(\n",
    "    model=\"mxbai-embed-large\",\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    timeout = 43200, # twelve hourses, depending on GPU, model, testsize, etc -> can experinment\n",
    "    max_wait = 30,\n",
    "    log_tenacity = True\n",
    ")\n",
    "\n",
    "cacher = DiskCacheBackend(\".cache\")\n",
    "\n",
    "llm = LangchainLLMWrapper(\n",
    "    langchain_llm=ollama_llm,\n",
    "    run_config=run_config,\n",
    "    cache=cacher\n",
    ")\n",
    "\n",
    "embeddings = LangchainEmbeddingsWrapper(\n",
    "    embeddings=ollama_embeddings,\n",
    "    run_config=run_config,\n",
    "    cache=cacher\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "Metrics in **RAGAs** can be classified depending on the mechanism they evaluate the application as:\n",
    "\n",
    "- **LLM-based** - where a LLM is used to perform the evaluation. There might be more than one queries submitted to the LLM to evaluate the application on a single metric. Furthermore, this type of metrics mimic a user, since they tend to be quite non-deterministic and unpredictable. \n",
    "\n",
    "- **Non-LLM based** - where no LLM is required or used to perform the evaluation. These metrics tend to be much faster and predictable than the previous type. However, in my opinion are maybe not the best for evaluating a RAG system where a single query can be submitted multiple times and each time a new response might be generated.\n",
    "\n",
    "The previously mentioned metric types can be further classified into **SingleTurnMetric** and **MultiTurnMetric** respectively. I would like to note again that in this project the **SingleTurnMetric** would be relevant and used.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Context Precision**\n",
    "\n",
    "**Context Precision** is a crucial metric for evaluating the **retrieval capabilities** of a RAG (Retrieval-Augmented Generation) system. Since the **LLM's response** depends heavily on the **context retrieved from the knowledge base**, it is essential to have a reliable retrieval mechanism that fetches **only relevant** information.  \n",
    "\n",
    "By achieving **high context precision**, the system ensures that the retrieved information is **more relevant**, leading to **more accurate responses** and **reduced hallucinations**.\n",
    "\n",
    "> **TL;DR:** What fraction of the retrieved chunks are **actually relevant**?\n",
    "\n",
    "---\n",
    "\n",
    "### **Formula:**\n",
    "For each retrieved chunk at **rank \\( k \\)**:\n",
    "\n",
    "$\\text{Precision@k} = \\frac{\\text{Number of relevant chunks at rank k}}{\\text{Total number of retrieved chunks at rank k}}$\n",
    "\n",
    "We then compute **Context Precision @ K** as the **mean of all Precision@k values**, weighted by relevance:\n",
    "\n",
    "$\\text{Context Precision @ K} = \\frac{\\sum_{k=1}^{K} \\text{Precision@k} \\times \\text{Relevance}(k)}{\\sum_{k=1}^{K} \\text{Relevance}(k)}$\n",
    "\n",
    "Where:\n",
    "- **Precision@k** is the proportion of relevant chunks at rank \\( k \\).\n",
    "- **Relevance(k)** is a binary indicator (1 if the chunk at rank \\( k \\) is relevant, 0 otherwise).\n",
    "- **K** is the total number of retrieved chunks.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Calculation**\n",
    "Suppose our **retriever fetches 4 chunks**, and **2 of them** are relevant. We calculate **Precision@k** at each rank:\n",
    "\n",
    "| Rank \\( k \\) | Retrieved Chunk | Relevant? | Precision@k |\n",
    "|-------------|----------------|------------|-------------|\n",
    "| 1           | ✅ Relevant      | ✅ (1)      | \\( $\\frac{1}{1}$ = 1.00 \\) |\n",
    "| 2           | ❌ Not Relevant  | ❌ (0)      | \\( $\\frac{1}{2}$ = 0.50 \\) |\n",
    "| 3           | ✅ Relevant      | ✅ (1)      | \\( $\\frac{2}{3}$ = 0.67 \\) |\n",
    "| 4           | ❌ Not Relevant  | ❌ (0)      | \\( $\\frac{2}{4}$ = 0.50 \\) |\n",
    "\n",
    "Now, using the **Context Precision formula**:\n",
    "\n",
    "\n",
    "$\\text{Context Precision @ 4} = \\frac{(1.00 \\times 1) + (0.50 \\times 0) + (0.67 \\times 1) + (0.50 \\times 0)}{2} = $\n",
    "\n",
    "= $\\frac{1.00 + 0 + 0.67 + 0}{2} = \\frac{1.67}{2}$ = 0.835\n",
    "\n",
    "Thus, **Context Precision @ 4 = 0.835**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why is This Useful?**\n",
    "\n",
    "- **Evaluates Retriever Quality** → Ensures retrieved chunks contain **relevant** information.\n",
    "\n",
    "- **Improves RAG Performance** → Helps **reduce hallucinations** and improves **LLM accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import LLMContextPrecisionWithoutReference\n",
    "\n",
    "context_precision = LLMContextPrecisionWithoutReference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Context Recall**\n",
    "\n",
    "**Context Recall** measures how many of the **relevant documents (or pieces of information)** were successfully retrieved. This metric ensures that the retrieval process does not **miss important information** that could be used to generate a response. A **higher recall** means that fewer relevant documents were left out, making it crucial for applications that prioritize completeness over precision. Since recall focuses on **not missing relevant data**, it always requires a reference set for comparison.\n",
    "\n",
    "### TL;DR:\n",
    "> **How many of the relevant chunks were actually retrieved?**\n",
    "\n",
    "---\n",
    "\n",
    "### LLM-Based Context Recall\n",
    "\n",
    "This metric evaluates recall by analyzing the **claims** in the reference (expected) response and checking whether they can be attributed to the retrieved context. In an ideal scenario, **all claims** in the reference answer should be **supported** by the retrieved context.\n",
    "\n",
    "### **Formula**:\n",
    "\n",
    "**Context Recall @ k** = \\( $\\frac{\\text{Number of claims in the reference supported by the retrieved context}}{\\text{Total number of claims in the reference}}$ \\), where `k` is the rank\n",
    "\n",
    "A recall score **closer to 1** indicates that most relevant data has been successfully retrieved, while a **lower recall** means that key information was missed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import LLMContextRecall\n",
    "\n",
    "llm_context_recall = LLMContextRecall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Context Entities Recall**\n",
    "\n",
    "### TL;DR:\n",
    "> **It measures the fraction of entities present in both context and reference relative to the total number of entities in the reference.**\n",
    "\n",
    "---\n",
    "\n",
    "### Definition:\n",
    "`ContextEntityRecall` metric measures the recall of the retrieved context, based on the number of entities present in both `reference` and `retrieved_contexts`, relative to the total number of entities in the `reference`. In simple terms, it evaluates how well the retrieved contexts capture the entities from the original reference.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula:\n",
    "\n",
    "To compute this metric, we define two sets:\n",
    "\n",
    "- **RE**: The set of entities in the reference.\n",
    "- **RCE**: The set of entities in the retrieved contexts.\n",
    "\n",
    "We determine the number of entities common to both sets (**RCE** $\\cap$  **RE**) and divide it by the total number of entities in the reference (**RE**). The formula is:\n",
    "\n",
    "$ \\text{Context Entity Recall} = \\frac{\\text{Number of common entities between RCE and RE}}{\\text{Total number of entities in RE}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import ContextEntityRecall\n",
    "\n",
    "context_entity_recall = ContextEntityRecall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Noise Sensitivity**\n",
    "\n",
    "### TL;DR:\n",
    "> **How prone is the system to generating incorrect claims from retrieved contexts?**\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition**:\n",
    "**Noise Sensitivity** measures how often a system makes errors by providing incorrect responses when utilizing either relevant or irrelevant retrieved documents. This metric evaluates the robustness of a Retrieval-Augmented Generation (RAG) system against potentially misleading or noisy information.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conceptual Insight**:\n",
    "The metric fundamentally tests how easily an LLM can be \"tricked\" into generating factually incorrect responses. It distinguishes between two critical scenarios:\n",
    "\n",
    "1. **Relevant Context Noise**: \n",
    "   - More subtle and dangerous\n",
    "   - Noise is camouflaged within seemingly pertinent information\n",
    "   - High risk of inadvertently incorporating incorrect claims\n",
    "\n",
    "2. **Irrelevant Context Noise**:\n",
    "   - A robust LLM should completely resist this\n",
    "   - Contexts unrelated to the user's query\n",
    "   - Zero tolerance for incorporating unrelated information\n",
    "\n",
    "---\n",
    "\n",
    "### LLM-Based Noise Sensitivity\n",
    "\n",
    "The metric assesses noise sensitivity by decomposing the response and reference into individual claims, then analyzing:\n",
    "- Whether claims are correct according to the ground truth\n",
    "- Whether incorrect claims can be attributed to retrieved contexts (either relevant or irrelevant)\n",
    "\n",
    "---\n",
    "\n",
    "### **Calculation Approach**:\n",
    "\n",
    "1. **Decompose Statements**: Break down the reference and response into individual claims\n",
    "2. **Context Verification**: Check if claims can be supported by retrieved contexts\n",
    "3. **Incorrect Claim Identification**: Determine which claims are unsupported by the ground truth\n",
    "\n",
    "---\n",
    "\n",
    "### **Formula**:\n",
    "\n",
    "**Noise Sensitivity** = \\( $\\frac{\\text{Number of incorrect claims in response}}{\\text{Total number of claims}}$ \\)\n",
    "\n",
    "A score **closer to 0** indicates better performance, suggesting:\n",
    "- Fewer incorrect claims\n",
    "- Less influence from noisy or irrelevant contexts\n",
    "- More robust response generation\n",
    "\n",
    "---\n",
    "\n",
    "### **Modes**:\n",
    "- **Relevant Mode** (default): Focuses on noise in relevant contexts\n",
    "- **Irrelevant Mode**: Analyzes noise from irrelevant retrieved documents\n",
    "\n",
    "---\n",
    "\n",
    "### **Evaluation Expectations**:\n",
    "- **Relevant Context**: Some noise tolerance, but should be minimal\n",
    "- **Irrelevant Context**: Virtually zero noise should be incorporated\n",
    "  - A high-quality LLM should completely disregard irrelevant information\n",
    "  - Any noise from irrelevant contexts indicates a significant vulnerability\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Scenario**:\n",
    "Consider a query about the Life Insurance Corporation of India (LIC):\n",
    "- **Reference**: Describes LIC as the largest insurance company established in 1956\n",
    "- **Response**: Includes an unsubstantiated claim about LIC's financial stability\n",
    "- **Retrieved Contexts**: Mix of relevant and irrelevant information\n",
    "- **Noise Sensitivity**: Measures how the response incorporates unverified claims\n",
    "\n",
    "### **Key Insights**:\n",
    "- Helps identify system vulnerabilities to hallucination\n",
    "- Provides a quantitative measure of response reliability\n",
    "- Distinguishes between subtle (relevant) and gross (irrelevant) information distortions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import NoiseSensitivity\n",
    "\n",
    "noise_sensitivity = NoiseSensitivity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Response Relevancy**\n",
    "\n",
    "### TL;DR:\n",
    "> **How well does the answer address the original user query?**\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition**:\n",
    "**Response Relevancy** measures how closely an AI-generated response aligns with the original user input. It evaluates the answer's ability to directly and appropriately address the user's question, penalizing responses that are incomplete, off-topic, or include unnecessary details. It doesn't judge the **factual accuracy**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conceptual Insight**:\n",
    "The metric focuses on the semantic alignment between:\n",
    "- The original user query\n",
    "- The generated response\n",
    "\n",
    "Key evaluation criteria:\n",
    "1. **Direct Address**: Does the answer directly tackle the user's question?\n",
    "2. **Information Completeness**: Does the response provide sufficient information?\n",
    "3. **Topic Coherence**: Does the answer stay focused on the query's intent?\n",
    "\n",
    "---\n",
    "\n",
    "### **Calculation Approach**:\n",
    "\n",
    "1. **Question Generation**: \n",
    "   - Use the LLM to generate artificial questions based on the response\n",
    "   - Default is to create 3 variant questions\n",
    "   - These questions should capture the essence of the response\n",
    "\n",
    "2. **Semantic Similarity**:\n",
    "   - Compute cosine similarity between:\n",
    "     - Original user input embedding\n",
    "     - Embeddings of generated questions\n",
    "   - Measures how closely the questions match the original query\n",
    "\n",
    "3. **Scoring**:\n",
    "   - Average the cosine similarity scores\n",
    "   - Higher scores indicate better relevance\n",
    "   - Scores typically range between 0 and 1\n",
    "\n",
    "---\n",
    "\n",
    "### **Formula**:\n",
    "\n",
    "**Response Relevancy** $\\rightarrow$ $\\frac{1}{N} \\sum_{i=1}^{N} \\text{cosine\\_similarity}(E_{g_{i}}, E_{o})$ \n",
    "\n",
    "Where:\n",
    "- $E_{g_{i}}$: Embedding of the i-th generated question\n",
    "- $E_{o}$: Embedding of the original user input\n",
    "- N: Number of generated questions (default: 3)\n",
    "\n",
    "---\n",
    "\n",
    "### **Evaluation Expectations**:\n",
    "- **High Score (Close to 1)**: \n",
    "  - Answer directly addresses the query\n",
    "  - Comprehensive and focused response\n",
    "  - Minimal irrelevant information\n",
    "\n",
    "- **Low Score (Close to 0)**: \n",
    "  - Response is off-topic\n",
    "  - Incomplete or evasive answer\n",
    "  - Includes excessive unrelated details\n",
    "\n",
    "---\n",
    "\n",
    "### **Important Limitations**:\n",
    "- **No Factuality Check**: \n",
    "  - Measures relevance, not accuracy\n",
    "  - Does not verify the truthfulness of the response\n",
    "- **Embedding-Based**: \n",
    "  - Relies on semantic similarity\n",
    "  - May not catch nuanced relevance\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Scenario**:\n",
    "- **User Query**: \"When was the first Super Bowl?\"\n",
    "- **Good Response**: \"The first Super Bowl was held on Jan 15, 1967, between the Green Bay Packers and Kansas City Chiefs.\"\n",
    "- **Poor Response**: \"Football is a popular sport in the United States with many interesting historical moments.\"\n",
    "\n",
    "### **Key Insights**:\n",
    "- Helps evaluate response quality beyond simple keyword matching\n",
    "- Provides a quantitative measure of semantic alignment\n",
    "- Supports improving AI system's query understanding\n",
    "- Identifies potential issues with off-topic or unfocused responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import ResponseRelevancy\n",
    "\n",
    "response_relevancy = ResponseRelevancy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Faithfulness**\n",
    "\n",
    "### TL;DR:\n",
    "> **How factually consistent is the response with the retrieved context?**\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition**:\n",
    "The **Faithfulness** metric measures how factually consistent a response is with the retrieved context. It ranges from 0 to 1, with higher scores indicating better consistency.\n",
    "\n",
    "A response is considered faithful if all its claims can be supported by the retrieved context.\n",
    "\n",
    "---\n",
    "\n",
    "### **Calculation Approach**:\n",
    "\n",
    "1. **Identify Claims**: \n",
    "   - Break down the response into individual statements\n",
    "   - Examine each claim systematically\n",
    "\n",
    "2. **Context Verification**:\n",
    "   - Check each claim to see if it can be inferred from the retrieved context\n",
    "   - Determine the support level of each statement\n",
    "\n",
    "3. **Scoring**:\n",
    "   - Compute the faithfulness score using the formula:\n",
    "     \n",
    "     $\\text{Faithfulness Score} = \\frac{\\text{Number of claims supported by the retrieved context}}{\\text{Total number of claims in the response}}$\n",
    "\n",
    "---\n",
    "\n",
    "### **Example**:\n",
    "\n",
    "**Question**: Where and when was Einstein born?\n",
    "\n",
    "**Context**: Albert Einstein (born 14 March 1879) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time.\n",
    "\n",
    "**High Faithfulness Answer**: Einstein was born in Germany on 14th March 1879.\n",
    "\n",
    "**Low Faithfulness Answer**: Einstein was born in Germany on 20th March 1879.\n",
    "\n",
    "#### Calculation Steps:\n",
    "- **Step 1**: Break the generated answer into individual statements\n",
    "- **Step 2**: Verify if each statement can be inferred from the given context\n",
    "- **Step 3**: Apply the faithfulness formula\n",
    "\n",
    "---\n",
    "\n",
    "### **Evaluation Expectations**:\n",
    "- **Perfect Score (1.0)**: \n",
    "  - All claims are directly supported by the context\n",
    "  - No extraneous or unsupported information\n",
    "\n",
    "- **Partial Score**: \n",
    "  - Some claims are supported\n",
    "  - Partial consistency with the retrieved context\n",
    "\n",
    "- **Low Score (Close to 0)**: \n",
    "  - Most claims cannot be verified\n",
    "  - Significant deviation from the original context\n",
    "\n",
    "---\n",
    "\n",
    "### **Advanced Verification**:\n",
    "**HEM-2.1-Open** can be used as a classifier model to:\n",
    "- Detect hallucinations in LLM-generated text\n",
    "- Cross-check claims with the given context\n",
    "- Efficiently determine claim inferability\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Insights**:\n",
    "- Measures the factual consistency of AI-generated responses\n",
    "- Helps identify potential hallucinations or fabrications\n",
    "- Provides a quantitative assessment of contextual alignment\n",
    "- Supports improving the reliability of AI-generated content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type HHEMv2Config to instantiate a model of type HHEMv2. This is not supported for all configurations of models and can yield errors.\n",
      "/home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import FaithfulnesswithHHEM\n",
    "\n",
    "faithfulness = FaithfulnesswithHHEM(device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import FactualCorrectness\n",
    "\n",
    "\"\"\"\n",
    "Measures the factual consistency between the reference and the actual response by the LLM.\n",
    "\n",
    "It uses true positives, false positives, false negatives.\n",
    "TP = claim/s which is/are supported both by the reference and the response\n",
    "FP = claim/s which is/are supported by the response, not by the reference\n",
    "FN = claim/s which is/are supported by the reference, not response\n",
    "\n",
    "Precision, Recall, and F1 modes\n",
    "\n",
    "Precision = TP / (TP + FP) => everything which is in the response (even the redundant/missing data)\n",
    "Recall = TP / (TP + FN) => all claims which are part and not part of the response\n",
    "F1 = 2 * Precision * Recall / (Precision + Recall)\n",
    "\n",
    "https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/factual_correctness/\n",
    "\"\"\"\n",
    "\n",
    "factual_correctness = FactualCorrectness(atomicity=\"high\", coverage=\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 260/260 [3:15:39<00:00, 45.15s/it]   \n"
     ]
    }
   ],
   "source": [
    "result = evaluate(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=[context_precision, llm_context_recall, response_relevancy, faithfulness, factual_correctness],\n",
    "    llm=llm,\n",
    "    embeddings=embeddings,\n",
    "    run_config=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm_context_precision_with_reference': 0.7500, 'context_recall': 0.5730, 'answer_relevancy': 0.9423, 'faithfulness_with_hhem': 0.4200, 'factual_correctness(mode=f1)': 0.4952}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = result.to_pandas()\n",
    "result_df.to_csv('metrics_evaluation.csv', index=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results uploaded! View at https://app.ragas.io/dashboard/alignment/evaluation/4cd1785f-206c-474d-a92e-f3a90538aa51\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://app.ragas.io/dashboard/alignment/evaluation/4cd1785f-206c-474d-a92e-f3a90538aa51'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.upload()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
