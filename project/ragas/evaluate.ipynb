{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\"\"\"\n",
    "Loads the ragas app token which enables one to upload results from evaluations for later reference\n",
    "Additionally, I have a Langsmith api key which enables one to track the evaluation in real time:\n",
    "    https://docs.ragas.io/en/latest/howtos/integrations/langsmith/#tracing-ragas-metrics\n",
    "\"\"\"\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ragas import EvaluationDataset\n",
    "\n",
    "def load_dataset(filepath: str = \"dataset.jsonl\") -> EvaluationDataset:\n",
    "    return EvaluationDataset.from_jsonl(filepath)\n",
    "\n",
    "eval_dataset = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "from ragas.evaluation import evaluate\n",
    "from ragas.run_config import RunConfig\n",
    "from ragas.cache import DiskCacheBackend\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "ollama_llm = OllamaLLM(\n",
    "    model=\"llama3.1\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.1,\n",
    "    num_ctx=24000,\n",
    "    format=\"json\"\n",
    ")\n",
    "\n",
    "ollama_embeddings = OllamaEmbeddings(\n",
    "    model=\"mxbai-embed-large\",\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    timeout = 14400, # four hours, depending on GPU, model, testsize, etc -> can experinment\n",
    "    max_wait = 30,\n",
    "    log_tenacity = True\n",
    ")\n",
    "\n",
    "cacher = DiskCacheBackend(\".cache\")\n",
    "\n",
    "llm = LangchainLLMWrapper(\n",
    "    langchain_llm=ollama_llm,\n",
    "    run_config=run_config,\n",
    "    cache=cacher\n",
    ")\n",
    "\n",
    "embeddings = LangchainEmbeddingsWrapper(\n",
    "    embeddings=ollama_embeddings,\n",
    "    run_config=run_config,\n",
    "    cache=cacher\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import LLMContextPrecisionWithReference\n",
    "\n",
    "\"\"\"\n",
    "Measures the number of relevant chunks with respect to the number of all chunks at a given rank.\n",
    "\n",
    "Example:\n",
    "    We have 4 chunks in total that were retrieved by RAG and 2 of those were deemed relevant\n",
    "    for answering the question of the user. For each rank k (1, 2, 3, 4), we calculate the \n",
    "    precision as the number of relevant chunks divided by the number of chunks at that rank.\n",
    "    \n",
    "    Assuming chunks at rank 1 and 3 were relevant it would look like this:\n",
    "        precision @ 1 => 1/1 = 1 (since the chunk is relevant and we have only 1 chunk at rank 1)\n",
    "        precision @ 2 => 1/2 = 0.5 (since there's only one relevant chunk, but 2 chunks at rank 2)\n",
    "        precision @ 3 => 2/3 = 0.67 (since 2 chunks were relevant at rank 3 were we have 3 in total)\n",
    "        precision @ 4 => 2/4 = 0.5 (since 2 out of all chunks were deemed relevant at rank 4)\n",
    "        \n",
    "        Final score in this case would be:\n",
    "        Context precision @ (K = 4) => (presicion @ 1 + precision @ 2 + precision @ 3 + precision @ 4) / # relevant chunks\n",
    "            => (1*1 + 0.5*0 + 0.67*1 + 0.5*0) / 2 = 0.835\n",
    "            \n",
    "    Abstract formula:\n",
    "        precision @ k = (true positives @ k) / (true positives @ k + false positives @ k)\n",
    "        context precision @ (K = n) = (precision @ 1 * v1 + ... + precision @ n * vn) / # relevant chunks\n",
    "            where v1, ..., vn are in {0,1} => so either a chunk is relevant or not\n",
    "\n",
    "https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/context_precision/\n",
    "\"\"\"\n",
    "context_precision = LLMContextPrecisionWithReference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import LLMContextRecall\n",
    "\n",
    "\"\"\"\n",
    "Measures how much of the relevant documents / pieces of information were retrieved, where the focus\n",
    "lies on not missing any relevant / important data. The previous metric focuses more on how \n",
    "relevant the retrieved chunks are. This metric is all about making sure that we retrieve \n",
    "all the neccesary information, without missing important data.\n",
    "Higher value for this metric means no missed or very few missed chunks.\n",
    "\n",
    "Abstract formula:\n",
    "    Context Recall = the intersection of claims in reference and retrieved context / Total number of claims in the reference\n",
    "\n",
    "https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/context_recall/\n",
    "\"\"\"\n",
    "llm_context_recall = LLMContextRecall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import ResponseRelevancy\n",
    "\n",
    "\"\"\"\n",
    "This metric measures the redundancy or lack of information in the answer with respect to the users query.\n",
    "The idea is that we use the response from the LLM, depending on the stricktness value (default 3) we use the LLM to\n",
    "create 3 artificial questions Qk(1-3) and we compute the vector similarity between the original query the user\n",
    "submitted and the questions we were able to infer from the answer of the LLM. Values scoring high means that the\n",
    "answer is relevant with respect to the question. \n",
    "\n",
    "NOTE: This doesn't measure factuality, since no reference is used.\n",
    "\"\"\"\n",
    "\n",
    "response_relevancy = ResponseRelevancy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type HHEMv2Config to instantiate a model of type HHEMv2. This is not supported for all configurations of models and can yield errors.\n",
      "/home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import FaithfulnesswithHHEM\n",
    "\n",
    "\"\"\"\n",
    "The Faithfulness metric measures how factually consistent a response is with the retrieved context. \n",
    "It ranges from 0 to 1, with higher scores indicating better consistency.\n",
    "\n",
    "A response is considered faithful if all its claims can be supported by the retrieved context.\n",
    "\n",
    "To calculate this:\n",
    "1. Identify all the claims in the response.\n",
    "2. Check each claim to see if it can be inferred from the retrieved context.\n",
    "3. Compute the faithfulness score using the formula:\n",
    "\n",
    "Faithfulness Score = Number of claims supported by the retrieved context / Total number of claims in the response\n",
    "\n",
    "This metric uses a particular model specificially trained to detect hallucinations.\n",
    "It will be used in the second step, when the claims from the response are compared to the retrieved context.\n",
    "\"\"\"\n",
    "\n",
    "faithfulness = FaithfulnesswithHHEM(device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import FactualCorrectness\n",
    "\n",
    "\"\"\"\n",
    "Measures the factual consistency between the reference and the actual response by the LLM.\n",
    "\n",
    "It uses true positives, false positives, false negatives.\n",
    "TP = claim/s which is/are supported both by the reference and the response\n",
    "FP = claim/s which is/are supported by the response, not by the reference\n",
    "FN = claim/s which is/are supported by the reference, not response\n",
    "\n",
    "Precision, Recall, and F1 modes\n",
    "\n",
    "Precision = TP / (TP + FP) => everything which is in the response (even the redundant/missing data)\n",
    "Recall = TP / (TP + FN) => all claims which are part and not part of the response\n",
    "F1 = 2 * Precision * Recall / (Precision + Recall)\n",
    "\n",
    "https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/factual_correctness/\n",
    "\"\"\"\n",
    "\n",
    "factual_correctness = FactualCorrectness(atomicity=\"high\", coverage=\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 260/260 [3:15:39<00:00, 45.15s/it]   \n"
     ]
    }
   ],
   "source": [
    "result = evaluate(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=[context_precision, llm_context_recall, response_relevancy, faithfulness, factual_correctness],\n",
    "    llm=llm,\n",
    "    embeddings=embeddings,\n",
    "    run_config=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm_context_precision_with_reference': 0.7500, 'context_recall': 0.5730, 'answer_relevancy': 0.9423, 'faithfulness_with_hhem': 0.4200, 'factual_correctness(mode=f1)': 0.4952}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = result.to_pandas()\n",
    "result_df.to_csv('metrics_evaluation.csv', index=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results uploaded! View at https://app.ragas.io/dashboard/alignment/evaluation/4cd1785f-206c-474d-a92e-f3a90538aa51\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://app.ragas.io/dashboard/alignment/evaluation/4cd1785f-206c-474d-a92e-f3a90538aa51'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.upload()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
