{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confident AI\n",
    "\n",
    "1. In short **Confident AI** is a cloud-based platform part of the **DeepEval** framework, which stores **datasets**, **evaluations** and **monitoring data**. \n",
    "\n",
    "2. If you want to use **Confident AI** platform create an account from here: [Confident AI](https://www.confident-ai.com/)\n",
    "\n",
    "3. After signing-up an **API key** will be generated, which can be used to interact with the platform from inside the notebook.\n",
    "\n",
    "---\n",
    "\n",
    "Example of .env file:\n",
    "```bash\n",
    "DEEPEVAL_RESULTS_FOLDER=<folder> # Results of evaluations can be saved locally\n",
    "DEEPEVAL_API_KEY=<your api key>  # Relevant if you want to use Confident AI\n",
    "DEEPEVAL_TELEMETRY_OPT_OUT=\"YES\" # Remove telemetry\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🎉🥳 Congratulations! You've successfully logged in! 🙌 \n",
       "</pre>\n"
      ],
      "text/plain": [
       "🎉🥳 Congratulations! You've successfully logged in! 🙌 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from deepeval import login_with_confident_api_key\n",
    "\n",
    "# Loads the environment variables from a `.env` file.\n",
    "# If you want to use Confident AI be sure to create one in this directory.\n",
    "load_dotenv()\n",
    "\n",
    "deepeval_api_key: str = os.getenv(\"DEEPEVAL_API_KEY\")\n",
    "\n",
    "# You should get a message letting you know you are logged-in.\n",
    "login_with_confident_api_key(deepeval_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMTestCase\n",
    "\n",
    "Unlike **RAGAs**, where a single interaction between a user and the AI system is represented by either a **SingleTurnSample** or **MultiTurnSample**, in **DeepEval** there's the concept of so called **LLMTestCase**/**MLLMTestCase** and **ConversationalTestCase**. For this project the **LLMTestCase** will be of relevance. Just like in **RAGAs**, **LLMTestCase** objects have the same fields just different names - input, actual_output, expected_output, etc.\n",
    "\n",
    "![Image showcasing what a LLMTestCase is.](https://confident-docs.s3.amazonaws.com/llm-test-case.svg \"LLMTestCase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "**Evaluation** should be a crucial component of every single application which uses **AI**. **DeepEval** provides more than 14 metrics for evaluation so that one can very easily iterate towards a better LLM application. Each default metric uses **LLM-As-A-Judge**. Optionally, one can use the **GEval** to set a custom criteria for evaluation if neither of the other metrics meet the requirements. Alternatively, there's the **DAGMetric**, whose purpose is similar to the **GEval**, however it uses a graph and it's fully **deterministic**.\n",
    "\n",
    "When evaluating a test case, multiple metrics can be used and the test would be **positive** iff all the **metrics thresholds** have been exceeded and **negative** in any other case. \n",
    "\n",
    "Evaluation workflow:\n",
    "![Image of evaluation workflox](https://d2lsxfc3p6r9rv.cloudfront.net/workflow.png \"Evaluation comparison\")\n",
    "\n",
    "---\n",
    "\n",
    "**Confident AI** enables users to compare evaluation runs between one another and to compare results. In some instances a new evaluation run might yield better overall results, however have a **failing** test which was previously **successful**. This is known as **regression** and will be marked in **red**. Tests marked in **green** show an improvement. \n",
    "\n",
    "![Image of evaluation run comparsion](https://confident-docs.s3.us-east-1.amazonaws.com/comparison-page.png \"Evaluation comparison\")\n",
    "\n",
    "* Here is the LLM development workflow that is highly recommended with **Confident AI**: \n",
    "    - Curate datasets (unless you don't have one already available)\n",
    "    - Run evaluations with dataset\n",
    "    - Analyze evaluation results\n",
    "    - Improve LLM application based on evaluation results\n",
    "    - Run another evaluation on the same dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Dataset\n",
    "\n",
    "An **evaluation dataset** is just a collection of **LLMTestCase**- or so called **Golden** objects. A **Golden** is structurally the same as a **LLMTestCase**, however it has no `actual_output` and `retrieval_context` fields, which can be generated by your LLM at evaluation time.\n",
    "\n",
    "Datasets can be **pushed**, **stored** and **pulled** from **Confident AIs** platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic datasets\n",
    "\n",
    "**DeepEval** can be used to generate **synthetic dataset** as well. The **Synthesizer** object is highly customizable.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "from deepeval.synthesizer import Synthesizer\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "synthesizer = Synthesizer()\n",
    "goldens = synthesizer.generate_goldens_from_docs(document_paths=['example.txt', 'example.docx', 'example.pdf'])\n",
    "\n",
    "# Since the synthesizer generates so-called goldens they don't have actual_output and retrieval_context fields\n",
    "# You can generate them prior to evaluation or during evaluation time\n",
    "dataset = EvaluationDataset(goldens=goldens)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM provider\n",
    "\n",
    "**DeepEval** uses **OpenAI** by default as a LLM, however **Ollama** is also available. To use it execute the code cell below. This will generate a `.deepeval` file where key-value pairs will be stored about that particular LLM-provider like model name, base url and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🙌 Congratulations! You're now using a local Ollama model for all evals that \n",
      "require an LLM.\n",
      "🙌 Congratulations! You're now using Ollama embeddings for all evals that \n",
      "require text embeddings.\n"
     ]
    }
   ],
   "source": [
    "!deepeval set-ollama llama3.1:latest --base-url=\"http://localhost:11434/\"\n",
    "!deepeval set-ollama-embeddings mxbai-embed-large --base-url=\"http://localhost11434\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pushing a **dataset** to **Confident AI**\n",
    "\n",
    "Since I already have a dataset which was generated by **RAGAs** I would like to create a **DeepEval** equivalent and upload it to the cloud so that I can use it in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast \n",
    "import typing as t\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "def upload_ragas_dataset_to_confident_ai(filepath: str, dataset_name: str):\n",
    "    try:\n",
    "        dataset: DataFrame = pd.read_csv(filepath)\n",
    "        test_cases: t.List[LLMTestCase] = []\n",
    "        for _, row in dataset.iterrows():\n",
    "            test_cases.append(\n",
    "                LLMTestCase(\n",
    "                    input=row['user_input'],\n",
    "                    actual_output=row['response'],\n",
    "                    expected_output=row['reference'],\n",
    "                    context=ast.literal_eval(row['reference_contexts']),\n",
    "                    retrieval_context=ast.literal_eval(row['retrieved_contexts']),        \n",
    "                )\n",
    "            )\n",
    "            \n",
    "        deepeval_dataset: EvaluationDataset = EvaluationDataset(test_cases)\n",
    "        deepeval_dataset.push(\n",
    "            alias=dataset_name,\n",
    "            auto_convert_test_cases_to_goldens=True\n",
    "        )\n",
    "    except FileNotFoundError as fnfe:\n",
    "        print(print(fnfe.strerror))\n",
    "    except TypeError as te:\n",
    "        print(str(te))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Dataset successfully pushed to Confident AI! View at \n",
       "<a href=\"https://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/datasets/cm8yr2rli0qycxek59i7r2pvo\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/datasets/cm8yr2rli0qycxek59i7r2pvo</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Dataset successfully pushed to Confident AI! View at \n",
       "\u001b]8;id=258279;https://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/datasets/cm8yr2rli0qycxek59i7r2pvo\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/datasets/cm8yr2rli0qycxek59i7r2pvo\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gtk-Message: 19:06:38.380: Failed to load module \"canberra-gtk-module\"\n",
      "Gtk-Message: 19:06:38.381: Failed to load module \"canberra-gtk-module\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening in existing browser session.\n"
     ]
    }
   ],
   "source": [
    "upload_ragas_dataset_to_confident_ai(\"../ragas/dataset.csv\", \"RAGAs Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling a **dataset** from **Confident AI**\n",
    "\n",
    "If you already have a dataset on the platform just use the `pull` method and specify the name/alias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "evaluation_dataset: EvaluationDataset = EvaluationDataset()\n",
    "evaluation_dataset.pull(\"RAGAs Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating a dataset/set of test cases using metric/s\n",
    "\n",
    "Evaluation in **DeepEval** works as follow:\n",
    "* Pull or create a dataset containing test cases or goldens:\n",
    "    - In the case of **goldens** have the LLM generate the `actual_output` and `retrieval_context` fields.\n",
    "    - Usually the **goldens** are preferred since one can modify the `prompt template` during evaluation.\n",
    "* Think about the application and the different use cases:\n",
    "    - What is it doing?\n",
    "    - How could I assure it's doing what it's supposed to be doing?\n",
    "    - Check out the existing metrics or create your own one.\n",
    "    - Create metric/s.\n",
    "* Run the evaluation:\n",
    "    - In notebooks use the `evaluate` method.\n",
    "    - If you choose to create test cases in python file/s:\n",
    "        - The filename/s should start with `test_` and can be ran using `deepeval test run <filename>`.\n",
    "        - Optionally, pass in flags like `-c` to use the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Correctness </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using llama3.</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">1</span><span style=\"color: #374151; text-decoration-color: #374151\">:latest </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">Ollama</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mCorrectness \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing llama3.\u001b[0m\u001b[1;38;2;55;65;81m1\u001b[0m\u001b[38;2;55;65;81m:latest \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mOllama\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |██████████|100% (1/1) [Time Taken: 00:00, 42.39test case/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Correctness (GEval) (score: 1.0, threshold: 0.5, strict: False, evaluation model: llama3.1:latest (Ollama), reason: Actual output matches expected output exactly and meets all requirements specified., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: what happend with spacex\n",
      "  - actual output: The query about SpaceX cannot be determined as there is no mention of it in the provided context. The context only discusses potential issues and resolutions for in-flight services, booking-related issues, and special assistance with airlines.\n",
      "  - expected output: There is no mention of SpaceX in the provided context.\n",
      "  - context: ['Special Assistance\\n\\nRagas Airlines provides special assistance services for passengers with disabilities, unaccompanied minors, and those requiring medical support. Below is a detailed breakdown of how to request and prepare for these services.\\n\\nPassengers with Disabilities\\n\\nRagas Airlines ensures accessibility for passengers requiring wheelchair assistance, mobility aid support, or other special needs accommodations.\\n\\n1: Requesting Assistance Before Travel\\n\\nRequest assistance at least 48 hours before departure through:\\n\\n“Manage My Booking” on the airline’s website.\\n\\nThe customer support hotline.\\n\\nYour travel agent (if booked through an agent).\\n\\n2: Available Assistance Options\\n\\nWheelchair Assistance → Available at check-in, security, boarding, and deplaning.\\n\\nPriority Boarding → Passengers needing assistance can board before other passengers.\\n\\nAirport Escort Service → Assistance from check-in to the boarding gate.\\n\\n3: Traveling with Medical Equipment or Service Animals\\n\\nMedical Equipment → Passengers may carry medical devices (e.g., portable oxygen concentrators) but must notify the airline 48 hours in advance.\\n\\nService Animals → Allowed on board but require advance notification and documentation.\\n\\nUnaccompanied Minors\\n\\nChildren traveling alone are provided with dedicated staff assistance to ensure a safe journey.\\n\\n1: Age Restrictions for Unaccompanied Minors\\n\\n5-12 years → Must use the airline’s Unaccompanied Minor (UM) service.\\n\\n13-17 years → Optional UM service available.\\n\\n2: Booking the UM Service\\n\\nStep 1: Contact customer service or your travel agent to book the UM service.\\n\\nStep 2: Provide parent/guardian details, including:\\n\\nFull name and contact number of the person dropping off the minor.\\n\\nFull name and contact number of the person receiving the minor.\\n\\n3: Airport Assistance\\n\\nA dedicated airline staff member will:\\n\\nEscort the child through security and boarding.\\n\\nSupervise them during the flight.\\n\\nEnsure a safe handover at the destination.\\n\\nPassengers with Medical Conditions\\n\\nPassengers requiring medical assistance or special accommodations must notify the airline at least 48 hours before departure.\\n\\n1: Traveling with Medications\\n\\nCarry medications in original packaging with a doctor’s prescription.\\n\\nIf medication requires refrigeration, notify the airline in advance.\\n\\n2: Medical Clearance for Travel\\n\\nPassengers may need a doctor’s approval if: - They recently had surgery. - They have a contagious illness. - They require in-flight oxygen or other medical support.\\n\\nTo obtain clearance: 1. Have your doctor complete a Medical Information Form (MEDIF). 2. Submit the form to the airline’s medical department at least 48 hours before the flight.\\n\\nPotential Issues and Resolutions for Special Assistance\\n\\n1. Late Requests for Special Assistance\\n\\nIf you did not request assistance in advance: - Step 1: Visit the airline’s check-in counter as early as possible. - Step 2: Inform the staff about your requirements. - Step 3: The airline will try to accommodate you, but some services may be unavailable on short notice.\\n\\n2. Missing Documents for Medical Clearance\\n\\nIf a medical clearance form (MEDIF) is missing, the airline may deny boarding. - Step 1: Contact your doctor immediately to request the required paperwork. - Step 2: Submit the form via email or fax to the airline’s medical department. - Step 3: If clearance is delayed, request to reschedule your flight instead of canceling.']\n",
      "  - retrieval context: ['Potential Issues and Resolutions for In-Flight Services\\n\\nMeal Not Available Despite Pre-Order - Step 1: Inform a flight attendant and show your meal confirmation. - Step 2: If your preferred meal is unavailable, request an alternative special meal. - Step 3: If no options are available, file a complaint online for possible compensation.\\n\\nWi-Fi Not Working After Purchase - Step 1: Try reconnecting to the network and ensure you entered the correct Wi-Fi access code. - Step 2: If the issue persists, inform a flight attendant. - Step 3: If Wi-Fi remains unavailable for most of the flight, request a refund after landing via the airline’s website.', \"available (cancellation fees apply).\\nMay allow travel credit instead of a full refund.\\nSeat selection may require an additional charge.\\n\\nBasic Economy & Promotional Fares\\n\\nNo modifications or cancellations\\nallowed.\\nNo refunds\\n, even if the passenger does not travel.\\nSeat selection may be\\nrestricted or auto\\nassigned\\n.\\nAdditional fees apply for\\nchecked baggage or upgrades\\n.\\nPotential Issues and Resolutions for Booking Related Issues Now, let's go over potential booking issues and detailed step-by-step solutions.\\n*\\n\\nPayment Failure*\", 'Option 3: Request a Full Refund - If none of the new flight options work for you, you can request a full refund instead. - Steps to request a refund: - Click “Cancel Flight” in \"Manage My Booking\". - Select “Request Full Refund”. - Refund processing: - 7 business days (for credit/debit card payments). - 20 business days (for bank transfers).\\n\\n*\\nPassenger\\nInitiated Flight Date Changes\\nPassengers may need to change their flight date due to personal reasons. Whether you can modify your booking depends on your fare type and availability.\\nStep 1: Check Your Fare Rules Before Modifying Your Flight\\nLog into\\n“Manage My Booking”\\n.\\nEnter your\\nbooking reference number\\nand\\nlast name\\n.\\nReview your\\nticket conditions\\nunder “Fare Rules.”\\nStep 2: Modify Your Ticket Based on Fare Type\\nRefundable Ticket\\n→ Change flight for free or get a full refund.\\nNon\\nRefundable Ticket\\n→ Pay a modification fee, or receive a travel credit.', 'Potential Issues and Resolutions for Special Assistance\\n\\nLate Requests for Special Assistance If you did not request assistance in advance: - Step 1: Visit the airline’s check-in counter as early as possible. - Step 2: Inform the staff about your requirements. - Step 3: The airline will try to accommodate you, but some services may be unavailable on short notice.\\n\\nMissing Documents for Medical Clearance If a medical clearance form (MEDIF) is missing, the airline may deny boarding. - Step 1: Contact your doctor immediately to request the required paperwork. - Step 2: Submit the form via email or fax to the airline’s medical department. - Step 3: If clearance is delayed, request to reschedule your flight instead of canceling.', 'reimbursement request\\nthrough the airline’s website under the\\n“Claim Compensation”\\nsection.\\nStep 3:\\nThe airline will review your claim and process a refund\\nwithin 14 business days\\n.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Correctness (GEval): 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Results saved in .<span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">test_data</span> as 20250403_151524\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Results saved in .\u001b[35m/\u001b[0m\u001b[95mtest_data\u001b[0m as 20250403_151524\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! View results on \n",
       "<a href=\"https://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/evaluation/test-runs/cm91dp4x309ebdrls074i03eo/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/evaluation/test-runs/cm91dp4x309ebdrls074i03eo/test-</span></a>\n",
       "<a href=\"https://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/evaluation/test-runs/cm91dp4x309ebdrls074i03eo/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">cases</span></a><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! View results on \n",
       "\u001b]8;id=695499;https://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/evaluation/test-runs/cm91dp4x309ebdrls074i03eo/test-cases\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/evaluation/test-runs/cm91dp4x309ebdrls074i03eo/test-\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b]8;id=695499;https://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/evaluation/test-runs/cm91dp4x309ebdrls074i03eo/test-cases\u001b\\\u001b[4;94mcases\u001b[0m\u001b]8;;\u001b\\\u001b[4;94m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gtk-Message: 15:15:25.955: Failed to load module \"canberra-gtk-module\"\n",
      "Gtk-Message: 15:15:25.955: Failed to load module \"canberra-gtk-module\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening in existing browser session.\n"
     ]
    }
   ],
   "source": [
    "from deepeval.evaluate import (\n",
    "    evaluate, \n",
    "    EvaluationResult\n",
    ")\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "\n",
    "correctness_metric = GEval(\n",
    "    name=\"Correctness\",\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],\n",
    "    criteria=\"Determine if the 'actual output' is correct based on the 'expected output'.\",\n",
    ")\n",
    "\n",
    "# For running an evaluation in notebook use this approach\n",
    "results: EvaluationResult = evaluate(\n",
    "    test_cases=[evaluation_dataset.test_cases[0]],\n",
    "    metrics=[correctness_metric],\n",
    "    use_cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **DagMetric**\n",
    "\n",
    "This metric can be used instead of **GEval** to build a more deterministic approach for a custom metric logic. It makes use of a directed acyclic graph (DAG). The basic idea is to create a decision tree with nodes and directed edges with leaf nodes representing the score/verdict. Users can specify their own rules/conditions and steps to be performed, in cases where the pre-defined metrics are not enough.\n",
    "\n",
    "Example:\n",
    "![Image DAG](https://confident-docs.s3.amazonaws.com/dag-formatting-metric.svg)\n",
    "\n",
    "There are **four** main types of nodes:\n",
    "* **TaskNode**: usually utilized for pre-processing and its output is further used by other nodes\n",
    "* **BinaryJudgementNode**: takes a `criteria` which is very often the output of the **TaskNode** and returns either `True` or `False`\n",
    "* **NonBinaryJudgementNode**: the same as the **BinaryJudgementNode**, however it can have values besides `True` or `False`\n",
    "* **VerdictNode**: Leaf nodes which hold a score, determining the final output of the **DagMetric** based on the path taken\n",
    "\n",
    "### Required arguments:\n",
    "* `input`\n",
    "* `actual_output`\n",
    "\n",
    "### **Context Utilization** *(Using DagMetric)*\n",
    "\n",
    "### **TL;DR**:\n",
    "> **How effectively is each piece of retrieved context utilized in the final response? Are we making good use of what we've retrieved?**\n",
    "\n",
    "---\n",
    "\n",
    "## **Definition**:\n",
    "\n",
    "**Context Utilization** measures how effectively the retrieved context is utilized in the generated response. This metric ensures that the generation process makes appropriate use of the available context. A **higher utilization score** means that more of the relevant retrieved information was incorporated into the response, making it crucial for applications that need to maximize the value of retrieved context. Unlike relevance metrics that focus on what was retrieved, utilization focuses on what was actually used.\n",
    "\n",
    "---\n",
    "\n",
    "## **DAG-Based Context Utilization**\n",
    "\n",
    "This metric evaluates context utilization by analyzing individual context items and tracking how their information flows into the final response. It uses a **Deep Acyclic Graph (DAG)** to make deterministic judgments about context usage patterns.\n",
    "\n",
    "## **Formula**:\n",
    "\n",
    "**Context Utilization** = Weighted combination of:\n",
    "- *Information Coverage* (percentage of relevant retrieved information used)\n",
    "- *Distribution Balance* (how evenly information was drawn from different context items)\n",
    "- *Relevance Alignment* (whether the most relevant context items were utilized most effectively)\n",
    "\n",
    "A utilization score **closer to 1** indicates efficient use of the retrieved context, while a **lower score** suggests that valuable retrieved information was ignored or underutilized.\n",
    "\n",
    "---\n",
    "\n",
    "## **How It Works**\n",
    "\n",
    "This metric calculates utilization using a decision tree approach with the following steps:\n",
    "\n",
    "1. **Process Context Items**  \n",
    "   - Breaks down each context item in the retrieved list into distinct information units.\n",
    "   - Rates the relevance of each context item to the query.\n",
    "\n",
    "2. **Track Information Usage**  \n",
    "   - For each context item, determines what percentage of its information appears in the final response.\n",
    "   - Notes whether information was used directly, modified, or contradicted.\n",
    "\n",
    "3. **Analyze Distribution Patterns**  \n",
    "   - Evaluates how evenly information was utilized across all context items.\n",
    "   - Checks if the response drew from many context items or focused on just a few.\n",
    "\n",
    "4. **Evaluate Relevance-Utilization Alignment**  \n",
    "   - Determines if the most relevant context items were given appropriate weight in the response.\n",
    "   - Highlights misalignments where highly relevant context was underutilized.\n",
    "\n",
    "5. **Calculate Overall Information Coverage**  \n",
    "   - Computes the total percentage of available relevant information that made it into the response.\n",
    "   - Adjusts based on the importance of the information that was used or omitted.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why is This Useful?**\n",
    "\n",
    "- **Complements Retrieval Metrics** → While recall measures if you retrieved the right context, utilization measures if you used it effectively.\n",
    "- **Identifies Generator Weaknesses** → Reveals if your model ignores important retrieved information.\n",
    "- **Optimizes Context Window Usage** → Helps determine if you're wasting tokens on context that never gets used.\n",
    "- **Improves Response Completeness** → Ensures all relevant retrieved information is considered in the response.\n",
    "- **Detects Pattern Biases** → Reveals if your model systematically ignores certain types of information.\n",
    "\n",
    "A utilization score **closer to 1** means that the retrieved context was used efficiently, while a **lower score** suggests that the generator is not making good use of the available information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Multi-Context Utilization Analysis </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">DAG</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using llama3.</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">1</span><span style=\"color: #374151; text-decoration-color: #374151\">:latest </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">Ollama</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mMulti-Context Utilization Analysis \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mDAG\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing llama3.\u001b[0m\u001b[1;38;2;55;65;81m1\u001b[0m\u001b[38;2;55;65;81m:latest \u001b[0m\n",
       "\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mOllama\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 52 test case(s) in parallel: |          |  0% (0/52) [Time Taken: 00:00, ?test case/s]"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics.dag import (\n",
    "    DeepAcyclicGraph,\n",
    "    TaskNode,\n",
    "    BinaryJudgementNode,\n",
    "    NonBinaryJudgementNode,\n",
    "    VerdictNode,\n",
    ")\n",
    "from deepeval.metrics import DAGMetric\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "\n",
    "# Step 1: Process each context item individually\n",
    "process_contexts_node = TaskNode(\n",
    "    instructions=\"\"\"\n",
    "    The retrieved_context is a list of separate context items. For each context item:\n",
    "    1. Identify it as 'Context Item #N' (where N is its position in the list)\n",
    "    2. Extract the key information units/facts from that context item\n",
    "    3. Rate the relevance of each context item to the query (high/medium/low)\n",
    "    \"\"\",\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.RETRIEVAL_CONTEXT],\n",
    "    output_label=\"Processed Context Items\",\n",
    "    children=[]  # Will be populated below\n",
    ")\n",
    "\n",
    "# Step 2: Track utilization of each context item\n",
    "track_context_usage_node = TaskNode(\n",
    "    instructions=\"\"\"\n",
    "    For each 'Context Item #N' identified in the previous step:\n",
    "    1. Determine if information from this context item appears in the actual_output\n",
    "    2. Estimate what percentage of the key information from this context item was used (0-100%)\n",
    "    3. Note whether the information was used directly, modified, or contradicted\n",
    "    4. Create a utilization summary for each context item\n",
    "    \"\"\",\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    output_label=\"Context Item Utilization\",\n",
    "    children=[]  # Will be populated below\n",
    ")\n",
    "\n",
    "# Step 3: Analyze per-item utilization patterns\n",
    "context_distribution_node = NonBinaryJudgementNode(\n",
    "    criteria=\"How evenly was information utilized across all context items? Did the response draw from many context items or just a few?\",\n",
    "    children=[\n",
    "        VerdictNode(verdict=\"Well-distributed across most context items\", score=10),\n",
    "        VerdictNode(verdict=\"Moderately distributed across several context items\", score=7),\n",
    "        VerdictNode(verdict=\"Heavily skewed toward few context items\", score=4),\n",
    "        VerdictNode(verdict=\"Drew from only one context item\", score=2),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Step 4: Evaluate relevance-utilization alignment\n",
    "relevance_utilization_node = BinaryJudgementNode(\n",
    "    criteria=\"Were the most relevant context items (as identified in step 1) utilized more than less relevant ones?\",\n",
    "    children=[\n",
    "        VerdictNode(verdict=False, score=3),\n",
    "        VerdictNode(verdict=True, score=10),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Step 5: Calculate overall information coverage\n",
    "information_coverage_node = NonBinaryJudgementNode(\n",
    "    criteria=\"Overall, what percentage of the total relevant information available across all context items was utilized in the response?\",\n",
    "    children=[\n",
    "        VerdictNode(verdict=\"Excellent (80-100%)\", score=10),\n",
    "        VerdictNode(verdict=\"Good (60-79%)\", score=8),\n",
    "        VerdictNode(verdict=\"Adequate (40-59%)\", score=6),\n",
    "        VerdictNode(verdict=\"Poor (20-39%)\", score=4),\n",
    "        VerdictNode(verdict=\"Very poor (0-19%)\", score=2),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Connect the nodes\n",
    "process_contexts_node.children = [track_context_usage_node]\n",
    "track_context_usage_node.children = [\n",
    "    context_distribution_node, \n",
    "    relevance_utilization_node,\n",
    "    information_coverage_node\n",
    "]\n",
    "\n",
    "# Create the DAG\n",
    "dag = DeepAcyclicGraph(root_nodes=[process_contexts_node])\n",
    "\n",
    "# Create the metric\n",
    "multi_context_utilization_metric = DAGMetric(\n",
    "    name=\"Multi-Context Utilization Analysis\",\n",
    "    dag=dag,\n",
    "    threshold=0.7,\n",
    "    verbose_mode=True\n",
    ")\n",
    "\n",
    "from deepeval.evaluate import (\n",
    "    evaluate, \n",
    "    EvaluationResult\n",
    ")\n",
    "\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "\n",
    "# For running an evaluation in notebook use this approach\n",
    "results: EvaluationResult = evaluate(\n",
    "    test_cases=evaluation_dataset.test_cases,\n",
    "    metrics=[multi_context_utilization_metric],\n",
    "    use_cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepeval_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
