{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confident AI\n",
    "\n",
    "1. In short **Confident AI** is a cloud-based platform part of the **DeepEval** framework, which stores **datasets**, **evaluations** and **monitoring data**. \n",
    "\n",
    "2. If you want to use **Confident AI** platform create an account from here: [Confident AI](https://www.confident-ai.com/)\n",
    "\n",
    "3. After signing-up an **API key** will be generated, which can be used to interact with the platform from inside the notebook.\n",
    "\n",
    "---\n",
    "\n",
    "Example of .env file:\n",
    "```bash\n",
    "DEEPEVAL_RESULTS_FOLDER=<folder> # Results of evaluations can be saved locally\n",
    "DEEPEVAL_API_KEY=<your api key>  # Relevant if you want to use Confident AI\n",
    "DEEPEVAL_TELEMETRY_OPT_OUT=\"YES\" # Remove telemetry\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🎉🥳 Congratulations! You've successfully logged in! 🙌 \n",
       "</pre>\n"
      ],
      "text/plain": [
       "🎉🥳 Congratulations! You've successfully logged in! 🙌 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from deepeval import login_with_confident_api_key\n",
    "\n",
    "# Loads the environment variables from a `.env` file.\n",
    "# If you want to use Confident AI be sure to create one in this directory.\n",
    "load_dotenv()\n",
    "\n",
    "deepeval_api_key: str = os.getenv(\"DEEPEVAL_API_KEY\")\n",
    "\n",
    "# You should get a message letting you know you are logged-in.\n",
    "login_with_confident_api_key(deepeval_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMTestCase\n",
    "\n",
    "Unlike **RAGAs**, where a single interaction between a user and the AI system is represented by either a **SingleTurnSample** or **MultiTurnSample**, in **DeepEval** there's the concept of so called **LLMTestCase**/**MLLMTestCase** and **ConversationalTestCase**. For this project the **LLMTestCase** will be of relevance. Just like in **RAGAs**, **LLMTestCase** objects have the same fields just different names - input, actual_output, expected_output, etc.\n",
    "\n",
    "![Image showcasing what a LLMTestCase is.](https://confident-docs.s3.amazonaws.com/llm-test-case.svg \"LLMTestCase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "**Evaluation** should be a crucial component of every single application which uses **AI**. **DeepEval** provides more than 14 metrics for evaluation so that one can very easily iterate towards a better LLM application. Each default metric uses **LLM-As-A-Judge**. Optionally, one can use the **GEval** to set a custom criteria for evaluation if neither of the other metrics meet the requirements. Alternatively, there's the **DAGMetric**, whose purpose is similar to the **GEval**, however it uses a graph and it's fully **deterministic**.\n",
    "\n",
    "When evaluating a test case, multiple metrics can be used and the test would be **positive** iff all the **metrics thresholds** have been exceeded and **negative** in any other case. \n",
    "\n",
    "Evaluation workflow:\n",
    "![Image of evaluation workflox](https://d2lsxfc3p6r9rv.cloudfront.net/workflow.png \"Evaluation comparison\")\n",
    "\n",
    "---\n",
    "\n",
    "**Confident AI** enables users to compare evaluation runs between one another and to compare results. In some instances a new evaluation run might yield better overall results, however have a **failing** test which was previously **successful**. This is known as **regression** and will be marked in **red**. Tests marked in **green** show an improvement. \n",
    "\n",
    "![Image of evaluation run comparsion](https://confident-docs.s3.us-east-1.amazonaws.com/comparison-page.png \"Evaluation comparison\")\n",
    "\n",
    "* Here is the LLM development workflow that is highly recommended with **Confident AI**: \n",
    "    - Curate datasets (unless you don't have one already available)\n",
    "    - Run evaluations with dataset\n",
    "    - Analyze evaluation results\n",
    "    - Improve LLM application based on evaluation results\n",
    "    - Run another evaluation on the same dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Dataset\n",
    "\n",
    "An **evaluation dataset** is just a collection of **LLMTestCase**- or so called **Golden** objects. A **Golden** is structurally the same as a **LLMTestCase**, however it has no `actual_output` and `retrieval_context` fields, which can be generated by your LLM at evaluation time.\n",
    "\n",
    "Datasets can be **pushed**, **stored** and **pulled** from **Confident AIs** platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic datasets\n",
    "\n",
    "**DeepEval** can be used to generate **synthetic dataset** as well. The **Synthesizer** object is highly customizable.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "from deepeval.synthesizer import Synthesizer\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "synthesizer = Synthesizer()\n",
    "goldens = synthesizer.generate_goldens_from_docs(document_paths=['example.txt', 'example.docx', 'example.pdf'])\n",
    "\n",
    "# Since the synthesizer generates so-called goldens they don't have actual_output and retrieval_context fields\n",
    "# You can generate them prior to evaluation or during evaluation time\n",
    "dataset = EvaluationDataset(goldens=goldens)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM provider\n",
    "\n",
    "**DeepEval** uses **OpenAI** by default as a LLM, however **Ollama** is also available. To use it execute the code cell below. This will generate a `.deepeval` file where key-value pairs will be stored about that particular LLM-provider like model name, base url and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🙌 Congratulations! You're now using a local Ollama model for all evals that \n",
      "require an LLM.\n",
      "🙌 Congratulations! You're now using Ollama embeddings for all evals that \n",
      "require text embeddings.\n"
     ]
    }
   ],
   "source": [
    "!deepeval set-ollama llama3.1:latest --base-url=\"http://localhost:11434/\"\n",
    "!deepeval set-ollama-embeddings mxbai-embed-large --base-url=\"http://localhost11434\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pushing a **dataset** to **Confident AI**\n",
    "\n",
    "Since I already have a dataset which was generated by **RAGAs** I would like to create a **DeepEval** equivalent and upload it to the cloud so that I can use it in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast \n",
    "import typing as t\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "def upload_ragas_dataset_to_confident_ai(filepath: str, dataset_name: str):\n",
    "    try:\n",
    "        dataset: DataFrame = pd.read_csv(filepath)\n",
    "        test_cases: t.List[LLMTestCase] = []\n",
    "        for _, row in dataset.iterrows():\n",
    "            test_cases.append(\n",
    "                LLMTestCase(\n",
    "                    input=row['user_input'],\n",
    "                    actual_output=row['response'],\n",
    "                    expected_output=row['reference'],\n",
    "                    context=ast.literal_eval(row['reference_contexts']),\n",
    "                    retrieval_context=ast.literal_eval(row['retrieved_contexts']),        \n",
    "                )\n",
    "            )\n",
    "            \n",
    "        deepeval_dataset: EvaluationDataset = EvaluationDataset(test_cases)\n",
    "        deepeval_dataset.push(\n",
    "            alias=dataset_name,\n",
    "            auto_convert_test_cases_to_goldens=True\n",
    "        )\n",
    "    except FileNotFoundError as fnfe:\n",
    "        print(print(fnfe.strerror))\n",
    "    except TypeError as te:\n",
    "        print(str(te))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Dataset successfully pushed to Confident AI! View at \n",
       "<a href=\"https://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/datasets/cm8yr2rli0qycxek59i7r2pvo\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/datasets/cm8yr2rli0qycxek59i7r2pvo</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Dataset successfully pushed to Confident AI! View at \n",
       "\u001b]8;id=258279;https://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/datasets/cm8yr2rli0qycxek59i7r2pvo\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/datasets/cm8yr2rli0qycxek59i7r2pvo\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gtk-Message: 19:06:38.380: Failed to load module \"canberra-gtk-module\"\n",
      "Gtk-Message: 19:06:38.381: Failed to load module \"canberra-gtk-module\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening in existing browser session.\n"
     ]
    }
   ],
   "source": [
    "upload_ragas_dataset_to_confident_ai(\"../ragas/dataset.csv\", \"RAGAs Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling a **dataset** from **Confident AI**\n",
    "\n",
    "If you already have a dataset on the platform just use the `pull` method and specify the name/alias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/\n",
       "lib/python3.12/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/\n",
       "lib/python3.12/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "evaluation_dataset: EvaluationDataset = EvaluationDataset()\n",
    "evaluation_dataset.pull(\"RAGAs Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating a dataset/set of test cases using metric/s\n",
    "\n",
    "Evaluation in **DeepEval** works as follow:\n",
    "* Pull or create a dataset containing test cases or goldens:\n",
    "    - In the case of **goldens** have the LLM generate the `actual_output` and `retrieval_context` fields.\n",
    "    - Usually the **goldens** are preferred since one can modify the `prompt template` during evaluation.\n",
    "* Think about the application and the different use cases:\n",
    "    - What is it doing?\n",
    "    - How could I assure it's doing what it's supposed to be doing?\n",
    "    - Check out the existing metrics or create your own one.\n",
    "    - Create metric/s.\n",
    "* Run the evaluation:\n",
    "    - In notebooks use the `evaluate` method.\n",
    "    - If you choose to create test cases in python file/s:\n",
    "        - The filename/s should start with `test_` and can be ran using `deepeval test run <filename>`.\n",
    "        - Optionally, pass in flags like `-c` to use the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Correctness </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using llama3.</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">1</span><span style=\"color: #374151; text-decoration-color: #374151\">:latest </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">Ollama</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mCorrectness \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing llama3.\u001b[0m\u001b[1;38;2;55;65;81m1\u001b[0m\u001b[38;2;55;65;81m:latest \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mOllama\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |██████████|100% (1/1) [Time Taken: 00:00, 42.39test case/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Correctness (GEval) (score: 1.0, threshold: 0.5, strict: False, evaluation model: llama3.1:latest (Ollama), reason: Actual output matches expected output exactly and meets all requirements specified., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: what happend with spacex\n",
      "  - actual output: The query about SpaceX cannot be determined as there is no mention of it in the provided context. The context only discusses potential issues and resolutions for in-flight services, booking-related issues, and special assistance with airlines.\n",
      "  - expected output: There is no mention of SpaceX in the provided context.\n",
      "  - context: ['Special Assistance\\n\\nRagas Airlines provides special assistance services for passengers with disabilities, unaccompanied minors, and those requiring medical support. Below is a detailed breakdown of how to request and prepare for these services.\\n\\nPassengers with Disabilities\\n\\nRagas Airlines ensures accessibility for passengers requiring wheelchair assistance, mobility aid support, or other special needs accommodations.\\n\\n1: Requesting Assistance Before Travel\\n\\nRequest assistance at least 48 hours before departure through:\\n\\n“Manage My Booking” on the airline’s website.\\n\\nThe customer support hotline.\\n\\nYour travel agent (if booked through an agent).\\n\\n2: Available Assistance Options\\n\\nWheelchair Assistance → Available at check-in, security, boarding, and deplaning.\\n\\nPriority Boarding → Passengers needing assistance can board before other passengers.\\n\\nAirport Escort Service → Assistance from check-in to the boarding gate.\\n\\n3: Traveling with Medical Equipment or Service Animals\\n\\nMedical Equipment → Passengers may carry medical devices (e.g., portable oxygen concentrators) but must notify the airline 48 hours in advance.\\n\\nService Animals → Allowed on board but require advance notification and documentation.\\n\\nUnaccompanied Minors\\n\\nChildren traveling alone are provided with dedicated staff assistance to ensure a safe journey.\\n\\n1: Age Restrictions for Unaccompanied Minors\\n\\n5-12 years → Must use the airline’s Unaccompanied Minor (UM) service.\\n\\n13-17 years → Optional UM service available.\\n\\n2: Booking the UM Service\\n\\nStep 1: Contact customer service or your travel agent to book the UM service.\\n\\nStep 2: Provide parent/guardian details, including:\\n\\nFull name and contact number of the person dropping off the minor.\\n\\nFull name and contact number of the person receiving the minor.\\n\\n3: Airport Assistance\\n\\nA dedicated airline staff member will:\\n\\nEscort the child through security and boarding.\\n\\nSupervise them during the flight.\\n\\nEnsure a safe handover at the destination.\\n\\nPassengers with Medical Conditions\\n\\nPassengers requiring medical assistance or special accommodations must notify the airline at least 48 hours before departure.\\n\\n1: Traveling with Medications\\n\\nCarry medications in original packaging with a doctor’s prescription.\\n\\nIf medication requires refrigeration, notify the airline in advance.\\n\\n2: Medical Clearance for Travel\\n\\nPassengers may need a doctor’s approval if: - They recently had surgery. - They have a contagious illness. - They require in-flight oxygen or other medical support.\\n\\nTo obtain clearance: 1. Have your doctor complete a Medical Information Form (MEDIF). 2. Submit the form to the airline’s medical department at least 48 hours before the flight.\\n\\nPotential Issues and Resolutions for Special Assistance\\n\\n1. Late Requests for Special Assistance\\n\\nIf you did not request assistance in advance: - Step 1: Visit the airline’s check-in counter as early as possible. - Step 2: Inform the staff about your requirements. - Step 3: The airline will try to accommodate you, but some services may be unavailable on short notice.\\n\\n2. Missing Documents for Medical Clearance\\n\\nIf a medical clearance form (MEDIF) is missing, the airline may deny boarding. - Step 1: Contact your doctor immediately to request the required paperwork. - Step 2: Submit the form via email or fax to the airline’s medical department. - Step 3: If clearance is delayed, request to reschedule your flight instead of canceling.']\n",
      "  - retrieval context: ['Potential Issues and Resolutions for In-Flight Services\\n\\nMeal Not Available Despite Pre-Order - Step 1: Inform a flight attendant and show your meal confirmation. - Step 2: If your preferred meal is unavailable, request an alternative special meal. - Step 3: If no options are available, file a complaint online for possible compensation.\\n\\nWi-Fi Not Working After Purchase - Step 1: Try reconnecting to the network and ensure you entered the correct Wi-Fi access code. - Step 2: If the issue persists, inform a flight attendant. - Step 3: If Wi-Fi remains unavailable for most of the flight, request a refund after landing via the airline’s website.', \"available (cancellation fees apply).\\nMay allow travel credit instead of a full refund.\\nSeat selection may require an additional charge.\\n\\nBasic Economy & Promotional Fares\\n\\nNo modifications or cancellations\\nallowed.\\nNo refunds\\n, even if the passenger does not travel.\\nSeat selection may be\\nrestricted or auto\\nassigned\\n.\\nAdditional fees apply for\\nchecked baggage or upgrades\\n.\\nPotential Issues and Resolutions for Booking Related Issues Now, let's go over potential booking issues and detailed step-by-step solutions.\\n*\\n\\nPayment Failure*\", 'Option 3: Request a Full Refund - If none of the new flight options work for you, you can request a full refund instead. - Steps to request a refund: - Click “Cancel Flight” in \"Manage My Booking\". - Select “Request Full Refund”. - Refund processing: - 7 business days (for credit/debit card payments). - 20 business days (for bank transfers).\\n\\n*\\nPassenger\\nInitiated Flight Date Changes\\nPassengers may need to change their flight date due to personal reasons. Whether you can modify your booking depends on your fare type and availability.\\nStep 1: Check Your Fare Rules Before Modifying Your Flight\\nLog into\\n“Manage My Booking”\\n.\\nEnter your\\nbooking reference number\\nand\\nlast name\\n.\\nReview your\\nticket conditions\\nunder “Fare Rules.”\\nStep 2: Modify Your Ticket Based on Fare Type\\nRefundable Ticket\\n→ Change flight for free or get a full refund.\\nNon\\nRefundable Ticket\\n→ Pay a modification fee, or receive a travel credit.', 'Potential Issues and Resolutions for Special Assistance\\n\\nLate Requests for Special Assistance If you did not request assistance in advance: - Step 1: Visit the airline’s check-in counter as early as possible. - Step 2: Inform the staff about your requirements. - Step 3: The airline will try to accommodate you, but some services may be unavailable on short notice.\\n\\nMissing Documents for Medical Clearance If a medical clearance form (MEDIF) is missing, the airline may deny boarding. - Step 1: Contact your doctor immediately to request the required paperwork. - Step 2: Submit the form via email or fax to the airline’s medical department. - Step 3: If clearance is delayed, request to reschedule your flight instead of canceling.', 'reimbursement request\\nthrough the airline’s website under the\\n“Claim Compensation”\\nsection.\\nStep 3:\\nThe airline will review your claim and process a refund\\nwithin 14 business days\\n.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Correctness (GEval): 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Results saved in .<span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">test_data</span> as 20250403_151524\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Results saved in .\u001b[35m/\u001b[0m\u001b[95mtest_data\u001b[0m as 20250403_151524\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! View results on \n",
       "<a href=\"https://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/evaluation/test-runs/cm91dp4x309ebdrls074i03eo/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/evaluation/test-runs/cm91dp4x309ebdrls074i03eo/test-</span></a>\n",
       "<a href=\"https://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/evaluation/test-runs/cm91dp4x309ebdrls074i03eo/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">cases</span></a><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! View results on \n",
       "\u001b]8;id=695499;https://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/evaluation/test-runs/cm91dp4x309ebdrls074i03eo/test-cases\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/evaluation/test-runs/cm91dp4x309ebdrls074i03eo/test-\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b]8;id=695499;https://app.confident-ai.com/project/cm8yaugri01pu126xyl2ybbmz/evaluation/test-runs/cm91dp4x309ebdrls074i03eo/test-cases\u001b\\\u001b[4;94mcases\u001b[0m\u001b]8;;\u001b\\\u001b[4;94m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gtk-Message: 15:15:25.955: Failed to load module \"canberra-gtk-module\"\n",
      "Gtk-Message: 15:15:25.955: Failed to load module \"canberra-gtk-module\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening in existing browser session.\n"
     ]
    }
   ],
   "source": [
    "from deepeval.evaluate import (\n",
    "    evaluate, \n",
    "    EvaluationResult\n",
    ")\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "\n",
    "correctness_metric = GEval(\n",
    "    name=\"Correctness\",\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],\n",
    "    criteria=\"Determine if the 'actual output' is correct based on the 'expected output'.\",\n",
    ")\n",
    "\n",
    "# For running an evaluation in notebook use this approach\n",
    "results: EvaluationResult = evaluate(\n",
    "    test_cases=[evaluation_dataset.test_cases[0]],\n",
    "    metrics=[correctness_metric],\n",
    "    use_cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **DagMetric**\n",
    "\n",
    "This metric can be used instead of **GEval** to build a more deterministic approach for a custom metric logic. It makes use of a directed acyclic graph (DAG). The basic idea is to create a decision tree with nodes and directed edges with leaf nodes representing the score/verdict. Users can specify their own rules/conditions and steps to be performed, in cases where the pre-defined metrics are not enough.\n",
    "\n",
    "Example:\n",
    "![Image DAG](https://confident-docs.s3.amazonaws.com/dag-formatting-metric.svg)\n",
    "\n",
    "There are **four** main types of nodes:\n",
    "* **TaskNode**: usually utilized for pre-processing and its output is further used by other nodes\n",
    "* **BinaryJudgementNode**: takes a `criteria` which is very often the output of the **TaskNode** and returns either `True` or `False`\n",
    "* **NonBinaryJudgementNode**: the same as the **BinaryJudgementNode**, however it can have values besides `True` or `False`\n",
    "* **VerdictNode**: Leaf nodes which hold a score, determining the final output of the **DagMetric** based on the path taken\n",
    "\n",
    "### Required arguments:\n",
    "* `input`\n",
    "* `actual_output`\n",
    "\n",
    "### **Context Utilization** *(Using DagMetric)*\n",
    "\n",
    "### **TL;DR**:\n",
    "> **How effectively is each piece of retrieved context utilized in the final response? Are we making good use of what we've retrieved?**\n",
    "\n",
    "---\n",
    "\n",
    "## **Definition**:\n",
    "\n",
    "**Context Utilization** measures how effectively the retrieved context is utilized in the generated response. This metric ensures that the generation process makes appropriate use of the available context. A **higher utilization score** means that more of the relevant retrieved information was incorporated into the response, making it crucial for applications that need to maximize the value of retrieved context. Unlike relevance metrics that focus on what was retrieved, utilization focuses on what was actually used.\n",
    "\n",
    "---\n",
    "\n",
    "## **DAG-Based Context Utilization**\n",
    "\n",
    "This metric evaluates context utilization by analyzing individual context items and tracking how their information flows into the final response. It uses a **Deep Acyclic Graph (DAG)** to make deterministic judgments about context usage patterns.\n",
    "\n",
    "## **Formula**:\n",
    "\n",
    "**Context Utilization** = Weighted combination of:\n",
    "- *Information Coverage* (percentage of relevant retrieved information used)\n",
    "- *Distribution Balance* (how evenly information was drawn from different context items)\n",
    "- *Relevance Alignment* (whether the most relevant context items were utilized most effectively)\n",
    "\n",
    "A utilization score **closer to 1** indicates efficient use of the retrieved context, while a **lower score** suggests that valuable retrieved information was ignored or underutilized.\n",
    "\n",
    "---\n",
    "\n",
    "## **How It Works**\n",
    "\n",
    "This metric calculates utilization using a decision tree approach with the following steps:\n",
    "\n",
    "1. **Process Context Items**  \n",
    "   - Breaks down each context item in the retrieved list into distinct information units.\n",
    "   - Rates the relevance of each context item to the query.\n",
    "\n",
    "2. **Track Information Usage**  \n",
    "   - For each context item, determines what percentage of its information appears in the final response.\n",
    "   - Notes whether information was used directly, modified, or contradicted.\n",
    "\n",
    "3. **Analyze Distribution Patterns**  \n",
    "   - Evaluates how evenly information was utilized across all context items.\n",
    "   - Checks if the response drew from many context items or focused on just a few.\n",
    "\n",
    "4. **Evaluate Relevance-Utilization Alignment**  \n",
    "   - Determines if the most relevant context items were given appropriate weight in the response.\n",
    "   - Highlights misalignments where highly relevant context was underutilized.\n",
    "\n",
    "5. **Calculate Overall Information Coverage**  \n",
    "   - Computes the total percentage of available relevant information that made it into the response.\n",
    "   - Adjusts based on the importance of the information that was used or omitted.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why is This Useful?**\n",
    "\n",
    "- **Complements Retrieval Metrics** → While recall measures if you retrieved the right context, utilization measures if you used it effectively.\n",
    "- **Identifies Generator Weaknesses** → Reveals if your model ignores important retrieved information.\n",
    "- **Optimizes Context Window Usage** → Helps determine if you're wasting tokens on context that never gets used.\n",
    "- **Improves Response Completeness** → Ensures all relevant retrieved information is considered in the response.\n",
    "- **Detects Pattern Biases** → Reveals if your model systematically ignores certain types of information.\n",
    "\n",
    "A utilization score **closer to 1** means that the retrieved context was used efficiently, while a **lower score** suggests that the generator is not making good use of the available information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Multi-Context Utilization Analysis </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">DAG</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using llama3.</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">1</span><span style=\"color: #374151; text-decoration-color: #374151\">:latest </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">Ollama</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mMulti-Context Utilization Analysis \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mDAG\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing llama3.\u001b[0m\u001b[1;38;2;55;65;81m1\u001b[0m\u001b[38;2;55;65;81m:latest \u001b[0m\n",
       "\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mOllama\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 52 test case(s) in parallel: |          |  0% (0/52) [Time Taken: 2:35:00, ?test case/s]\n"
     ]
    },
    {
     "ename": "RemoteProtocolError",
     "evalue": "Server disconnected without sending a response.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRemoteProtocolError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/httpx/_transports/default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/httpx/_transports/default.py:394\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pool.handle_async_request(req)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:256\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:236\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m connection.handle_async_request(\n\u001b[32m    237\u001b[39m         pool_request.request\n\u001b[32m    238\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/httpcore/_async/connection.py:103\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_async_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/httpcore/_async/http11.py:136\u001b[39m, in \u001b[36mAsyncHTTP11Connection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/httpcore/_async/http11.py:106\u001b[39m, in \u001b[36mAsyncHTTP11Connection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._receive_response_headers(**kwargs)\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/httpcore/_async/http11.py:177\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._receive_event(timeout=timeout)\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/httpcore/_async/http11.py:231\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    230\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mServer disconnected without sending a response.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteProtocolError(msg)\n\u001b[32m    233\u001b[39m \u001b[38;5;28mself\u001b[39m._h11_state.receive_data(data)\n",
      "\u001b[31mRemoteProtocolError\u001b[39m: Server disconnected without sending a response.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRemoteProtocolError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 114\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepeval\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtest_case\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMTestCaseParams\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# For running an evaluation in notebook use this approach\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m results: EvaluationResult = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_cases\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluation_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtest_cases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmulti_context_utilization_metric\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    118\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/deepeval/evaluate.py:1067\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(test_cases, metrics, hyperparameters, run_async, show_indicator, print_results, write_cache, use_cache, ignore_errors, skip_on_missing_params, verbose_mode, identifier, throttle_value, max_concurrent, display)\u001b[39m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_async:\n\u001b[32m   1066\u001b[39m     loop = get_or_create_event_loop()\n\u001b[32m-> \u001b[39m\u001b[32m1067\u001b[39m     test_results = \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1068\u001b[39m \u001b[43m        \u001b[49m\u001b[43ma_execute_test_cases\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtest_cases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1070\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[43m            \u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[43m            \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[43m            \u001b[49m\u001b[43mverbose_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m            \u001b[49m\u001b[43msave_to_disk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrite_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m            \u001b[49m\u001b[43mshow_indicator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_indicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m            \u001b[49m\u001b[43mskip_on_missing_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_on_missing_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m            \u001b[49m\u001b[43mthrottle_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthrottle_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m            \u001b[49m\u001b[43midentifier\u001b[49m\u001b[43m=\u001b[49m\u001b[43midentifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_concurrent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_concurrent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1083\u001b[39m     test_results = execute_test_cases(\n\u001b[32m   1084\u001b[39m         test_cases,\n\u001b[32m   1085\u001b[39m         metrics,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1092\u001b[39m         show_indicator=show_indicator,\n\u001b[32m   1093\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/asyncio/futures.py:202\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/asyncio/tasks.py:316\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    314\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._must_cancel:\n\u001b[32m    319\u001b[39m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/deepeval/evaluate.py:694\u001b[39m, in \u001b[36ma_execute_test_cases\u001b[39m\u001b[34m(test_cases, metrics, ignore_errors, skip_on_missing_params, use_cache, show_indicator, throttle_value, max_concurrent, save_to_disk, verbose_mode, identifier, test_run_manager, _use_bar_indicator)\u001b[39m\n\u001b[32m    691\u001b[39m                     tasks.append(asyncio.create_task(task))\n\u001b[32m    693\u001b[39m                 \u001b[38;5;28;01mawait\u001b[39;00m asyncio.sleep(throttle_value)\n\u001b[32m--> \u001b[39m\u001b[32m694\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n\u001b[32m    695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    696\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m test_case \u001b[38;5;129;01min\u001b[39;00m test_cases:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/asyncio/tasks.py:385\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    388\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/asyncio/tasks.py:316\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    314\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._must_cancel:\n\u001b[32m    319\u001b[39m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/deepeval/evaluate.py:590\u001b[39m, in \u001b[36ma_execute_test_cases.<locals>.execute_with_semaphore\u001b[39m\u001b[34m(func, *args, **kwargs)\u001b[39m\n\u001b[32m    588\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute_with_semaphore\u001b[39m(func: Callable, *args, **kwargs):\n\u001b[32m    589\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m semaphore:\n\u001b[32m--> \u001b[39m\u001b[32m590\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/deepeval/evaluate.py:801\u001b[39m, in \u001b[36ma_execute_llm_test_cases\u001b[39m\u001b[34m(metrics, test_case, test_run_manager, test_results, count, test_run, ignore_errors, skip_on_missing_params, use_cache, show_indicator, _use_bar_indicator, pbar)\u001b[39m\n\u001b[32m    799\u001b[39m new_cached_test_case: CachedTestCase = CachedTestCase()\n\u001b[32m    800\u001b[39m test_start_time = time.perf_counter()\n\u001b[32m--> \u001b[39m\u001b[32m801\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m measure_metrics_with_indicator(\n\u001b[32m    802\u001b[39m     metrics=metrics,\n\u001b[32m    803\u001b[39m     test_case=test_case,\n\u001b[32m    804\u001b[39m     cached_test_case=cached_test_case,\n\u001b[32m    805\u001b[39m     skip_on_missing_params=skip_on_missing_params,\n\u001b[32m    806\u001b[39m     ignore_errors=ignore_errors,\n\u001b[32m    807\u001b[39m     show_indicator=show_metrics_indicator,\n\u001b[32m    808\u001b[39m )\n\u001b[32m    810\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m metrics:\n\u001b[32m    811\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m metric.skipped:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/deepeval/metrics/indicator.py:201\u001b[39m, in \u001b[36mmeasure_metrics_with_indicator\u001b[39m\u001b[34m(metrics, test_case, cached_test_case, ignore_errors, skip_on_missing_params, show_indicator)\u001b[39m\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    195\u001b[39m         tasks.append(\n\u001b[32m    196\u001b[39m             safe_a_measure(\n\u001b[32m    197\u001b[39m                 metric, test_case, ignore_errors, skip_on_missing_params\n\u001b[32m    198\u001b[39m             )\n\u001b[32m    199\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/asyncio/tasks.py:385\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    388\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/asyncio/tasks.py:316\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    314\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._must_cancel:\n\u001b[32m    319\u001b[39m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/deepeval/metrics/indicator.py:211\u001b[39m, in \u001b[36msafe_a_measure\u001b[39m\u001b[34m(metric, tc, ignore_errors, skip_on_missing_params)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msafe_a_measure\u001b[39m(\n\u001b[32m    205\u001b[39m     metric: Union[BaseMetric, BaseMultimodalMetric, BaseConversationalMetric],\n\u001b[32m    206\u001b[39m     tc: Union[LLMTestCase, MLLMTestCase, ConversationalTestCase],\n\u001b[32m    207\u001b[39m     ignore_errors: \u001b[38;5;28mbool\u001b[39m,\n\u001b[32m    208\u001b[39m     skip_on_missing_params: \u001b[38;5;28mbool\u001b[39m,\n\u001b[32m    209\u001b[39m ):\n\u001b[32m    210\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m metric.a_measure(tc, _show_indicator=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m MissingTestCaseParamsError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    213\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m skip_on_missing_params:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/deepeval/metrics/dag/dag.py:93\u001b[39m, in \u001b[36mDAGMetric.a_measure\u001b[39m\u001b[34m(self, test_case, _show_indicator)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28mself\u001b[39m.evaluation_cost = \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.using_native_model \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m metric_progress_indicator(\n\u001b[32m     91\u001b[39m     \u001b[38;5;28mself\u001b[39m, async_mode=\u001b[38;5;28;01mTrue\u001b[39;00m, _show_indicator=_show_indicator\n\u001b[32m     92\u001b[39m ):\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dag._a_execute(metric=\u001b[38;5;28mself\u001b[39m, test_case=test_case)\n\u001b[32m     94\u001b[39m     \u001b[38;5;28mself\u001b[39m.success = \u001b[38;5;28mself\u001b[39m.is_successful()\n\u001b[32m     95\u001b[39m     \u001b[38;5;28mself\u001b[39m.verbose_logs = construct_verbose_logs(\n\u001b[32m     96\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     97\u001b[39m         steps=[\n\u001b[32m   (...)\u001b[39m\u001b[32m    100\u001b[39m         ],\n\u001b[32m    101\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/deepeval/metrics/dag/graph.py:38\u001b[39m, in \u001b[36mDeepAcyclicGraph._a_execute\u001b[39m\u001b[34m(self, metric, test_case)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_a_execute\u001b[39m(\n\u001b[32m     34\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     35\u001b[39m     metric: BaseMetric,\n\u001b[32m     36\u001b[39m     test_case: LLMTestCase,\n\u001b[32m     37\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m     39\u001b[39m         *(\n\u001b[32m     40\u001b[39m             root_node._a_execute(\n\u001b[32m     41\u001b[39m                 metric=metric, test_case=test_case, depth=\u001b[32m0\u001b[39m\n\u001b[32m     42\u001b[39m             )\n\u001b[32m     43\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m root_node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.root_nodes\n\u001b[32m     44\u001b[39m         )\n\u001b[32m     45\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/asyncio/tasks.py:385\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    388\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/asyncio/tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/deepeval/metrics/dag/nodes.py:323\u001b[39m, in \u001b[36mTaskNode._a_execute\u001b[39m\u001b[34m(self, metric, test_case, depth)\u001b[39m\n\u001b[32m    317\u001b[39m prompt = TaskNodeTemplate.generate_task_output(\n\u001b[32m    318\u001b[39m     instructions=\u001b[38;5;28mself\u001b[39m.instructions,\n\u001b[32m    319\u001b[39m     text=text,\n\u001b[32m    320\u001b[39m )\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metric.using_native_model:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     res, cost = \u001b[38;5;28;01mawait\u001b[39;00m metric.model.a_generate(\n\u001b[32m    324\u001b[39m         prompt, schema=TaskNodeOutput\n\u001b[32m    325\u001b[39m     )\n\u001b[32m    326\u001b[39m     metric.evaluation_cost += cost\n\u001b[32m    327\u001b[39m     \u001b[38;5;28mself\u001b[39m._output = res.output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/deepeval/models/llms/ollama_model.py:46\u001b[39m, in \u001b[36mOllamaModel.a_generate\u001b[39m\u001b[34m(self, prompt, schema)\u001b[39m\n\u001b[32m     44\u001b[39m chat_model = \u001b[38;5;28mself\u001b[39m.load_model(async_mode=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     45\u001b[39m model_name = KEY_FILE_HANDLER.fetch_data(KeyValues.LOCAL_MODEL_NAME)\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m response: ChatResponse = \u001b[38;5;28;01mawait\u001b[39;00m chat_model.chat(\n\u001b[32m     47\u001b[39m     model=model_name,\n\u001b[32m     48\u001b[39m     messages=[{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: prompt}],\n\u001b[32m     49\u001b[39m     \u001b[38;5;28mformat\u001b[39m=schema.model_json_schema() \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     50\u001b[39m )\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m     52\u001b[39m     (\n\u001b[32m     53\u001b[39m         schema.model_validate_json(response.message.content)\n\u001b[32m   (...)\u001b[39m\u001b[32m     57\u001b[39m     \u001b[32m0\u001b[39m,\n\u001b[32m     58\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/ollama/_client.py:837\u001b[39m, in \u001b[36mAsyncClient.chat\u001b[39m\u001b[34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[39m\n\u001b[32m    792\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat\u001b[39m(\n\u001b[32m    793\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    794\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    801\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    802\u001b[39m ) -> Union[ChatResponse, AsyncIterator[ChatResponse]]:\n\u001b[32m    803\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    804\u001b[39m \u001b[33;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[32m    805\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    834\u001b[39m \u001b[33;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns an asynchronous `ChatResponse` generator.\u001b[39;00m\n\u001b[32m    835\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m837\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(\n\u001b[32m    838\u001b[39m     ChatResponse,\n\u001b[32m    839\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    840\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m/api/chat\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    841\u001b[39m     json=ChatRequest(\n\u001b[32m    842\u001b[39m       model=model,\n\u001b[32m    843\u001b[39m       messages=[message \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m _copy_messages(messages)],\n\u001b[32m    844\u001b[39m       tools=[tool \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m _copy_tools(tools)],\n\u001b[32m    845\u001b[39m       stream=stream,\n\u001b[32m    846\u001b[39m       \u001b[38;5;28mformat\u001b[39m=\u001b[38;5;28mformat\u001b[39m,\n\u001b[32m    847\u001b[39m       options=options,\n\u001b[32m    848\u001b[39m       keep_alive=keep_alive,\n\u001b[32m    849\u001b[39m     ).model_dump(exclude_none=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m    850\u001b[39m     stream=stream,\n\u001b[32m    851\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/ollama/_client.py:682\u001b[39m, in \u001b[36mAsyncClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    678\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    680\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m682\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**(\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request_raw(*args, **kwargs)).json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/ollama/_client.py:622\u001b[39m, in \u001b[36mAsyncClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_request_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    621\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m622\u001b[39m     r = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.request(*args, **kwargs)\n\u001b[32m    623\u001b[39m     r.raise_for_status()\n\u001b[32m    624\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/httpx/_client.py:1540\u001b[39m, in \u001b[36mAsyncClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m   1525\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m   1527\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m   1528\u001b[39m     method=method,\n\u001b[32m   1529\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1538\u001b[39m     extensions=extensions,\n\u001b[32m   1539\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1540\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.send(request, auth=auth, follow_redirects=follow_redirects)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/httpx/_client.py:1629\u001b[39m, in \u001b[36mAsyncClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m   1625\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m   1627\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_auth(\n\u001b[32m   1630\u001b[39m     request,\n\u001b[32m   1631\u001b[39m     auth=auth,\n\u001b[32m   1632\u001b[39m     follow_redirects=follow_redirects,\n\u001b[32m   1633\u001b[39m     history=[],\n\u001b[32m   1634\u001b[39m )\n\u001b[32m   1635\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1636\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/httpx/_client.py:1657\u001b[39m, in \u001b[36mAsyncClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m   1654\u001b[39m request = \u001b[38;5;28;01mawait\u001b[39;00m auth_flow.\u001b[34m__anext__\u001b[39m()\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1657\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_redirects(\n\u001b[32m   1658\u001b[39m         request,\n\u001b[32m   1659\u001b[39m         follow_redirects=follow_redirects,\n\u001b[32m   1660\u001b[39m         history=history,\n\u001b[32m   1661\u001b[39m     )\n\u001b[32m   1662\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1663\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/httpx/_client.py:1694\u001b[39m, in \u001b[36mAsyncClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m   1691\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1692\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_single_request(request)\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1696\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/httpx/_client.py:1730\u001b[39m, in \u001b[36mAsyncClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1725\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1726\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an sync request with an AsyncClient instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1727\u001b[39m     )\n\u001b[32m   1729\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1730\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m transport.handle_async_request(request)\n\u001b[32m   1732\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, AsyncByteStream)\n\u001b[32m   1733\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/httpx/_transports/default.py:393\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhttpcore\u001b[39;00m\n\u001b[32m    381\u001b[39m req = httpcore.Request(\n\u001b[32m    382\u001b[39m     method=request.method,\n\u001b[32m    383\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     extensions=request.extensions,\n\u001b[32m    392\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m    394\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pool.handle_async_request(req)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    156\u001b[39m     value = typ()\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/httpx/_transports/default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mRemoteProtocolError\u001b[39m: Server disconnected without sending a response."
     ]
    }
   ],
   "source": [
    "from deepeval.metrics.dag import (\n",
    "    DeepAcyclicGraph,\n",
    "    TaskNode,\n",
    "    BinaryJudgementNode,\n",
    "    NonBinaryJudgementNode,\n",
    "    VerdictNode,\n",
    ")\n",
    "from deepeval.metrics import DAGMetric\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "\n",
    "# Step 1: Process each context item individually\n",
    "process_contexts_node = TaskNode(\n",
    "    instructions=\"\"\"\n",
    "    The retrieved_context is a list of separate context items. For each context item:\n",
    "    1. Identify it as 'Context Item #N' (where N is its position in the list)\n",
    "    2. Extract the key information units/facts from that context item\n",
    "    3. Rate the relevance of each context item to the query (high/medium/low)\n",
    "    \"\"\",\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.RETRIEVAL_CONTEXT],\n",
    "    output_label=\"Processed Context Items\",\n",
    "    children=[]  # Will be populated below\n",
    ")\n",
    "\n",
    "# Step 2: Track utilization of each context item\n",
    "track_context_usage_node = TaskNode(\n",
    "    instructions=\"\"\"\n",
    "    For each 'Context Item #N' identified in the previous step:\n",
    "    1. Determine if information from this context item appears in the actual_output\n",
    "    2. Estimate what percentage of the key information from this context item was used (0-100%)\n",
    "    3. Note whether the information was used directly, modified, or contradicted\n",
    "    4. Create a utilization summary for each context item\n",
    "    \"\"\",\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    output_label=\"Context Item Utilization\",\n",
    "    children=[]  # Will be populated below\n",
    ")\n",
    "\n",
    "# Step 3: Analyze per-item utilization patterns\n",
    "context_distribution_node = NonBinaryJudgementNode(\n",
    "    criteria=\"How evenly was information utilized across all context items? Did the response draw from many context items or just a few?\",\n",
    "    children=[\n",
    "        VerdictNode(verdict=\"Well-distributed across most context items\", score=10),\n",
    "        VerdictNode(verdict=\"Moderately distributed across several context items\", score=7),\n",
    "        VerdictNode(verdict=\"Heavily skewed toward few context items\", score=4),\n",
    "        VerdictNode(verdict=\"Drew from only one context item\", score=2),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Step 4: Evaluate relevance-utilization alignment\n",
    "relevance_utilization_node = BinaryJudgementNode(\n",
    "    criteria=\"Were the most relevant context items (as identified in step 1) utilized more than less relevant ones?\",\n",
    "    children=[\n",
    "        VerdictNode(verdict=False, score=3),\n",
    "        VerdictNode(verdict=True, score=10),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Step 5: Calculate overall information coverage\n",
    "information_coverage_node = NonBinaryJudgementNode(\n",
    "    criteria=\"Overall, what percentage of the total relevant information available across all context items was utilized in the response?\",\n",
    "    children=[\n",
    "        VerdictNode(verdict=\"Excellent (80-100%)\", score=10),\n",
    "        VerdictNode(verdict=\"Good (60-79%)\", score=8),\n",
    "        VerdictNode(verdict=\"Adequate (40-59%)\", score=6),\n",
    "        VerdictNode(verdict=\"Poor (20-39%)\", score=4),\n",
    "        VerdictNode(verdict=\"Very poor (0-19%)\", score=2),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Connect the nodes\n",
    "process_contexts_node.children = [track_context_usage_node]\n",
    "track_context_usage_node.children = [\n",
    "    context_distribution_node, \n",
    "    relevance_utilization_node,\n",
    "    information_coverage_node\n",
    "]\n",
    "\n",
    "# Create the DAG\n",
    "dag = DeepAcyclicGraph(root_nodes=[process_contexts_node])\n",
    "\n",
    "# Create the metric\n",
    "multi_context_utilization_metric = DAGMetric(\n",
    "    name=\"Multi-Context Utilization Analysis\",\n",
    "    dag=dag,\n",
    "    threshold=0.7,\n",
    "    verbose_mode=True\n",
    ")\n",
    "\n",
    "from deepeval.evaluate import (\n",
    "    evaluate, \n",
    "    EvaluationResult\n",
    ")\n",
    "\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "\n",
    "# For running an evaluation in notebook use this approach\n",
    "results: EvaluationResult = evaluate(\n",
    "    test_cases=evaluation_dataset.test_cases,\n",
    "    metrics=[multi_context_utilization_metric],\n",
    "    use_cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Answer Relevancy**\n",
    "\n",
    "It uses the **LLM as a judge** to determine how well the response answers the user input. This metric is especially relevant when it comes to testing the **RAG pipelines generator** by comparing the degree of relevance of the output in regards to the user query. \n",
    "\n",
    "---\n",
    "\n",
    "### Required arguments:\n",
    "* `user input`\n",
    "* `actual output`\n",
    "\n",
    "---\n",
    "\n",
    "### Approach:\n",
    "\n",
    "The LLM is used to retrieve claims from the response and then they get compared with the user input. The LLM classifies each statement as follows:\n",
    "* `yes` if relevant\n",
    "* `no` if irrelevant\n",
    "* `idk` if non-determined or partially relevant.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula:\n",
    "\n",
    "__Answer Relevancy__ = $\\frac{\\text{number of relevant statements}}{\\text{total number of statements}}$\n",
    "\n",
    "---\n",
    "\n",
    "This metric is very similar if not the same as the **Response Relevance** in **RAGAs** in terms of what it evaluates, however it does it in a diffrerent way. In **RAGAs**, after the statements have been decomposed they get compared with the user query using **Semantic similarity** and then the sum of all comparison is divided by the total number of statements to get the final response. In **DeepEval** the statements are still decomposed, however we use the LLM to judge the relevance (no embedding is performed). \n",
    "\n",
    "|           | Answer Relevancy | Response Relevancy |\n",
    "|-----------|------------------|-------------|\n",
    "| Framework | DeepEval | RAGAs |\n",
    "| Approach  | Statemement decomposition + LLM-as-a-judge  | Statement decomposition + Semantic similarity  |\n",
    "| Score     |  $\\in{[0-1]} $ | $\\in{[0-1]} $ |\n",
    "| Component | Generator      | Generator     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.evaluate import evaluate, EvaluationResult\n",
    "from prompts.llama31_answer_relevancy_prompt import Llama31AnswerRelevancyTemplate\n",
    "\n",
    "# Currently, my custom template works OK in some instances, however in general the original one is superior.\n",
    "\n",
    "answer_relevancy = AnswerRelevancyMetric(\n",
    "    #threshold=0.7,\n",
    "    verbose_mode=True,\n",
    "    #evaluation_template=Llama31AnswerRelevancyTemplate\n",
    ")\n",
    "\n",
    "results: EvaluationResult = evaluate(\n",
    "    test_cases=evaluation_dataset.test_cases[40:48],\n",
    "    metrics=[answer_relevancy],\n",
    "    identifier=\"Answer Relevancy with default template Cases [40:48]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Faithfulness**\n",
    "\n",
    "The **Faithfulness** metric measures how factually consistent a response is with the retrieval context. It ranges from 0 to 1, with higher scores indicating better consistency.\n",
    "\n",
    "A response is considered faithful if its claims can be supported by the retrieved context.\n",
    "\n",
    "---\n",
    "\n",
    "### Required arguments:\n",
    "\n",
    "* `user input`\n",
    "* `actual output`\n",
    "* `retrieval context`\n",
    "\n",
    "---\n",
    "\n",
    "### Approach:\n",
    "\n",
    "1. **Identify Claims**:\n",
    "   - Break down the response into individual statements\n",
    "\n",
    "2. **Identify Truths**:\n",
    "   - Break down the retrieval context into individual statements\n",
    "\n",
    "3. **Use NLI**\n",
    "    - Using NLI determine if a claim in the response is factually based on claim/s in the context\n",
    "\n",
    "---\n",
    "\n",
    "### Formula:\n",
    "\n",
    "__Faithfulness__ = $\\frac{\\text{Number of truthful claims}}{\\text{Total number of claims}}$\n",
    "\n",
    "---\n",
    "\n",
    "### Framework Comparison:\n",
    "\n",
    "|              | Faithfulness | Faithfulness |\n",
    "|--------------|-------------------------|---------------------|\n",
    "| Framework    | DeepEval                | RAGAS               |\n",
    "| Approach     | Statement decomposition, LLM-as-a-judge with NLI | Statement decomposition, LLM-as-a-judge with NLI |\n",
    "| Components   | Generator             | Generator           |\n",
    "| Scoring      | $\\in{[0-1]}$          | $\\in{[0-1]}$        |\n",
    "| Customization| - | Can use HHEM-2.1-Open classifier |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using llama3.</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">1</span><span style=\"color: #374151; text-decoration-color: #374151\">:latest </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">Ollama</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing llama3.\u001b[0m\u001b[1;38;2;55;65;81m1\u001b[0m\u001b[38;2;55;65;81m:latest \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mOllama\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 10 test case(s) sequentially: |          |  0% (0/10) [Time Taken: 00:00, ?test case/s]"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/\n",
       "lib/python3.12/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/\n",
       "lib/python3.12/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 10 test case(s) sequentially: |          |  0% (0/10) [Time Taken: 02:46, ?test case/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Llama31FaithfulnessTemplate.generate_reason() got an unexpected keyword argument 'contradictions'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/deepeval/evaluate.py:350\u001b[39m, in \u001b[36mexecute_test_cases.<locals>.evaluate_test_cases\u001b[39m\u001b[34m(pbar)\u001b[39m\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m     \u001b[43mmetric\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmeasure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest_case\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_show_indicator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_metric_indicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m MissingTestCaseParamsError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/deepeval/metrics/faithfulness/faithfulness.py:81\u001b[39m, in \u001b[36mFaithfulnessMetric.measure\u001b[39m\u001b[34m(self, test_case, _show_indicator)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28mself\u001b[39m.score = \u001b[38;5;28mself\u001b[39m._calculate_score()\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[38;5;28mself\u001b[39m.reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_reason\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mself\u001b[39m.success = \u001b[38;5;28mself\u001b[39m.score >= \u001b[38;5;28mself\u001b[39m.threshold\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/deepeval/metrics/faithfulness/faithfulness.py:164\u001b[39m, in \u001b[36mFaithfulnessMetric._generate_reason\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    162\u001b[39m         contradictions.append(verdict.reason)\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m prompt = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluation_template\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_reason\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontradictions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontradictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.2f\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.using_native_model:\n",
      "\u001b[31mTypeError\u001b[39m: Llama31FaithfulnessTemplate.generate_reason() got an unexpected keyword argument 'contradictions'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mprompts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllama31_faithfulness_prompt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Llama31FaithfulnessTemplate \n\u001b[32m      5\u001b[39m faithfulness = FaithfulnessMetric(\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m#threshold=0.7,\u001b[39;00m\n\u001b[32m      7\u001b[39m     verbose_mode=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      8\u001b[39m     evaluation_template=Llama31FaithfulnessTemplate\n\u001b[32m      9\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m results: EvaluationResult = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_cases\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluation_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtest_cases\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfaithfulness\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43midentifier\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFaithfulness with my own template Cases [0:10]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_async\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/deepeval/evaluate.py:1083\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(test_cases, metrics, hyperparameters, run_async, show_indicator, print_results, write_cache, use_cache, ignore_errors, skip_on_missing_params, verbose_mode, identifier, throttle_value, max_concurrent, display)\u001b[39m\n\u001b[32m   1067\u001b[39m         test_results = loop.run_until_complete(\n\u001b[32m   1068\u001b[39m             a_execute_test_cases(\n\u001b[32m   1069\u001b[39m                 test_cases,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1080\u001b[39m             )\n\u001b[32m   1081\u001b[39m         )\n\u001b[32m   1082\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1083\u001b[39m         test_results = \u001b[43mexecute_test_cases\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtest_cases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1085\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[43m            \u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1087\u001b[39m \u001b[43m            \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[43m            \u001b[49m\u001b[43mverbose_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1089\u001b[39m \u001b[43m            \u001b[49m\u001b[43msave_to_disk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrite_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[43m            \u001b[49m\u001b[43mskip_on_missing_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_on_missing_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1091\u001b[39m \u001b[43m            \u001b[49m\u001b[43midentifier\u001b[49m\u001b[43m=\u001b[49m\u001b[43midentifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m            \u001b[49m\u001b[43mshow_indicator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_indicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m end_time = time.perf_counter()\n\u001b[32m   1096\u001b[39m run_duration = end_time - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/deepeval/evaluate.py:562\u001b[39m, in \u001b[36mexecute_test_cases\u001b[39m\u001b[34m(test_cases, metrics, skip_on_missing_params, ignore_errors, use_cache, show_indicator, save_to_disk, verbose_mode, identifier, test_run_manager, _use_bar_indicator)\u001b[39m\n\u001b[32m    555\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m show_indicator \u001b[38;5;129;01mand\u001b[39;00m _use_bar_indicator:\n\u001b[32m    556\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m tqdm(\n\u001b[32m    557\u001b[39m         desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_cases)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m test case(s) sequentially\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    558\u001b[39m         unit=\u001b[33m\"\u001b[39m\u001b[33mtest case\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    559\u001b[39m         total=\u001b[38;5;28mlen\u001b[39m(test_cases),\n\u001b[32m    560\u001b[39m         bar_format=\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{desc}\u001b[39;00m\u001b[33m: |\u001b[39m\u001b[38;5;132;01m{bar}\u001b[39;00m\u001b[33m|\u001b[39m\u001b[38;5;132;01m{percentage:3.0f}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{n_fmt}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{total_fmt}\u001b[39;00m\u001b[33m) [Time Taken: \u001b[39m\u001b[38;5;132;01m{elapsed}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{rate_fmt}\u001b[39;00m\u001b[38;5;132;01m{postfix}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    561\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m         \u001b[43mevaluate_test_cases\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    564\u001b[39m     evaluate_test_cases()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/deepeval/evaluate.py:365\u001b[39m, in \u001b[36mexecute_test_cases.<locals>.evaluate_test_cases\u001b[39m\u001b[34m(pbar)\u001b[39m\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    364\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m365\u001b[39m         \u001b[43mmetric\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmeasure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_case\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    366\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m MissingTestCaseParamsError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    367\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m skip_on_missing_params:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/deepeval/metrics/faithfulness/faithfulness.py:81\u001b[39m, in \u001b[36mFaithfulnessMetric.measure\u001b[39m\u001b[34m(self, test_case, _show_indicator)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28mself\u001b[39m.verdicts = \u001b[38;5;28mself\u001b[39m._generate_verdicts()\n\u001b[32m     80\u001b[39m \u001b[38;5;28mself\u001b[39m.score = \u001b[38;5;28mself\u001b[39m._calculate_score()\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[38;5;28mself\u001b[39m.reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_reason\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mself\u001b[39m.success = \u001b[38;5;28mself\u001b[39m.score >= \u001b[38;5;28mself\u001b[39m.threshold\n\u001b[32m     83\u001b[39m \u001b[38;5;28mself\u001b[39m.verbose_logs = construct_verbose_logs(\n\u001b[32m     84\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     85\u001b[39m     steps=[\n\u001b[32m   (...)\u001b[39m\u001b[32m     90\u001b[39m     ],\n\u001b[32m     91\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/project/deepeval/deepeval_venv/lib/python3.12/site-packages/deepeval/metrics/faithfulness/faithfulness.py:164\u001b[39m, in \u001b[36mFaithfulnessMetric._generate_reason\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    161\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m verdict.verdict.strip().lower() == \u001b[33m\"\u001b[39m\u001b[33mno\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    162\u001b[39m         contradictions.append(verdict.reason)\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m prompt = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluation_template\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_reason\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontradictions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontradictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.2f\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.using_native_model:\n\u001b[32m    170\u001b[39m     res, cost = \u001b[38;5;28mself\u001b[39m.model.generate(prompt, schema=Reason)\n",
      "\u001b[31mTypeError\u001b[39m: Llama31FaithfulnessTemplate.generate_reason() got an unexpected keyword argument 'contradictions'"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import FaithfulnessMetric\n",
    "from deepeval.evaluate import evaluate, EvaluationResult\n",
    "from prompts.llama31_faithfulness_prompt import Llama31FaithfulnessTemplate \n",
    "\n",
    "faithfulness = FaithfulnessMetric(\n",
    "    #threshold=0.7,\n",
    "    verbose_mode=True,\n",
    "    evaluation_template=Llama31FaithfulnessTemplate\n",
    ")\n",
    "\n",
    "results: EvaluationResult = evaluate(\n",
    "    test_cases=evaluation_dataset.test_cases[0:10],\n",
    "    metrics=[faithfulness],\n",
    "    identifier=\"Faithfulness with my own template Cases [0:10]\",\n",
    "    run_async=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Contextual Precision**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_case_3 - 0.75\n",
      "test_case_0 - 0.5714285714285714\n",
      "test_case_4 - 0.75\n",
      "test_case_5 - 0.6666666666666666\n",
      "test_case_9 - 0.6666666666666666\n",
      "test_case_8 - 1.0\n",
      "test_case_1 - 1.0\n",
      "test_case_7 - 0.7142857142857143\n",
      "test_case_2 - 0.8571428571428571\n",
      "test_case_6 - 1.0\n"
     ]
    }
   ],
   "source": [
    "for result in results.test_results:\n",
    "    print(f\"{result.name} - {result.metrics_data[0].score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepeval_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
