{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()\n",
    "app_token = os.getenv('RAGAS_APP_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/p3tr0vv/Desktop/Evaluation-Approaches-for-Retrieval-Augmented-Generation-RAG-/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "from ragas import SingleTurnSample, EvaluationDataset\n",
    "\n",
    "def load_dataset() -> EvaluationDataset:\n",
    "    df = pd.read_csv('dataset.csv')\n",
    "\n",
    "    # Convert the string representation of lists to actual Python lists\n",
    "    df['retrieved_contexts'] = df['retrieved_contexts'].apply(ast.literal_eval)\n",
    "\n",
    "    samples = []\n",
    "    for i in range(2):\n",
    "        sample = SingleTurnSample(\n",
    "            user_input = df['user_input'].iloc[i],\n",
    "            retrieved_contexts = df['retrieved_contexts'].iloc[i],\n",
    "            response = df['response'].iloc[i],\n",
    "            reference = df['reference'].iloc[i]\n",
    "        )\n",
    "        samples.append(sample)\n",
    "\n",
    "    eval_dataset = EvaluationDataset(samples)\n",
    "    return eval_dataset\n",
    "\n",
    "eval_dataset = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "from ragas.evaluation import evaluate\n",
    "from ragas.run_config import RunConfig\n",
    "from ragas.cache import DiskCacheBackend\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "llm_model = OllamaLLM(\n",
    "    model=\"llama3.1\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.1,\n",
    "    num_ctx=24000\n",
    ")\n",
    "\n",
    "embeddings_model = OllamaEmbeddings(\n",
    "    model=\"mxbai-embed-large\",\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    timeout = 3600, # One hour\n",
    "    max_wait = 30,\n",
    "    log_tenacity = True\n",
    ")\n",
    "\n",
    "cacher = DiskCacheBackend(\".cache\")\n",
    "\n",
    "llm = LangchainLLMWrapper(\n",
    "    langchain_llm=llm_model,\n",
    "    cache=cacher\n",
    ")\n",
    "\n",
    "embeddings = LangchainEmbeddingsWrapper(\n",
    "    embeddings=embeddings_model,\n",
    "    cache=cacher\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import LLMContextPrecisionWithoutReference\n",
    "\n",
    "\"\"\"\n",
    "Measures the number of relevant chunks with respect to the number of all chunks at a given rank.\n",
    "\n",
    "Example:\n",
    "    We have 4 chunks in total that were retrieved by RAG and 2 of those were deemed relevant\n",
    "    for answering the question of the user. For each rank k (1, 2, 3, 4), we calculate the \n",
    "    precision as the number of relevant chunks divided by the number of chunks at that rank.\n",
    "    \n",
    "    Assuming chunks at rank 1 and 3 were relevant it would look like this:\n",
    "        precision @ 1 => 1/1 = 1 (since the chunk is relevant and we have only 1 chunk at rank 1)\n",
    "        precision @ 2 => 1/2 = 0.5 (since there's only one relevant chunk, but 2 chunks at rank 2)\n",
    "        precision @ 3 => 2/3 = 0.67 (since 2 chunks were relevant at rank 3 were we have 3 in total)\n",
    "        precision @ 4 => 2/4 = 0.5 (since 2 out of all chunks were deemed relevant at rank 4)\n",
    "        \n",
    "        Final score in this case would be:\n",
    "        Context precision @ (K = 4) => (presicion @ 1 + precision @ 2 + precision @ 3 + precision @ 4) / # relevant chunks\n",
    "            => (1*1 + 0.5*0 + 0.67*1 + 0.5*0) / 2 = 0.835\n",
    "            \n",
    "    Abstract formula:\n",
    "        precision @ k = (true positives @ k) / (true positives @ k + false positives @ k)\n",
    "        context precision @ (K = n) = (precision @ 1 * v1 + ... + precision @ n * vn) / # relevant chunks\n",
    "            where v1, ..., vn are in {0,1} => so either a chunk is relevant or not\n",
    "\n",
    "https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/context_precision/\n",
    "\"\"\"\n",
    "context_precision = LLMContextPrecisionWithoutReference()\n",
    "\n",
    "context_precision_results = evaluate(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=[context_precision],\n",
    "    llm=llm,\n",
    "    embeddings=embeddings,\n",
    "    run_config=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import ContextRecall\n",
    "\n",
    "\"\"\"\n",
    "Measures how much of the relevant documents / pieces of information were retrieved, where the focus\n",
    "lies on not missing any relevant / important data. The previous metric focuses more on how \n",
    "relevant the retrieved chunks are. This metric is all about making sure that we retrieve \n",
    "all the neccesary information, without missing important data.\n",
    "Higher value for this metric means no missed or very few missed chunks.\n",
    "\n",
    "Abstract formula:\n",
    "    Context Recall = / Total number of claims in the reference\n",
    "\n",
    "https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/context_recall/\n",
    "\"\"\"\n",
    "context_recall = ContextRecall()\n",
    "\n",
    "context_recall_results = evaluate(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=[context_recall],\n",
    "    llm=llm,\n",
    "    embeddings=embeddings,\n",
    "    run_config=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 2/2 [01:08<00:00, 34.50s/it]\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import Faithfulness, AnswerRelevancy\n",
    "from ragas.evaluation import evaluate\n",
    "from ragas.run_config import RunConfig\n",
    "\n",
    "run_config = RunConfig(\n",
    "    timeout = 3600, # One hour\n",
    "    max_wait = 30,\n",
    "    log_tenacity = True\n",
    ")\n",
    "\n",
    "answer_relevancy = AnswerRelevancy()\n",
    "\n",
    "result = evaluate(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=[answer_relevancy],\n",
    "    llm=llm,\n",
    "    embeddings=embeddings,\n",
    "    run_config=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = result.to_pandas()\n",
    "result_df.to_csv('eval_results/faithfulness.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results uploaded! View at https://app.ragas.io/dashboard/alignment/evaluation/b53b5f82-d241-4905-a5c4-57d93dfc1a0f\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://app.ragas.io/dashboard/alignment/evaluation/b53b5f82-d241-4905-a5c4-57d93dfc1a0f'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.exceptions import RagasOutputParserException"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
