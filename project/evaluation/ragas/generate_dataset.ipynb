{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from r2r import R2RClient, R2RException\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "client = R2RClient(\n",
    "    base_url='http://localhost:7272',\n",
    "    timeout=600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file=\"./questions.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    questions = f.readlines()\n",
    "    questions = [q.strip() for q in questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_settings = {\n",
    "    \"use_semantic_search\": True,\n",
    "    \"limit\": 3,\n",
    "    \"offset\": 0,\n",
    "    \"include_metadatas\": True,\n",
    "    \"include_scores\": True,\n",
    "    \"search_strategy\": \"vanilla\",\n",
    "}\n",
    "\n",
    "rag_generation_config = {\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 1,\n",
    "    \"max_tokens_to_sample\": 512\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "## Task:\n",
    " \n",
    "Answer the query given below using the provided context. Keep your answer very short and concise!\n",
    "     \n",
    " - Aim to answer in 1-2 sentences whenever possible\n",
    " - If a longer answer is needed, make it as concise as possible focusing on the relevant\n",
    " - For step-by-step guides, use numbered steps with each step on a new line\n",
    " - If there're multiple answers, use numbered steps with each step on a new line\n",
    " - DO NOT use line item references to the context\n",
    " - If there is no context available locally to answer, inform the user of insufficient information\n",
    " - NEVER provide an answer if there's no context that discusses it\n",
    " - NEVER reason about a possible answer! If no context can answer the query there should be NO answer\n",
    " \n",
    " ### Query:\n",
    " \n",
    " {query}\n",
    " \n",
    " \n",
    " ### Context:\n",
    " \n",
    " {context}\n",
    " \n",
    " \n",
    " ### Query:\n",
    " \n",
    " {query}\n",
    " \n",
    " \n",
    " # Reminder: Provide short and concise answers and NEVER answer something that is not in the provided context!\n",
    " \n",
    " ## Response:\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_ctx_template(context: str) -> str:\n",
    "    summarize_prompt = f\"\"\"\n",
    "    Summarize the following context while preserving all key information needed to answer future questions:\n",
    "    \n",
    "    {context}\n",
    "    \n",
    "    Provide a concise summary that includes all essential facts, data points, and information.\n",
    "    Try to stay under 4 sentences. Only provide the summary and no further explanation or details.\n",
    "    Don't mention things like: Here is a concise summary of the key information.\n",
    "    \"\"\"\n",
    "    return summarize_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1 out of 51\n",
      "Question 2 out of 51\n",
      "Question 3 out of 51\n",
      "Question 4 out of 51\n",
      "Question 5 out of 51\n",
      "Question 6 out of 51\n",
      "Question 7 out of 51\n",
      "Question 8 out of 51\n",
      "Question 9 out of 51\n",
      "Question 10 out of 51\n",
      "Question 11 out of 51\n",
      "Question 12 out of 51\n",
      "Question 13 out of 51\n",
      "Question 14 out of 51\n",
      "Question 15 out of 51\n",
      "Question 16 out of 51\n",
      "Question 17 out of 51\n",
      "Question 18 out of 51\n",
      "Question 19 out of 51\n",
      "Question 20 out of 51\n",
      "Question 21 out of 51\n",
      "Question 22 out of 51\n",
      "Question 23 out of 51\n",
      "Question 24 out of 51\n",
      "Question 25 out of 51\n",
      "Question 26 out of 51\n",
      "Question 27 out of 51\n",
      "Question 28 out of 51\n",
      "Question 29 out of 51\n",
      "Question 30 out of 51\n",
      "Question 31 out of 51\n",
      "Question 32 out of 51\n",
      "Question 33 out of 51\n",
      "Question 34 out of 51\n",
      "Question 35 out of 51\n",
      "Question 36 out of 51\n",
      "Question 37 out of 51\n",
      "Question 38 out of 51\n",
      "Question 39 out of 51\n",
      "Question 40 out of 51\n",
      "Question 41 out of 51\n",
      "Question 42 out of 51\n",
      "Question 43 out of 51\n",
      "Question 44 out of 51\n",
      "Question 45 out of 51\n",
      "Question 46 out of 51\n",
      "Question 47 out of 51\n",
      "Question 48 out of 51\n",
      "Question 49 out of 51\n",
      "Question 50 out of 51\n",
      "Question 51 out of 51\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import ollama\n",
    "import pandas as pd\n",
    "\n",
    "# User question, context retrieved, actual answer, and LLM answer\n",
    "df = pd.DataFrame(columns=[\"question\", \"retrieved_context\", \"generated_response\", \"reference_answer\"])\n",
    "\n",
    "for i, q in enumerate(questions):\n",
    "    try:\n",
    "        # Submit a user query\n",
    "        resp = client.retrieval.rag(\n",
    "            query = q,\n",
    "            search_mode = \"custom\",\n",
    "            search_settings = search_settings,\n",
    "            rag_generation_config = rag_generation_config,\n",
    "            task_prompt_override = template,\n",
    "            include_title_if_available = True\n",
    "        ).results\n",
    "        \n",
    "        # After getting the response summarize the context\n",
    "        full_ctx = \"\\n\".join([re.sub(r\"\\n+\", \"\\n\", chunk.text) for chunk in resp.search_results.chunk_search_results])\n",
    "        \n",
    "        # Generate the summary by using the LLM\n",
    "        summary_ctx = ollama.generate(\n",
    "            model=\"llama3.1\",\n",
    "            prompt = summarize_ctx_template(full_ctx),\n",
    "            options = {\n",
    "                \"temperature\": 0.1,\n",
    "                \"num_predict\": 512\n",
    "            }\n",
    "        )['response']\n",
    "        \n",
    "        llm_asnwer = resp.completion\n",
    "\n",
    "        # Save on each iteration\n",
    "        df.loc[len(df)] = [q, summary_ctx, llm_asnwer, None]\n",
    "        \n",
    "        print(f\"Question {i+1} out of {len(questions)}\")\n",
    "    except R2RException as r2re:\n",
    "        print(f\"Skipping {i+1} because of {str(r2re)}\")\n",
    "    except ollama.ResponseError | ollama.RequestError as oe:\n",
    "        print(f\"Skipping {i+1} because of {str(oe)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {i+1} because of {str(e)}\")\n",
    "    \n",
    "# Finally, save data to disk\n",
    "df.to_csv(\"dataset.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
