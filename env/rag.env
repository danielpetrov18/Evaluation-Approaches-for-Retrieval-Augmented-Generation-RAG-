# Tweak these parameters when evaluating to see which offers best performance
TOP_K=5
TOP_P=1
MAX_TOKENS=512 # Limit on maximum number of tokens a response can be from the LLM
CHUNK_SIZE=768
TEMPERATURE=0.5
CHUNK_OVERLAP=64

CHAT_MODEL=llama3.1
EMBEDDING_MODEL=mxbai-embed-large

LLM_CONTEXT_WINDOW_TOKENS=16000 

MAX_RELEVANT_MESSAGES=10   # Relevant for custom history logic in the chat
SIMILARITY_THRESHOLD=0.70  # Relevant for custom history logic in the chat

# This will allow the R2R service to communicate with Ollama which will be running locally
OLLAMA_API_BASE=http://host.docker.internal:11434