# Tweak these parameters when evaluating to see which offers best performance
TOP_K=3
TOP_P=1
# Max tokens limit on the LLM response
MAX_TOKENS=512
# https://docs.unstructured.io/api-reference/partition/chunking
CHUNK_SIZE=512  # This represents the soft-max option - `new_after_n_chars`
CHUNK_OVERLAP=0 # This represents the overlap 
# Can range from 0-1. Higher values make the LLM more `creative` (non-deterministic).
TEMPERATURE=0.0

CHAT_MODEL=llama3.1:8b
EMBEDDING_MODEL=mxbai-embed-large

# For my project I use the same models for creation of goldens and evaluation of the datasets.
# Goldens are objects consisting of `input`, `expected output` and `reference context`.
# For generating synthetic data in my project I make use of `RAGAs`, however `DeepEval` also provides a solution.
DATA_GENERATION_MODEL=llama3.1:8b-instruct-q4_1
DATA_GENERATION_TEMPERATURE=0.0
EVALUATION_MODEL=llama3.1:8b-instruct-q4_1
EVALUATION_TEMPERATURE=0.0

# One experiment makes use of a more specialized form of RAG called `RAG-fusion`.
# If this parameter is True we use the basic RAG, otherwise the special one.
# This would be useful only during evaluation.
# https://r2r-docs.sciphi.ai/documentation/advanced-rag#rag-fusion
VANILLA_RAG=True

# To avoid lengthy prompts being truncated
# https://r2r-docs.sciphi.ai/cookbooks/local-llms
LLM_CONTEXT_WINDOW_TOKENS=16000 

# Since I wanted to add a simple history to my RAG application I store messages for each conversation.
# When a user query gets submitted I fetch all messages of a conversation, perform semantic similarity search
# to retrieve the MAX_RELEVANT_MESSAGES and then order them in descending order based on relevance.
# They are grouped together, and a LLM request is made to summarize the information.
# Finally, I submit the user query + `relevant summary` to the LLM to get a response.
MAX_RELEVANT_MESSAGES=5    # Relevant for custom history logic in the chat
SIMILARITY_THRESHOLD=0.70  # Relevant for custom history logic in the chat

# This will allow the `R2R` service to communicate with Ollama which will be running locally.
OLLAMA_API_BASE=http://host.docker.internal:11434