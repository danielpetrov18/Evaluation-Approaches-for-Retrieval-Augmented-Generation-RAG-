# Tweak these parameters when evaluating to see which offers best performance
TOP_K=10
TOP_P=1
MAX_TOKENS=1536 # Limit on maximum number of tokens a response can be from the LLM
CHUNK_SIZE=1024
TEMPERATURE=0.5
CHUNK_OVERLAP=128

CHAT_MODEL=deepseek-r1:7b
EMBEDDING_MODEL=mxbai-embed-large

LLM_CONTEXT_WINDOW_TOKENS=16000 

MAX_RELEVANT_MESSAGES=10   # Relevant for custom history logic in the chat
SIMILARITY_THRESHOLD=0.70  # Relevant for custom history logic in the chat

# This will allow the R2R service to communicate with Ollama which will be running locally
OLLAMA_API_BASE=http://host.docker.internal:11434