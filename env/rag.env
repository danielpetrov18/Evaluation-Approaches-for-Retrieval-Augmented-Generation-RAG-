# Tweak these parameters when evaluating to see which offers best performance
TOP_K=5
TOP_P=1
MAX_TOKENS=512 # Limit on maximum number of tokens a response can be from the LLM
CHUNK_SIZE=512
TEMPERATURE=0.0
CHUNK_OVERLAP=0

CHAT_MODEL=llama3.1:8b
EMBEDDING_MODEL=mxbai-embed-large

# For my project I use the same models for creation of goldens and evaluation of the datasets
# Goldens are objects consisting of `input`, `expected output` and `reference context`
# For generating synthetic data in my project I make use of RAGAs, however DeepEval also provides a solution

DATA_GENERATION_MODEL=llama3.1:8b-instruct-q4_1
DATA_GENERATION_TEMPERATURE=0.0

EVALUATION_MODEL=llama3.1:8b-instruct-q4_1
EVALUATION_TEMPERATURE=0.0

LLM_CONTEXT_WINDOW_TOKENS=16000 

MAX_RELEVANT_MESSAGES=10   # Relevant for custom history logic in the chat
SIMILARITY_THRESHOLD=0.70  # Relevant for custom history logic in the chat

# This will allow the R2R service to communicate with Ollama which will be running locally
OLLAMA_API_BASE=http://host.docker.internal:11434