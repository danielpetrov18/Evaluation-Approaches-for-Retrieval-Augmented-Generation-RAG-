# Tweak these parameters when evaluating to see which offers best performance
TOP_K=3
TOP_P=1
# Max tokens limit on the LLM response
MAX_TOKENS=512
# Do have in mind that both `chunk_size` and `chunk_overlap` are in characters.
# `R2R` makes use of `RecursiveCharacterTextSplitter`.
CHUNK_SIZE=512
CHUNK_OVERLAP=0
# Can range from 0-1. Higher values make the LLM more `creative` (non-deterministic).
TEMPERATURE=0.0

CHAT_MODEL=llama3.1:8b
EMBEDDING_MODEL=mxbai-embed-large

# For my project I use the same models for creation of goldens and evaluation of the datasets.
# Goldens are objects consisting of `input`, `expected output` and `reference context`.
# For generating synthetic data in my project I make use of `RAGAs`, however `DeepEval` also provides a solution.
DATA_GENERATION_MODEL=llama3.1:8b-instruct-q4_1
DATA_GENERATION_TEMPERATURE=0.0
EVALUATION_MODEL=llama3.1:8b-instruct-q4_1
EVALUATION_TEMPERATURE=0.0

# One experiment makes use of a more specialized form of RAG called `HyDE`.
# If this parameter is True we use the basic RAG, otherwise the special one.
# This would be useful only during evaluation.
VANILLA_RAG=True

# To avoid lengthy prompts being truncated 
LLM_CONTEXT_WINDOW_TOKENS=16000 

# Since I wanted to add a simple history to my RAG application I store messages for each conversation.
# When a user query gets submitted I fetch all messages of a conversation, perform semantic similarity search
# to retrieve the MAX_RELEVANT_MESSAGES and then order them in descending order based on relevance.
# They are grouped together, and a LLM request is made to summarize the information.
# Finally, I submit the user query + `relevant summary` to the LLM to get a response.
MAX_RELEVANT_MESSAGES=5    # Relevant for custom history logic in the chat
SIMILARITY_THRESHOLD=0.70  # Relevant for custom history logic in the chat

# This will allow the `R2R` service to communicate with Ollama which will be running locally.
OLLAMA_API_BASE=http://host.docker.internal:11434