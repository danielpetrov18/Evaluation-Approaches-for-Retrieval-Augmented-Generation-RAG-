# LLM environment variables for R2R (RAG) application
TOP_P=1                   # Nucleus sampling parameter, controls diversity of the output. 
MAX_TOKENS=512            # Maximum number of tokens to generate in the response.
TEMPERATURE=0.0           # The same across all experiments - deterministic results.
CHAT_MODEL=llama3.1:8b    # This might vary depending on the experiment.
# One experiment makes use of a more specialized form of RAG called `RAG-fusion`.
# If this parameter is True we use the basic RAG, otherwise the special one.
# This would be useful only during evaluation.
# https://r2r-docs.sciphi.ai/documentation/advanced-rag#rag-fusion
VANILLA_RAG=True          # This might vary depending on the experiment (Experiment 7 - set to False).


# Retrieval environment variables for R2R (RAG) application
# https://unstructured.io/blog/chunking-for-rag-best-practices
TOP_K=5           # This might vary depending on the experiment
CHUNK_SIZE=1024   # This might vary depending on the experiment, it represents the hard-max option - `max_characters` 
CHUNK_OVERLAP=128 # This might vary depending on the experiment, it represents the overlap 
EMBEDDING_MODEL=mxbai-embed-large


# For my project I use the same models for creation of goldens and evaluation of the datasets.
# Goldens are objects consisting of `input`, `expected output` and `reference context`.
# For generating synthetic data in my project I make use of `RAGAs`.
# https://docs.ragas.io/en/latest/getstarted/rag_testset_generation/
DATA_GENERATION_MODEL=llama3.1:8b-instruct-q4_1
EVALUATION_MODEL=llama3.1:8b-instruct-q4_1


# Default prompt length for Ollama is 2048 tokens.
# To avoid lengthy prompts being truncated.
# https://r2r-docs.sciphi.ai/cookbooks/local-llms
LLM_CONTEXT_WINDOW_TOKENS=16000 
# This will allow the `R2R` service to communicate with Ollama which will be running locally.
OLLAMA_API_BASE=http://host.docker.internal:11434
