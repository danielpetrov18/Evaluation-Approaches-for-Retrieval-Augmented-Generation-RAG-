# Tweak these parameters when evaluating to see which offers best performance
TOP_K=5
TOP_P=1
MAX_TOKENS=1024
CHUNK_SIZE=1024
TEMPERATURE=0.1
CHUNK_OVERLAP=64

CHAT_MODEL="llama3.1"
EMBEDDING_MODEL="mxbai-embed-large"

MAX_RELEVANT_MESSAGES=5   # Relevant for custom history logic
SIMILARITY_THRESHOLD=0.70 # Relevant for custom history logic

# This will allow the R2R service to communicate with Ollama which will be running locally
OLLAMA_API_BASE="http://host.docker.internal:11434"